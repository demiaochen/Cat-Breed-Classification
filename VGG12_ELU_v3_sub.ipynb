{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from helper import get_cat_count, count_parameters, compute_confusion_matrix, show_examples, plot_training_loss, plot_accuracy, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9444 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat breed classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**student.py**\n",
    "\n",
    "UNSW COMP9444 Neural Networks and Deep Learning\n",
    "\n",
    "You may modify this file however you wish, including creating additional\n",
    "variables, functions, classes, etc., so long as your code runs with the\n",
    "hw2main.py file unmodified, and you are only using the approved packages.\n",
    "\n",
    "You have been given some default values for the variables train_val_split,\n",
    "batch_size as well as the transform function.\n",
    "You are encouraged to modify these to improve the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question:**\n",
    "\n",
    "Briefly describe how your program works, and explain any design and training\n",
    "decisions you made along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "######     Specify transform(s) to be applied to the input images     ######\n",
    "############################################################################\n",
    "\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    # channel size = 3\n",
    "\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "    \"\"\"\n",
    "    # Data Augmentation\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                transforms.RandomResizedCrop(size=80, scale=(0.55, 1.0), ratio=(0.75, 1.3)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomPerspective(p=0.2),\n",
    "                transforms.RandomAffine(degrees=(-15, 15), translate=(0.0, 0.5)),\n",
    "                transforms.RandomRotation((-10,10)),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.1, hue=0.02),\n",
    "                transforms.RandomPosterize(bits=3, p=0.3),\n",
    "                transforms.RandomEqualize(p=0.1),\n",
    "                transforms.RandomGrayscale(p=0.01),\n",
    "                transforms.RandomPerspective(distortion_scale=0.05, p=0.1, fill=0),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )\n",
    "    # Keep the testing data original to ensure accuracy\n",
    "    elif mode == 'test':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomResizedCrop(size=(80, 80), scale=(0.55, 1.0), ratio=(0.75, 1.3), interpolation=bilinear)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomPerspective(p=0.2)\n",
      "    RandomAffine(degrees=[-15.0, 15.0], translate=(0.0, 0.5))\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.9, 1.1], hue=[-0.02, 0.02])\n",
      "    RandomPosterize(bits=3,p=0.3)\n",
      "    RandomEqualize(p=0.1)\n",
      "    RandomGrayscale(p=0.01)\n",
      "    RandomPerspective(p=0.1)\n",
      "    RandomAdjustSharpness(sharpness_factor=2,p=0.5)\n",
      "    ToTensor()\n",
      ")\n",
      "VGG12(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0, inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ELU(alpha=1.0, inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ELU(alpha=1.0, inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ELU(alpha=1.0, inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ELU(alpha=1.0, inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ELU(alpha=1.0, inplace=True)\n",
      "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ELU(alpha=1.0, inplace=True)\n",
      "    (27): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ELU(alpha=1.0, inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=3072, out_features=2048, bias=True)\n",
      "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.6, inplace=False)\n",
      "    (5): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.4, inplace=False)\n",
      "    (9): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "+----------------------+------------+\n",
      "|       Modules        | Parameters |\n",
      "+----------------------+------------+\n",
      "| cnn_layers.0.weight  |    1728    |\n",
      "|  cnn_layers.0.bias   |     64     |\n",
      "| cnn_layers.1.weight  |     64     |\n",
      "|  cnn_layers.1.bias   |     64     |\n",
      "| cnn_layers.3.weight  |   36864    |\n",
      "|  cnn_layers.3.bias   |     64     |\n",
      "| cnn_layers.4.weight  |     64     |\n",
      "|  cnn_layers.4.bias   |     64     |\n",
      "| cnn_layers.7.weight  |   73728    |\n",
      "|  cnn_layers.7.bias   |    128     |\n",
      "| cnn_layers.8.weight  |    128     |\n",
      "|  cnn_layers.8.bias   |    128     |\n",
      "| cnn_layers.10.weight |   147456   |\n",
      "|  cnn_layers.10.bias  |    128     |\n",
      "| cnn_layers.11.weight |    128     |\n",
      "|  cnn_layers.11.bias  |    128     |\n",
      "| cnn_layers.14.weight |   294912   |\n",
      "|  cnn_layers.14.bias  |    256     |\n",
      "| cnn_layers.15.weight |    256     |\n",
      "|  cnn_layers.15.bias  |    256     |\n",
      "| cnn_layers.17.weight |   589824   |\n",
      "|  cnn_layers.17.bias  |    256     |\n",
      "| cnn_layers.18.weight |    256     |\n",
      "|  cnn_layers.18.bias  |    256     |\n",
      "| cnn_layers.21.weight |   589824   |\n",
      "|  cnn_layers.21.bias  |    256     |\n",
      "| cnn_layers.22.weight |    256     |\n",
      "|  cnn_layers.22.bias  |    256     |\n",
      "| cnn_layers.24.weight |   589824   |\n",
      "|  cnn_layers.24.bias  |    256     |\n",
      "| cnn_layers.25.weight |    256     |\n",
      "|  cnn_layers.25.bias  |    256     |\n",
      "| cnn_layers.27.weight |   442368   |\n",
      "|  cnn_layers.27.bias  |    192     |\n",
      "| cnn_layers.28.weight |    192     |\n",
      "|  cnn_layers.28.bias  |    192     |\n",
      "|  fc_layers.1.weight  |  6291456   |\n",
      "|   fc_layers.1.bias   |    2048    |\n",
      "|  fc_layers.2.weight  |    2048    |\n",
      "|   fc_layers.2.bias   |    2048    |\n",
      "|  fc_layers.5.weight  |  2097152   |\n",
      "|   fc_layers.5.bias   |    1024    |\n",
      "|  fc_layers.6.weight  |    1024    |\n",
      "|   fc_layers.6.bias   |    1024    |\n",
      "|  fc_layers.9.weight  |    8192    |\n",
      "|   fc_layers.9.bias   |     8      |\n",
      "+----------------------+------------+\n",
      "Total Trainable Params: 11177352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11177352"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "#####                      Specify NN to be used                           ######\n",
    "#################################################################################\n",
    "\n",
    "### Simplified implementation of VGG16 with 12 layers instead of 16.\n",
    "### Cut layer = 256 - 256 conv layer. 512-512 * 3 conv layers at the end.\n",
    "### Reduced number of nodes on FC layer from 4096 to 1024.\n",
    "vgg_12 = [64, 64, 'maxpool', 128, 128, 'maxpool', 256, 256, 'maxpool', 512, 512, 512, 'maxpool', 'avgpool', 'fc1', 'fc2', 'fc3']    \n",
    "##########################################################################################\n",
    "# trying to take some inspirations from vgg16 but with less channels and fc layer nodes. #\n",
    "##########################################################################################\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ######### block 1 #########\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            \n",
    "            ######### block 2 #########\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            ######### block 3 #########   \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "        \n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            \n",
    "            ######### block 4 #########\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 192, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # shrink final conv layer width to 4\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4,4))\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten from conv layers\n",
    "\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(192*4*4, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(1024, 8)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.avgpool(x)       \n",
    "        x = self.fc_layers(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Network()\n",
    "\n",
    "############################################################################\n",
    "######      Specify the optimizer and loss function                   ######\n",
    "############################################################################\n",
    "learning_rate = 0.0005\n",
    "# optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=learning_rate)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# loss_func = F.nll_loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "############################################################################\n",
    "######  Custom weight initialization and lr scheduling are optional   ######\n",
    "############################################################################\n",
    "\n",
    "# Normally, the default weight initialization and fixed learing rate\n",
    "# should work fine. But, we have made it possible for you to define\n",
    "# your own custom weight initialization and lr scheduler, if you wish.\n",
    "def weights_init(m):\n",
    "    return\n",
    "\n",
    "scheduler = None\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#######              Metaparameters and training options              ######\n",
    "############################################################################\n",
    "dataset = \"./data\"\n",
    "train_val_split = 1\n",
    "batch_size = 256 \n",
    "epochs = 500\n",
    "\n",
    "\n",
    "###############################################\n",
    "#**          Print Network Information      **#\n",
    "###############################################\n",
    "print(transform('train'))\n",
    "print(net)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###########################\n",
    "## Cat breed dictionary  ##\n",
    "###########################\n",
    "cat_dict = {\n",
    "    0: 'bombay',\n",
    "    1: 'calico',\n",
    "    2: 'persian',\n",
    "    3: 'russianblue',\n",
    "    4: 'siamese',\n",
    "    5: 'tiger',\n",
    "    6: 'tortoiseshell',\n",
    "    7: 'tuxedo'\n",
    "}\n",
    "\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "# Test network on validation set, if it exists.\n",
    "## Added params\n",
    "def test_network(net,testloader,test_accuracy_list,print_confusion=False):\n",
    "    net.eval()\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "    conf_matrix = np.zeros((8,8))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            conf_matrix = conf_matrix + metrics.confusion_matrix(\n",
    "                labels.cpu(),predicted.cpu(),labels=[0,1,2,3,4,5,6,7])\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    test_accuracy_list.append(model_accuracy)\n",
    "    print(', {0} test {1:.2f}%'.format(total_images,model_accuracy))\n",
    "    if print_confusion:\n",
    "        np.set_printoptions(precision=2, suppress=True)\n",
    "        print(conf_matrix)\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Start training...\n",
      "ep 1, loss: 50.17, 6400 train 22.58%, 1600 test 17.38%\n",
      "ep 2, loss: 46.57, 6400 train 29.09%, 1600 test 34.56%\n",
      "ep 3, loss: 44.86, 6400 train 31.62%, 1600 test 31.25%\n",
      "ep 4, loss: 43.47, 6400 train 33.83%, 1600 test 37.38%\n",
      "ep 5, loss: 42.93, 6400 train 35.09%, 1600 test 27.44%\n",
      "ep 6, loss: 41.88, 6400 train 36.84%, 1600 test 44.44%\n",
      "ep 7, loss: 40.35, 6400 train 39.16%, 1600 test 43.75%\n",
      "ep 8, loss: 39.59, 6400 train 41.23%, 1600 test 41.88%\n",
      "ep 9, loss: 39.13, 6400 train 41.48%, 1600 test 39.81%\n",
      "ep 10, loss: 38.59, 6400 train 42.28%, 1600 test 50.50%\n",
      "[[ 97.   0.   0.  14.   6.   8.  36.  35.]\n",
      " [  0.  99.   2.   4.  26.  22.  34.  11.]\n",
      " [ 13.  25.  40.  25.  56.  24.  22.  11.]\n",
      " [  9.   5.   3. 105.  16.  24.  27.   9.]\n",
      " [  0.  58.   7.   6.  78.   7.   8.  20.]\n",
      " [  0.  12.   9.  10.   6. 148.  16.   0.]\n",
      " [  3.  12.   0.  10.   2.  30. 143.   5.]\n",
      " [ 12.  38.   0.   3.   5.   4.  42.  98.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:03:35.935966\n",
      "ep 11, loss: 38.18, 6400 train 43.42%, 1600 test 48.00%\n",
      "ep 12, loss: 37.73, 6400 train 44.55%, 1600 test 51.25%\n",
      "ep 13, loss: 36.91, 6400 train 45.97%, 1600 test 53.37%\n",
      "ep 14, loss: 36.16, 6400 train 46.84%, 1600 test 51.19%\n",
      "ep 15, loss: 35.86, 6400 train 46.42%, 1600 test 50.12%\n",
      "ep 16, loss: 35.04, 6400 train 48.61%, 1600 test 44.50%\n",
      "ep 17, loss: 34.45, 6400 train 49.19%, 1600 test 55.38%\n",
      "ep 18, loss: 33.52, 6400 train 51.19%, 1600 test 53.62%\n",
      "ep 19, loss: 34.18, 6400 train 49.55%, 1600 test 51.75%\n",
      "ep 20, loss: 33.52, 6400 train 50.58%, 1600 test 50.69%\n",
      "[[102.   1.   1.   6.   0.   0.  15.  71.]\n",
      " [  0.  63.  24.   3.  30.   4.   3.  71.]\n",
      " [ 17.   6.  96.   6.  50.   5.   7.  29.]\n",
      " [ 29.   1.   9.  73.  44.   1.   2.  39.]\n",
      " [  3.   2.  38.   3. 104.   0.   0.  34.]\n",
      " [  3.  23.  29.  28.  11.  89.  11.   7.]\n",
      " [  6.  31.   6.   5.  13.   3.  96.  45.]\n",
      " [  4.   3.   0.   0.   5.   0.   2. 188.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:07:11.406452\n",
      "ep 21, loss: 32.95, 6400 train 51.91%, 1600 test 54.50%\n",
      "ep 22, loss: 33.19, 6400 train 52.06%, 1600 test 54.37%\n",
      "ep 23, loss: 32.51, 6400 train 53.16%, 1600 test 54.25%\n",
      "ep 24, loss: 31.66, 6400 train 54.58%, 1600 test 55.56%\n",
      "ep 25, loss: 31.43, 6400 train 54.28%, 1600 test 55.12%\n",
      "ep 26, loss: 31.06, 6400 train 55.02%, 1600 test 60.62%\n",
      "ep 27, loss: 30.51, 6400 train 55.72%, 1600 test 60.12%\n",
      "ep 28, loss: 30.21, 6400 train 56.52%, 1600 test 60.75%\n",
      "ep 29, loss: 30.43, 6400 train 56.45%, 1600 test 51.44%\n",
      "ep 30, loss: 30.26, 6400 train 56.89%, 1600 test 53.87%\n",
      "[[ 65.   5.   2.   1.   1.   1.  71.  50.]\n",
      " [  0. 161.   2.   0.   2.   5.  22.   6.]\n",
      " [  9.  37.  73.   3.  38.   9.  38.   9.]\n",
      " [ 14.  18.  11.  43.  22.  22.  55.  13.]\n",
      " [  0.  70.  10.   0.  89.   1.   5.   9.]\n",
      " [  0.  28.  12.   0.   0. 127.  34.   0.]\n",
      " [  1.  24.   0.   0.   0.   3. 176.   1.]\n",
      " [  3.  57.   0.   0.   0.   0.  14. 128.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:10:45.690236\n",
      "ep 31, loss: 30.21, 6400 train 56.67%, 1600 test 65.69%\n",
      "ep 32, loss: 29.58, 6400 train 57.23%, 1600 test 63.69%\n",
      "ep 33, loss: 29.35, 6400 train 57.77%, 1600 test 66.88%\n",
      "ep 34, loss: 29.17, 6400 train 58.11%, 1600 test 62.38%\n",
      "ep 35, loss: 28.80, 6400 train 58.92%, 1600 test 51.38%\n",
      "ep 36, loss: 28.30, 6400 train 58.72%, 1600 test 67.50%\n",
      "ep 37, loss: 28.38, 6400 train 59.66%, 1600 test 66.31%\n",
      "ep 38, loss: 28.40, 6400 train 58.75%, 1600 test 60.06%\n",
      "ep 39, loss: 27.99, 6400 train 60.44%, 1600 test 69.75%\n",
      "ep 40, loss: 27.05, 6400 train 61.39%, 1600 test 47.12%\n",
      "[[ 74.  19.   7.  20.   3.   6.  41.  26.]\n",
      " [  0. 192.   1.   0.   1.   3.   0.   1.]\n",
      " [  1. 110.  55.   3.  11.  15.  17.   4.]\n",
      " [  1.  36.   3.  45.  36.  64.  12.   1.]\n",
      " [  0. 118.   4.   0.  59.   2.   1.   0.]\n",
      " [  0.  64.   1.   0.   0. 130.   6.   0.]\n",
      " [  0.  73.   1.   0.   1.   8. 122.   0.]\n",
      " [  2. 112.   0.   3.   4.   3.   1.  77.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:14:32.991446\n",
      "ep 41, loss: 27.06, 6400 train 60.88%, 1600 test 69.56%\n",
      "ep 42, loss: 26.65, 6400 train 62.06%, 1600 test 70.00%\n",
      "ep 43, loss: 26.95, 6400 train 61.84%, 1600 test 72.19%\n",
      "ep 44, loss: 26.78, 6400 train 62.39%, 1600 test 72.69%\n",
      "ep 45, loss: 26.35, 6400 train 62.75%, 1600 test 74.88%\n",
      "ep 46, loss: 26.09, 6400 train 62.55%, 1600 test 74.62%\n",
      "ep 47, loss: 26.19, 6400 train 62.72%, 1600 test 69.44%\n",
      "ep 48, loss: 26.02, 6400 train 62.77%, 1600 test 70.50%\n",
      "ep 49, loss: 26.09, 6400 train 62.78%, 1600 test 67.56%\n",
      "ep 50, loss: 24.94, 6400 train 64.31%, 1600 test 74.94%\n",
      "[[172.   2.   2.   2.   0.   1.  13.   4.]\n",
      " [  0. 133.  12.   0.  18.   4.  25.   6.]\n",
      " [ 22.   9. 130.   6.  18.   7.  17.   7.]\n",
      " [ 20.   1.   8. 144.   6.   2.  15.   2.]\n",
      " [  2.   7.  21.   1. 147.   0.   4.   2.]\n",
      " [  1.   9.   8.   1.   2. 151.  28.   1.]\n",
      " [ 11.  11.   3.   2.   1.   2. 175.   0.]\n",
      " [ 13.  20.   1.   2.   7.   0.  12. 147.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:18:18.730700\n",
      "ep 51, loss: 25.03, 6400 train 63.70%, 1600 test 65.50%\n",
      "ep 52, loss: 25.80, 6400 train 63.89%, 1600 test 70.56%\n",
      "ep 53, loss: 24.91, 6400 train 64.81%, 1600 test 73.94%\n",
      "ep 54, loss: 25.18, 6400 train 64.42%, 1600 test 71.56%\n",
      "ep 55, loss: 25.22, 6400 train 64.39%, 1600 test 75.62%\n",
      "ep 56, loss: 24.32, 6400 train 65.62%, 1600 test 75.12%\n",
      "ep 57, loss: 24.96, 6400 train 65.20%, 1600 test 75.69%\n",
      "ep 58, loss: 24.93, 6400 train 65.00%, 1600 test 77.94%\n",
      "ep 59, loss: 24.33, 6400 train 65.28%, 1600 test 76.44%\n",
      "ep 60, loss: 23.90, 6400 train 66.16%, 1600 test 76.56%\n",
      "[[156.   3.   9.  16.   0.   0.   3.   9.]\n",
      " [  0. 149.   8.   1.   6.  14.   4.  16.]\n",
      " [ 15.  10. 125.  10.  17.  27.   3.   9.]\n",
      " [  2.   1.   9. 148.  14.  16.   2.   6.]\n",
      " [  1.  13.  16.   3. 141.   4.   0.   6.]\n",
      " [  0.   6.   4.   0.   1. 185.   3.   2.]\n",
      " [  9.  18.   7.   4.   1.  10. 152.   4.]\n",
      " [  9.  16.   2.   2.   3.   0.   1. 169.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:22:04.419172\n",
      "ep 61, loss: 23.69, 6400 train 66.73%, 1600 test 74.50%\n",
      "ep 62, loss: 23.93, 6400 train 65.53%, 1600 test 76.19%\n",
      "ep 63, loss: 23.50, 6400 train 67.11%, 1600 test 69.06%\n",
      "ep 64, loss: 23.52, 6400 train 66.44%, 1600 test 77.25%\n",
      "ep 65, loss: 22.60, 6400 train 67.81%, 1600 test 77.12%\n",
      "ep 66, loss: 22.90, 6400 train 67.64%, 1600 test 78.06%\n",
      "ep 67, loss: 22.54, 6400 train 68.30%, 1600 test 78.38%\n",
      "ep 68, loss: 22.50, 6400 train 68.62%, 1600 test 73.56%\n",
      "ep 69, loss: 22.64, 6400 train 68.19%, 1600 test 70.62%\n",
      "ep 70, loss: 22.67, 6400 train 67.84%, 1600 test 78.44%\n",
      "[[170.   2.   5.  11.   2.   0.   3.   3.]\n",
      " [  0. 128.   8.   3.  20.  13.   8.  18.]\n",
      " [ 14.   5. 126.   8.  36.  11.   6.  10.]\n",
      " [ 10.   0.   3. 170.   8.   2.   3.   2.]\n",
      " [  3.   0.   9.   4. 166.   0.   0.   2.]\n",
      " [  0.   4.   7.   8.   8. 169.   3.   2.]\n",
      " [  9.   7.   9.   9.   8.   3. 158.   2.]\n",
      " [ 21.   4.   1.   1.   6.   0.   1. 168.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:25:48.376954\n",
      "ep 71, loss: 21.97, 6400 train 68.81%, 1600 test 78.94%\n",
      "ep 72, loss: 21.96, 6400 train 68.77%, 1600 test 70.94%\n",
      "ep 73, loss: 22.46, 6400 train 68.61%, 1600 test 76.44%\n",
      "ep 74, loss: 22.05, 6400 train 69.62%, 1600 test 71.69%\n",
      "ep 75, loss: 22.24, 6400 train 68.94%, 1600 test 78.31%\n",
      "ep 76, loss: 21.67, 6400 train 69.33%, 1600 test 77.25%\n",
      "ep 77, loss: 21.66, 6400 train 70.30%, 1600 test 77.94%\n",
      "ep 78, loss: 21.71, 6400 train 69.48%, 1600 test 77.56%\n",
      "ep 79, loss: 21.21, 6400 train 69.95%, 1600 test 78.44%\n",
      "ep 80, loss: 21.17, 6400 train 70.03%, 1600 test 80.75%\n",
      "[[175.   1.   3.   8.   0.   1.   5.   3.]\n",
      " [  2. 138.   6.   1.  15.  16.  11.   9.]\n",
      " [ 14.   5. 116.  10.  33.  23.   8.   7.]\n",
      " [  8.   0.   1. 174.   3.  11.   1.   0.]\n",
      " [  3.   2.   6.   5. 166.   0.   0.   2.]\n",
      " [  0.   3.   0.   2.   2. 189.   4.   1.]\n",
      " [ 11.   8.   3.   8.   0.   5. 168.   2.]\n",
      " [ 19.  10.   1.   0.   4.   0.   2. 166.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:29:32.638547\n",
      "ep 81, loss: 21.58, 6400 train 69.95%, 1600 test 76.69%\n",
      "ep 82, loss: 21.21, 6400 train 69.73%, 1600 test 76.88%\n",
      "ep 83, loss: 21.17, 6400 train 70.05%, 1600 test 77.88%\n",
      "ep 84, loss: 20.45, 6400 train 70.83%, 1600 test 78.44%\n",
      "ep 85, loss: 20.60, 6400 train 70.97%, 1600 test 76.69%\n",
      "ep 86, loss: 20.05, 6400 train 71.88%, 1600 test 80.88%\n",
      "ep 87, loss: 20.59, 6400 train 70.58%, 1600 test 80.38%\n",
      "ep 88, loss: 20.63, 6400 train 71.78%, 1600 test 81.38%\n",
      "ep 89, loss: 20.62, 6400 train 70.97%, 1600 test 78.75%\n",
      "ep 90, loss: 20.06, 6400 train 72.19%, 1600 test 77.75%\n",
      "[[147.   1.   4.   8.   0.   3.  30.   3.]\n",
      " [  0. 134.   1.   0.   4.  30.  23.   6.]\n",
      " [  8.   9. 115.   5.  14.  42.  14.   9.]\n",
      " [  2.   0.   1. 152.   2.  28.  10.   3.]\n",
      " [  0.   8.   4.   2. 154.   9.   5.   2.]\n",
      " [  0.   1.   0.   0.   0. 192.   8.   0.]\n",
      " [  0.   8.   1.   2.   0.   7. 187.   0.]\n",
      " [  8.  12.   1.   1.   1.   3.  13. 163.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:33:18.816293\n",
      "ep 91, loss: 19.59, 6400 train 72.72%, 1600 test 80.50%\n",
      "ep 92, loss: 20.05, 6400 train 72.00%, 1600 test 80.06%\n",
      "ep 93, loss: 19.70, 6400 train 72.89%, 1600 test 79.62%\n",
      "ep 94, loss: 19.42, 6400 train 72.41%, 1600 test 80.75%\n",
      "ep 95, loss: 19.38, 6400 train 73.16%, 1600 test 79.50%\n",
      "ep 96, loss: 19.42, 6400 train 72.19%, 1600 test 77.38%\n",
      "ep 97, loss: 19.43, 6400 train 72.98%, 1600 test 80.12%\n",
      "ep 98, loss: 19.53, 6400 train 73.16%, 1600 test 78.94%\n",
      "ep 99, loss: 18.99, 6400 train 73.53%, 1600 test 82.88%\n",
      "ep 100, loss: 19.24, 6400 train 73.08%, 1600 test 76.81%\n",
      "[[142.   3.   2.   0.   0.   2.  34.  13.]\n",
      " [  0. 168.   1.   0.   2.   6.  15.   6.]\n",
      " [  5.  21. 129.   5.  14.  18.  13.  11.]\n",
      " [  7.   1.   7. 114.   3.  22.  38.   6.]\n",
      " [  2.  25.   6.   1. 141.   3.   5.   1.]\n",
      " [  0.   9.   1.   0.   0. 176.  15.   0.]\n",
      " [  2.   9.   0.   0.   0.   3. 190.   1.]\n",
      " [  2.  21.   1.   0.   1.   0.   8. 169.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:36:57.951967\n",
      "ep 101, loss: 19.24, 6400 train 73.03%, 1600 test 79.81%\n",
      "ep 102, loss: 18.74, 6400 train 73.61%, 1600 test 81.81%\n",
      "ep 103, loss: 18.51, 6400 train 74.14%, 1600 test 79.62%\n",
      "ep 104, loss: 18.90, 6400 train 73.47%, 1600 test 80.38%\n",
      "ep 105, loss: 18.86, 6400 train 73.50%, 1600 test 83.25%\n",
      "ep 106, loss: 18.50, 6400 train 74.23%, 1600 test 79.81%\n",
      "ep 107, loss: 18.82, 6400 train 73.61%, 1600 test 81.62%\n",
      "ep 108, loss: 18.31, 6400 train 74.20%, 1600 test 82.44%\n",
      "ep 109, loss: 18.70, 6400 train 74.16%, 1600 test 81.56%\n",
      "ep 110, loss: 18.63, 6400 train 73.92%, 1600 test 82.25%\n",
      "[[160.   4.   8.   8.   2.   1.   8.   5.]\n",
      " [  0. 175.   4.   0.   3.   4.   5.   7.]\n",
      " [  5.  14. 156.   5.  12.  12.   4.   8.]\n",
      " [  1.   5.   7. 149.  15.  15.   2.   4.]\n",
      " [  0.  13.  14.   0. 156.   0.   0.   1.]\n",
      " [  0.  17.   1.   0.   2. 178.   2.   1.]\n",
      " [  5.  22.   7.   2.   1.   4. 163.   1.]\n",
      " [  9.  11.   1.   1.   0.   0.   1. 179.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:40:43.222511\n",
      "ep 111, loss: 18.15, 6400 train 74.77%, 1600 test 82.31%\n",
      "ep 112, loss: 18.33, 6400 train 73.69%, 1600 test 82.00%\n",
      "ep 113, loss: 18.03, 6400 train 74.95%, 1600 test 80.31%\n",
      "ep 114, loss: 18.11, 6400 train 74.42%, 1600 test 82.69%\n",
      "ep 115, loss: 18.28, 6400 train 74.45%, 1600 test 84.31%\n",
      "ep 116, loss: 17.82, 6400 train 74.72%, 1600 test 82.94%\n",
      "ep 117, loss: 17.73, 6400 train 74.78%, 1600 test 82.94%\n",
      "ep 118, loss: 17.86, 6400 train 75.56%, 1600 test 79.75%\n",
      "ep 119, loss: 17.74, 6400 train 74.75%, 1600 test 82.75%\n",
      "ep 120, loss: 17.61, 6400 train 75.23%, 1600 test 81.00%\n",
      "[[174.   1.   0.   5.   1.   6.   6.   3.]\n",
      " [  0. 145.   1.   1.   4.  23.  12.  12.]\n",
      " [ 10.  11. 114.   7.  18.  42.   8.   6.]\n",
      " [  5.   0.   0. 160.   4.  19.   5.   5.]\n",
      " [  0.   6.   2.   1. 161.  12.   1.   1.]\n",
      " [  0.   4.   0.   0.   0. 194.   2.   1.]\n",
      " [  7.  11.   0.   2.   0.   8. 174.   3.]\n",
      " [  9.  10.   1.   0.   3.   2.   3. 174.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:44:25.372930\n",
      "ep 121, loss: 17.78, 6400 train 75.62%, 1600 test 82.31%\n",
      "ep 122, loss: 17.87, 6400 train 75.41%, 1600 test 84.69%\n",
      "ep 123, loss: 17.84, 6400 train 75.70%, 1600 test 82.31%\n",
      "ep 124, loss: 17.55, 6400 train 75.50%, 1600 test 81.44%\n",
      "ep 125, loss: 17.45, 6400 train 75.48%, 1600 test 83.44%\n",
      "ep 126, loss: 17.14, 6400 train 76.77%, 1600 test 83.88%\n",
      "ep 127, loss: 17.25, 6400 train 75.84%, 1600 test 82.19%\n",
      "ep 128, loss: 16.70, 6400 train 76.70%, 1600 test 83.38%\n",
      "ep 129, loss: 17.84, 6400 train 74.70%, 1600 test 81.75%\n",
      "ep 130, loss: 16.57, 6400 train 76.56%, 1600 test 83.62%\n",
      "[[171.   1.   0.  13.   0.   2.   5.   4.]\n",
      " [  0. 139.   1.   1.  15.  23.  13.   6.]\n",
      " [  7.   9. 128.  12.  26.  24.   5.   5.]\n",
      " [  2.   1.   1. 183.   0.  10.   1.   0.]\n",
      " [  0.   1.   3.   3. 172.   3.   0.   2.]\n",
      " [  0.   0.   0.   0.   3. 196.   1.   1.]\n",
      " [  5.  12.   0.   7.   0.   9. 172.   0.]\n",
      " [  7.  12.   0.   2.   2.   0.   2. 177.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:48:07.020144\n",
      "ep 131, loss: 16.96, 6400 train 75.81%, 1600 test 84.12%\n",
      "ep 132, loss: 16.70, 6400 train 77.00%, 1600 test 83.31%\n",
      "ep 133, loss: 16.45, 6400 train 77.06%, 1600 test 84.69%\n",
      "ep 134, loss: 16.94, 6400 train 76.88%, 1600 test 84.38%\n",
      "ep 135, loss: 16.41, 6400 train 76.59%, 1600 test 82.81%\n",
      "ep 136, loss: 16.90, 6400 train 76.30%, 1600 test 81.31%\n",
      "ep 137, loss: 16.38, 6400 train 76.17%, 1600 test 84.31%\n",
      "ep 138, loss: 16.34, 6400 train 76.64%, 1600 test 82.62%\n",
      "ep 139, loss: 16.41, 6400 train 76.91%, 1600 test 82.94%\n",
      "ep 140, loss: 16.46, 6400 train 76.77%, 1600 test 85.12%\n",
      "[[177.   1.   2.   1.   1.   1.  11.   2.]\n",
      " [  0. 156.   6.   0.   7.   7.  12.  10.]\n",
      " [  8.   7. 168.   2.   8.  14.   6.   3.]\n",
      " [ 12.   0.   4. 154.   8.  11.   6.   3.]\n",
      " [  0.   5.  12.   3. 160.   2.   0.   2.]\n",
      " [  0.   7.   0.   1.   1. 189.   2.   1.]\n",
      " [  4.  10.   3.   2.   1.   6. 178.   1.]\n",
      " [  9.   7.   2.   0.   1.   1.   2. 180.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:52:01.306419\n",
      "ep 141, loss: 16.89, 6400 train 76.11%, 1600 test 83.56%\n",
      "ep 142, loss: 16.53, 6400 train 76.84%, 1600 test 84.88%\n",
      "ep 143, loss: 16.29, 6400 train 77.34%, 1600 test 81.88%\n",
      "ep 144, loss: 15.92, 6400 train 78.00%, 1600 test 84.50%\n",
      "ep 145, loss: 15.89, 6400 train 77.89%, 1600 test 85.06%\n",
      "ep 146, loss: 16.34, 6400 train 77.03%, 1600 test 85.19%\n",
      "ep 147, loss: 15.57, 6400 train 78.34%, 1600 test 83.25%\n",
      "ep 148, loss: 15.62, 6400 train 77.80%, 1600 test 83.88%\n",
      "ep 149, loss: 15.87, 6400 train 77.77%, 1600 test 84.56%\n",
      "ep 150, loss: 16.27, 6400 train 77.56%, 1600 test 84.81%\n",
      "[[182.   1.   2.   3.   1.   0.   4.   3.]\n",
      " [  0. 136.   8.   0.   7.  12.  13.  22.]\n",
      " [  8.   5. 156.   6.  11.  20.   6.   4.]\n",
      " [ 11.   0.   2. 164.   3.   6.   5.   7.]\n",
      " [  1.   1.   5.   1. 169.   4.   0.   3.]\n",
      " [  0.   3.   0.   2.   0. 187.   6.   3.]\n",
      " [  8.   8.   1.   3.   0.   6. 178.   1.]\n",
      " [  9.   5.   1.   0.   1.   0.   1. 185.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:55:24.679918\n",
      "ep 151, loss: 16.32, 6400 train 77.72%, 1600 test 82.88%\n",
      "ep 152, loss: 15.94, 6400 train 78.14%, 1600 test 85.12%\n",
      "ep 153, loss: 15.36, 6400 train 78.72%, 1600 test 84.62%\n",
      "ep 154, loss: 15.82, 6400 train 78.00%, 1600 test 82.75%\n",
      "ep 155, loss: 16.15, 6400 train 77.22%, 1600 test 85.62%\n",
      "ep 156, loss: 15.98, 6400 train 78.12%, 1600 test 84.62%\n",
      "ep 157, loss: 15.32, 6400 train 78.36%, 1600 test 82.75%\n",
      "ep 158, loss: 15.74, 6400 train 78.36%, 1600 test 84.62%\n",
      "ep 159, loss: 15.99, 6400 train 77.56%, 1600 test 85.94%\n",
      "ep 160, loss: 15.05, 6400 train 78.67%, 1600 test 84.00%\n",
      "[[174.   1.   1.   1.   1.   0.   3.  15.]\n",
      " [  0. 134.   5.   0.  13.  10.  13.  23.]\n",
      " [  8.   0. 148.   4.  28.  11.   4.  13.]\n",
      " [  4.   0.   5. 159.  17.   3.   2.   8.]\n",
      " [  0.   0.   3.   0. 175.   0.   1.   5.]\n",
      " [  0.   0.   3.   3.   4. 183.   5.   3.]\n",
      " [  7.   7.   1.   5.   0.   3. 177.   5.]\n",
      " [  1.   3.   0.   0.   3.   0.   1. 194.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:59:05.390340\n",
      "ep 161, loss: 14.99, 6400 train 79.27%, 1600 test 82.56%\n",
      "ep 162, loss: 15.39, 6400 train 78.44%, 1600 test 84.69%\n",
      "ep 163, loss: 15.33, 6400 train 78.59%, 1600 test 84.06%\n",
      "ep 164, loss: 15.11, 6400 train 78.34%, 1600 test 84.88%\n",
      "ep 165, loss: 15.58, 6400 train 77.92%, 1600 test 82.88%\n",
      "ep 166, loss: 15.83, 6400 train 77.98%, 1600 test 84.19%\n",
      "ep 167, loss: 15.50, 6400 train 78.89%, 1600 test 83.38%\n",
      "ep 168, loss: 15.31, 6400 train 78.59%, 1600 test 84.06%\n",
      "ep 169, loss: 14.52, 6400 train 79.41%, 1600 test 84.25%\n",
      "ep 170, loss: 15.08, 6400 train 78.69%, 1600 test 84.38%\n",
      "[[185.   0.   1.   5.   0.   0.   4.   1.]\n",
      " [  0. 135.   6.   1.  13.  15.   6.  22.]\n",
      " [ 10.   3. 164.   6.   8.  17.   2.   6.]\n",
      " [  9.   0.   1. 176.   1.   6.   1.   4.]\n",
      " [  1.   1.  17.   3. 159.   0.   0.   3.]\n",
      " [  0.   0.   3.   2.   2. 190.   2.   2.]\n",
      " [ 25.   9.   3.   5.   0.   9. 152.   2.]\n",
      " [  4.   4.   1.   1.   3.   0.   0. 189.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:02:47.211308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 171, loss: 14.70, 6400 train 79.62%, 1600 test 85.38%\n",
      "ep 172, loss: 14.59, 6400 train 79.84%, 1600 test 85.88%\n",
      "ep 173, loss: 14.50, 6400 train 78.94%, 1600 test 84.81%\n",
      "ep 174, loss: 14.78, 6400 train 79.25%, 1600 test 84.88%\n",
      "ep 175, loss: 14.78, 6400 train 78.80%, 1600 test 84.88%\n",
      "ep 176, loss: 14.35, 6400 train 79.88%, 1600 test 85.75%\n",
      "ep 177, loss: 14.83, 6400 train 78.97%, 1600 test 83.94%\n",
      "ep 178, loss: 15.09, 6400 train 79.06%, 1600 test 83.31%\n",
      "ep 179, loss: 14.10, 6400 train 80.94%, 1600 test 86.62%\n",
      "ep 180, loss: 14.15, 6400 train 79.80%, 1600 test 84.25%\n",
      "[[171.   2.   3.   5.   0.   1.   7.   7.]\n",
      " [  0. 123.   8.   1.   6.  21.  14.  25.]\n",
      " [  4.   4. 166.   4.  11.  16.   2.   9.]\n",
      " [  3.   0.   4. 173.   3.   6.   4.   5.]\n",
      " [  0.   2.   8.   3. 164.   4.   0.   3.]\n",
      " [  0.   0.   5.   1.   1. 188.   4.   2.]\n",
      " [  4.  11.   4.   3.   0.   6. 173.   4.]\n",
      " [  3.   2.   2.   1.   2.   1.   1. 190.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:05:48.622618\n",
      "ep 181, loss: 14.32, 6400 train 80.36%, 1600 test 84.12%\n",
      "ep 182, loss: 14.51, 6400 train 79.86%, 1600 test 85.38%\n",
      "ep 183, loss: 14.41, 6400 train 80.22%, 1600 test 84.75%\n",
      "ep 184, loss: 14.27, 6400 train 80.28%, 1600 test 86.25%\n",
      "ep 185, loss: 14.15, 6400 train 80.19%, 1600 test 83.81%\n",
      "ep 186, loss: 14.08, 6400 train 80.17%, 1600 test 83.06%\n",
      "ep 187, loss: 14.43, 6400 train 79.16%, 1600 test 83.44%\n",
      "ep 188, loss: 14.06, 6400 train 80.45%, 1600 test 84.69%\n",
      "ep 189, loss: 14.51, 6400 train 79.69%, 1600 test 85.75%\n",
      "ep 190, loss: 14.06, 6400 train 80.05%, 1600 test 86.56%\n",
      "[[178.   2.   1.   5.   0.   0.   8.   2.]\n",
      " [  0. 158.   1.   0.   4.   9.   9.  17.]\n",
      " [ 10.  10. 146.   5.  17.  17.   5.   6.]\n",
      " [  4.   1.   2. 176.   4.   3.   3.   5.]\n",
      " [  1.   2.   4.   3. 169.   1.   0.   4.]\n",
      " [  0.   3.   1.   2.   3. 186.   4.   2.]\n",
      " [  5.   9.   1.   2.   0.   3. 183.   2.]\n",
      " [  7.   5.   0.   0.   0.   0.   1. 189.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:08:50.121555\n",
      "ep 191, loss: 13.71, 6400 train 80.70%, 1600 test 85.94%\n",
      "ep 192, loss: 13.81, 6400 train 80.84%, 1600 test 84.44%\n",
      "ep 193, loss: 13.93, 6400 train 80.42%, 1600 test 85.88%\n",
      "ep 194, loss: 13.71, 6400 train 80.86%, 1600 test 87.56%\n",
      "ep 195, loss: 13.80, 6400 train 80.78%, 1600 test 84.94%\n",
      "ep 196, loss: 13.53, 6400 train 81.31%, 1600 test 86.12%\n",
      "ep 197, loss: 13.95, 6400 train 80.25%, 1600 test 85.06%\n",
      "ep 198, loss: 13.81, 6400 train 80.86%, 1600 test 86.94%\n",
      "ep 199, loss: 13.80, 6400 train 80.84%, 1600 test 83.62%\n",
      "ep 200, loss: 14.28, 6400 train 79.88%, 1600 test 85.06%\n",
      "[[164.   2.   0.  16.   1.   1.   2.  10.]\n",
      " [  0. 144.   2.   0.   9.  14.   4.  25.]\n",
      " [  5.   4. 143.   7.  33.  13.   1.  10.]\n",
      " [  1.   0.   0. 184.   2.   4.   1.   6.]\n",
      " [  0.   2.   2.   1. 174.   1.   0.   4.]\n",
      " [  0.   2.   0.   3.   4. 189.   1.   2.]\n",
      " [  4.   9.   2.   7.   0.   6. 168.   9.]\n",
      " [  1.   1.   1.   0.   3.   0.   1. 195.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:11:49.588749\n",
      "ep 201, loss: 13.55, 6400 train 81.14%, 1600 test 85.69%\n",
      "ep 202, loss: 13.68, 6400 train 81.23%, 1600 test 85.50%\n",
      "ep 203, loss: 13.87, 6400 train 80.02%, 1600 test 86.88%\n",
      "ep 204, loss: 13.79, 6400 train 80.41%, 1600 test 85.25%\n",
      "ep 205, loss: 13.65, 6400 train 80.98%, 1600 test 84.81%\n",
      "ep 206, loss: 13.44, 6400 train 80.70%, 1600 test 85.94%\n",
      "ep 207, loss: 13.76, 6400 train 80.86%, 1600 test 83.94%\n",
      "ep 208, loss: 13.13, 6400 train 81.39%, 1600 test 86.50%\n",
      "ep 209, loss: 13.66, 6400 train 80.75%, 1600 test 85.94%\n",
      "ep 210, loss: 13.73, 6400 train 80.59%, 1600 test 84.06%\n",
      "[[171.   0.   3.   7.   2.   1.   3.   9.]\n",
      " [  0. 136.   1.   0.  12.  10.   4.  35.]\n",
      " [  3.   5. 148.   5.  26.  10.   2.  17.]\n",
      " [  6.   0.   3. 171.   6.   2.   0.  10.]\n",
      " [  0.   1.   3.   0. 175.   1.   0.   4.]\n",
      " [  0.   2.   1.   5.   4. 183.   3.   3.]\n",
      " [  5.   7.   1.   4.   2.   5. 168.  13.]\n",
      " [  2.   5.   0.   0.   2.   0.   0. 193.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:14:55.136487\n",
      "ep 211, loss: 13.75, 6400 train 80.92%, 1600 test 86.06%\n",
      "ep 212, loss: 13.31, 6400 train 81.33%, 1600 test 84.69%\n",
      "ep 213, loss: 13.36, 6400 train 81.22%, 1600 test 87.12%\n",
      "ep 214, loss: 13.18, 6400 train 81.47%, 1600 test 85.88%\n",
      "ep 215, loss: 13.23, 6400 train 81.67%, 1600 test 86.75%\n",
      "ep 216, loss: 12.34, 6400 train 82.53%, 1600 test 87.00%\n",
      "ep 217, loss: 12.80, 6400 train 82.02%, 1600 test 87.12%\n",
      "ep 218, loss: 13.18, 6400 train 80.61%, 1600 test 84.56%\n",
      "ep 219, loss: 13.22, 6400 train 81.27%, 1600 test 84.94%\n",
      "ep 220, loss: 13.05, 6400 train 81.80%, 1600 test 85.62%\n",
      "[[178.   0.   1.   8.   0.   0.   4.   5.]\n",
      " [  0. 132.   2.   4.   4.  18.  10.  28.]\n",
      " [ 11.   3. 155.  13.  18.   9.   1.   6.]\n",
      " [  4.   0.   1. 184.   0.   4.   0.   5.]\n",
      " [  1.   2.   6.   7. 160.   3.   0.   5.]\n",
      " [  0.   1.   0.   2.   1. 191.   3.   3.]\n",
      " [  6.   5.   2.   4.   0.   5. 177.   6.]\n",
      " [  3.   2.   1.   1.   1.   0.   1. 193.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:18:05.747555\n",
      "ep 221, loss: 12.84, 6400 train 82.14%, 1600 test 86.56%\n",
      "ep 222, loss: 12.96, 6400 train 81.52%, 1600 test 86.31%\n",
      "ep 223, loss: 13.30, 6400 train 81.44%, 1600 test 86.75%\n",
      "ep 224, loss: 12.93, 6400 train 82.03%, 1600 test 87.31%\n",
      "ep 225, loss: 13.04, 6400 train 82.19%, 1600 test 86.12%\n",
      "ep 226, loss: 12.47, 6400 train 82.12%, 1600 test 85.19%\n",
      "ep 227, loss: 13.21, 6400 train 81.33%, 1600 test 86.69%\n",
      "ep 228, loss: 12.76, 6400 train 82.12%, 1600 test 85.12%\n",
      "ep 229, loss: 13.15, 6400 train 81.53%, 1600 test 87.25%\n",
      "ep 230, loss: 12.87, 6400 train 81.83%, 1600 test 86.44%\n",
      "[[169.   1.   6.   2.   0.   2.  11.   5.]\n",
      " [  0. 154.   2.   1.   7.  13.  16.   5.]\n",
      " [  4.   6. 173.   8.   9.   8.   3.   5.]\n",
      " [  6.   0.   6. 170.   1.   4.   3.   8.]\n",
      " [  1.   4.  12.   3. 161.   1.   0.   2.]\n",
      " [  0.   0.   5.   2.   0. 188.   5.   1.]\n",
      " [  2.  10.   2.   4.   0.   6. 181.   0.]\n",
      " [  2.  10.   1.   0.   1.   0.   1. 187.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:21:19.637557\n",
      "ep 231, loss: 12.50, 6400 train 82.06%, 1600 test 86.81%\n",
      "ep 232, loss: 12.31, 6400 train 82.16%, 1600 test 85.06%\n",
      "ep 233, loss: 12.77, 6400 train 82.16%, 1600 test 85.06%\n",
      "ep 234, loss: 12.52, 6400 train 82.23%, 1600 test 87.19%\n",
      "ep 235, loss: 12.39, 6400 train 82.72%, 1600 test 85.50%\n",
      "ep 236, loss: 12.11, 6400 train 83.05%, 1600 test 87.69%\n",
      "ep 237, loss: 11.70, 6400 train 83.89%, 1600 test 86.88%\n",
      "ep 238, loss: 12.54, 6400 train 82.66%, 1600 test 86.62%\n",
      "ep 239, loss: 12.15, 6400 train 82.47%, 1600 test 86.50%\n",
      "ep 240, loss: 12.22, 6400 train 82.69%, 1600 test 85.75%\n",
      "[[177.   2.   1.   2.   0.   1.   6.   7.]\n",
      " [  0. 158.   1.   1.   4.  12.  16.   6.]\n",
      " [  8.   8. 143.   3.  18.  24.   8.   4.]\n",
      " [ 12.   1.   1. 169.   3.   3.   1.   8.]\n",
      " [  1.   6.   2.   0. 165.   4.   0.   6.]\n",
      " [  0.   4.   1.   1.   1. 191.   2.   1.]\n",
      " [  5.   8.   0.   3.   0.   5. 181.   3.]\n",
      " [  2.   6.   1.   0.   1.   0.   4. 188.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:24:36.573230\n",
      "ep 241, loss: 12.24, 6400 train 82.52%, 1600 test 86.50%\n",
      "ep 242, loss: 12.06, 6400 train 83.33%, 1600 test 86.94%\n",
      "ep 243, loss: 12.79, 6400 train 82.33%, 1600 test 86.75%\n",
      "ep 244, loss: 12.56, 6400 train 82.33%, 1600 test 85.88%\n",
      "ep 245, loss: 12.21, 6400 train 82.64%, 1600 test 84.00%\n",
      "ep 246, loss: 11.74, 6400 train 83.45%, 1600 test 86.69%\n",
      "ep 247, loss: 12.26, 6400 train 82.72%, 1600 test 86.12%\n",
      "ep 248, loss: 12.17, 6400 train 82.94%, 1600 test 86.94%\n",
      "ep 249, loss: 12.33, 6400 train 82.83%, 1600 test 85.69%\n",
      "ep 250, loss: 11.48, 6400 train 83.61%, 1600 test 86.94%\n",
      "[[170.   2.   4.   2.   2.   1.   4.  11.]\n",
      " [  0. 160.   2.   0.   7.  11.   9.   9.]\n",
      " [  5.   5. 160.   2.  17.  17.   1.   9.]\n",
      " [  3.   2.   5. 170.   7.   1.   5.   5.]\n",
      " [  0.   2.   3.   2. 174.   0.   0.   3.]\n",
      " [  0.   2.   0.   2.   1. 191.   4.   1.]\n",
      " [  3.  11.   2.   3.   0.   5. 178.   3.]\n",
      " [  3.   6.   0.   0.   3.   0.   2. 188.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:27:58.624532\n",
      "ep 251, loss: 11.53, 6400 train 83.59%, 1600 test 86.75%\n",
      "ep 252, loss: 12.17, 6400 train 83.00%, 1600 test 87.06%\n",
      "ep 253, loss: 11.43, 6400 train 84.02%, 1600 test 86.50%\n",
      "ep 254, loss: 12.04, 6400 train 83.02%, 1600 test 87.12%\n",
      "ep 255, loss: 11.81, 6400 train 83.16%, 1600 test 86.62%\n",
      "ep 256, loss: 12.23, 6400 train 83.16%, 1600 test 86.19%\n",
      "ep 257, loss: 12.31, 6400 train 83.08%, 1600 test 86.88%\n",
      "ep 258, loss: 11.35, 6400 train 83.55%, 1600 test 85.94%\n",
      "ep 259, loss: 11.66, 6400 train 83.81%, 1600 test 86.69%\n",
      "ep 260, loss: 11.86, 6400 train 83.41%, 1600 test 86.25%\n",
      "[[174.   2.   4.   5.   0.   0.   8.   3.]\n",
      " [  0. 152.   0.   1.  11.  10.  13.  11.]\n",
      " [  7.   7. 162.   7.  18.   7.   5.   3.]\n",
      " [ 12.   1.   2. 168.   2.   5.   3.   5.]\n",
      " [  1.   4.   3.   1. 172.   1.   0.   2.]\n",
      " [  0.   4.   1.   1.   3. 188.   3.   1.]\n",
      " [  5.  11.   1.   4.   0.   3. 179.   2.]\n",
      " [  5.   7.   1.   0.   3.   0.   1. 185.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:31:11.284305\n",
      "ep 261, loss: 12.62, 6400 train 81.94%, 1600 test 87.88%\n",
      "ep 262, loss: 11.89, 6400 train 83.62%, 1600 test 86.06%\n",
      "ep 263, loss: 11.97, 6400 train 82.91%, 1600 test 86.50%\n",
      "ep 264, loss: 11.51, 6400 train 84.08%, 1600 test 87.19%\n",
      "ep 265, loss: 11.12, 6400 train 84.39%, 1600 test 88.06%\n",
      "ep 266, loss: 11.57, 6400 train 84.22%, 1600 test 87.56%\n",
      "ep 267, loss: 11.45, 6400 train 83.83%, 1600 test 84.62%\n",
      "ep 268, loss: 11.87, 6400 train 83.45%, 1600 test 86.62%\n",
      "ep 269, loss: 11.50, 6400 train 83.89%, 1600 test 87.06%\n",
      "ep 270, loss: 12.28, 6400 train 82.83%, 1600 test 86.00%\n",
      "[[173.   2.   1.   7.   0.   1.   5.   7.]\n",
      " [  0. 157.   1.   1.   2.  18.   7.  12.]\n",
      " [  8.   9. 148.   6.  15.  22.   3.   5.]\n",
      " [  3.   2.   0. 176.   2.   9.   0.   6.]\n",
      " [  1.   9.   5.   3. 160.   1.   1.   4.]\n",
      " [  0.   2.   0.   1.   0. 195.   1.   2.]\n",
      " [  3.  14.   0.   4.   0.   5. 176.   3.]\n",
      " [  2.   5.   0.   2.   1.   0.   1. 191.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:34:47.449129\n",
      "ep 271, loss: 11.68, 6400 train 83.39%, 1600 test 86.56%\n",
      "ep 272, loss: 10.88, 6400 train 84.80%, 1600 test 87.06%\n",
      "ep 273, loss: 11.53, 6400 train 84.17%, 1600 test 87.88%\n",
      "ep 274, loss: 11.13, 6400 train 84.03%, 1600 test 86.06%\n",
      "ep 275, loss: 11.38, 6400 train 83.28%, 1600 test 86.62%\n",
      "ep 276, loss: 11.26, 6400 train 84.12%, 1600 test 87.06%\n",
      "ep 277, loss: 11.73, 6400 train 83.28%, 1600 test 87.25%\n",
      "ep 278, loss: 11.37, 6400 train 84.70%, 1600 test 85.38%\n",
      "ep 279, loss: 11.19, 6400 train 84.47%, 1600 test 87.06%\n",
      "ep 280, loss: 11.24, 6400 train 84.17%, 1600 test 87.50%\n",
      "[[181.   2.   1.   5.   0.   0.   6.   1.]\n",
      " [  0. 158.   5.   1.   2.  10.  10.  12.]\n",
      " [  8.   5. 172.   7.   6.   8.   4.   6.]\n",
      " [  6.   1.   4. 177.   1.   1.   3.   5.]\n",
      " [  1.  10.  10.   1. 156.   1.   0.   5.]\n",
      " [  0.   2.   1.   2.   2. 191.   1.   2.]\n",
      " [  6.  11.   4.   5.   0.   2. 176.   1.]\n",
      " [  7.   5.   0.   0.   0.   1.   0. 189.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:38:41.924460\n",
      "ep 281, loss: 10.57, 6400 train 84.73%, 1600 test 87.12%\n",
      "ep 282, loss: 10.81, 6400 train 84.64%, 1600 test 85.75%\n",
      "ep 283, loss: 11.81, 6400 train 83.20%, 1600 test 87.00%\n",
      "ep 284, loss: 10.76, 6400 train 84.92%, 1600 test 85.56%\n",
      "ep 285, loss: 11.45, 6400 train 83.94%, 1600 test 86.44%\n",
      "ep 286, loss: 11.18, 6400 train 84.39%, 1600 test 86.00%\n",
      "ep 287, loss: 10.61, 6400 train 85.11%, 1600 test 86.75%\n",
      "ep 288, loss: 10.75, 6400 train 84.98%, 1600 test 87.44%\n",
      "ep 289, loss: 10.88, 6400 train 84.84%, 1600 test 86.19%\n",
      "ep 290, loss: 10.98, 6400 train 84.47%, 1600 test 86.50%\n",
      "[[172.   2.   2.   6.   1.   0.   5.   8.]\n",
      " [  0. 150.   3.   0.   6.   9.  12.  18.]\n",
      " [  8.   4. 163.   6.  19.  10.   1.   5.]\n",
      " [  5.   0.   0. 181.   5.   2.   0.   5.]\n",
      " [  0.   4.   5.   2. 169.   0.   0.   4.]\n",
      " [  0.   3.   2.   5.   2. 185.   2.   2.]\n",
      " [  4.  13.   1.   8.   0.   2. 174.   3.]\n",
      " [  3.   3.   2.   1.   2.   0.   1. 190.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:42:14.098083\n",
      "ep 291, loss: 10.58, 6400 train 85.11%, 1600 test 85.06%\n",
      "ep 292, loss: 10.65, 6400 train 84.73%, 1600 test 86.94%\n",
      "ep 293, loss: 10.94, 6400 train 84.66%, 1600 test 87.06%\n",
      "ep 294, loss: 10.04, 6400 train 86.34%, 1600 test 87.88%\n",
      "ep 295, loss: 10.68, 6400 train 85.22%, 1600 test 86.81%\n",
      "ep 296, loss: 10.58, 6400 train 84.95%, 1600 test 86.06%\n",
      "ep 297, loss: 10.86, 6400 train 84.64%, 1600 test 87.50%\n",
      "ep 298, loss: 10.63, 6400 train 84.97%, 1600 test 86.94%\n",
      "ep 299, loss: 10.82, 6400 train 85.00%, 1600 test 87.19%\n",
      "ep 300, loss: 10.97, 6400 train 84.45%, 1600 test 87.19%\n",
      "[[166.   2.   1.   4.   1.   1.  10.  11.]\n",
      " [  0. 161.   1.   0.   6.  10.  12.   8.]\n",
      " [  5.   9. 159.   8.  16.   8.   5.   6.]\n",
      " [  4.   0.   2. 179.   3.   4.   0.   6.]\n",
      " [  0.   1.   1.   0. 178.   0.   0.   4.]\n",
      " [  0.   6.   0.   0.   4. 187.   2.   2.]\n",
      " [  3.  17.   1.   5.   0.   3. 175.   1.]\n",
      " [  2.   5.   0.   0.   3.   0.   2. 190.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:45:54.469559\n",
      "ep 301, loss: 10.73, 6400 train 84.91%, 1600 test 86.81%\n",
      "ep 302, loss: 10.44, 6400 train 85.28%, 1600 test 86.06%\n",
      "ep 303, loss: 10.86, 6400 train 84.78%, 1600 test 85.50%\n",
      "ep 304, loss: 10.48, 6400 train 85.81%, 1600 test 87.06%\n",
      "ep 305, loss: 10.36, 6400 train 85.30%, 1600 test 86.94%\n",
      "ep 306, loss: 11.09, 6400 train 84.38%, 1600 test 85.75%\n",
      "ep 307, loss: 10.48, 6400 train 84.84%, 1600 test 86.94%\n",
      "ep 308, loss: 10.78, 6400 train 85.22%, 1600 test 87.00%\n",
      "ep 309, loss: 10.34, 6400 train 85.58%, 1600 test 87.69%\n",
      "ep 310, loss: 10.25, 6400 train 85.61%, 1600 test 86.88%\n",
      "[[172.   3.   1.   7.   0.   1.   5.   7.]\n",
      " [  0. 158.   4.   0.   1.  20.   9.   6.]\n",
      " [  6.   8. 163.   6.   6.  21.   5.   1.]\n",
      " [  2.   1.   1. 179.   1.   8.   1.   5.]\n",
      " [  2.  10.   7.   3. 157.   3.   0.   2.]\n",
      " [  0.   3.   0.   0.   0. 196.   2.   0.]\n",
      " [  4.  10.   1.   5.   0.   8. 177.   0.]\n",
      " [  3.   7.   1.   0.   1.   1.   1. 188.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:49:28.720339\n",
      "ep 311, loss: 10.40, 6400 train 85.36%, 1600 test 86.94%\n",
      "ep 312, loss: 10.25, 6400 train 85.34%, 1600 test 87.62%\n",
      "ep 313, loss: 10.96, 6400 train 84.39%, 1600 test 86.31%\n",
      "ep 314, loss: 10.47, 6400 train 85.36%, 1600 test 86.69%\n",
      "ep 315, loss: 10.20, 6400 train 85.67%, 1600 test 87.00%\n",
      "ep 316, loss: 10.17, 6400 train 85.84%, 1600 test 88.00%\n",
      "ep 317, loss: 10.20, 6400 train 85.70%, 1600 test 86.69%\n",
      "ep 318, loss: 9.81, 6400 train 86.11%, 1600 test 86.94%\n",
      "ep 319, loss: 10.39, 6400 train 85.86%, 1600 test 87.50%\n",
      "ep 320, loss: 10.55, 6400 train 84.89%, 1600 test 86.25%\n",
      "[[170.   3.   2.   5.   0.   1.  13.   2.]\n",
      " [  0. 179.   0.   0.   1.   9.   6.   3.]\n",
      " [  5.  21. 157.   4.   6.  17.   3.   3.]\n",
      " [  8.   4.   2. 165.   3.   6.   6.   4.]\n",
      " [  0.  17.   3.   1. 160.   0.   1.   2.]\n",
      " [  0.   3.   0.   0.   0. 196.   2.   0.]\n",
      " [  4.  20.   0.   4.   0.   7. 170.   0.]\n",
      " [  4.  11.   0.   0.   1.   1.   2. 183.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:53:05.067471\n",
      "ep 321, loss: 9.78, 6400 train 86.23%, 1600 test 87.00%\n",
      "ep 322, loss: 10.37, 6400 train 85.55%, 1600 test 86.56%\n",
      "ep 323, loss: 10.15, 6400 train 85.89%, 1600 test 87.69%\n",
      "ep 324, loss: 10.47, 6400 train 85.55%, 1600 test 87.50%\n",
      "ep 325, loss: 10.28, 6400 train 85.45%, 1600 test 87.12%\n",
      "ep 326, loss: 10.71, 6400 train 85.14%, 1600 test 86.88%\n",
      "ep 327, loss: 10.38, 6400 train 85.62%, 1600 test 85.50%\n",
      "ep 328, loss: 10.03, 6400 train 86.20%, 1600 test 88.62%\n",
      "ep 329, loss: 9.72, 6400 train 85.92%, 1600 test 87.06%\n",
      "ep 330, loss: 9.96, 6400 train 86.02%, 1600 test 87.38%\n",
      "[[179.   2.   2.   6.   0.   1.   5.   1.]\n",
      " [  0. 144.   5.   1.   4.  13.  16.  15.]\n",
      " [  6.   3. 168.   6.  16.  12.   3.   2.]\n",
      " [ 10.   0.   1. 178.   2.   1.   1.   5.]\n",
      " [  0.   3.   7.   1. 168.   0.   0.   5.]\n",
      " [  0.   2.   0.   1.   3. 192.   1.   2.]\n",
      " [  5.   9.   1.   4.   0.   6. 179.   1.]\n",
      " [  4.   3.   1.   1.   2.   0.   1. 190.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:56:43.453581\n",
      "ep 331, loss: 10.43, 6400 train 85.62%, 1600 test 86.50%\n",
      "ep 332, loss: 10.22, 6400 train 85.69%, 1600 test 87.38%\n",
      "ep 333, loss: 10.20, 6400 train 85.53%, 1600 test 86.50%\n",
      "ep 334, loss: 9.96, 6400 train 86.41%, 1600 test 87.50%\n",
      "ep 335, loss: 10.23, 6400 train 85.84%, 1600 test 88.06%\n",
      "ep 336, loss: 10.06, 6400 train 86.09%, 1600 test 88.19%\n",
      "ep 337, loss: 9.46, 6400 train 86.92%, 1600 test 87.94%\n",
      "ep 338, loss: 9.75, 6400 train 86.58%, 1600 test 87.31%\n",
      "ep 339, loss: 9.93, 6400 train 86.20%, 1600 test 87.44%\n",
      "ep 340, loss: 10.22, 6400 train 85.62%, 1600 test 86.44%\n",
      "[[162.   2.   2.   4.   0.   0.  17.   9.]\n",
      " [  0. 154.   3.   0.   5.   9.  18.   9.]\n",
      " [  2.   8. 171.   5.  12.   6.   8.   4.]\n",
      " [  8.   0.   1. 169.   2.   2.   9.   7.]\n",
      " [  0.   6.   6.   1. 166.   0.   2.   3.]\n",
      " [  0.   5.   0.   1.   4. 183.   6.   2.]\n",
      " [  2.   7.   1.   4.   0.   1. 188.   2.]\n",
      " [  2.   8.   0.   0.   1.   0.   1. 190.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:00:25.169664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 341, loss: 9.81, 6400 train 86.17%, 1600 test 88.50%\n",
      "ep 342, loss: 10.05, 6400 train 86.00%, 1600 test 87.62%\n",
      "ep 343, loss: 9.37, 6400 train 86.98%, 1600 test 86.25%\n",
      "ep 344, loss: 9.39, 6400 train 86.88%, 1600 test 86.75%\n",
      "ep 345, loss: 9.99, 6400 train 86.11%, 1600 test 87.38%\n",
      "ep 346, loss: 9.57, 6400 train 86.69%, 1600 test 87.31%\n",
      "ep 347, loss: 9.55, 6400 train 86.69%, 1600 test 86.88%\n",
      "ep 348, loss: 9.44, 6400 train 86.92%, 1600 test 88.44%\n",
      "ep 349, loss: 9.43, 6400 train 86.91%, 1600 test 87.06%\n",
      "ep 350, loss: 9.60, 6400 train 86.66%, 1600 test 88.12%\n",
      "[[177.   2.   1.   9.   0.   0.   4.   3.]\n",
      " [  0. 154.   3.   0.   5.  14.  10.  12.]\n",
      " [  7.   6. 165.   9.  10.  11.   4.   4.]\n",
      " [  7.   0.   3. 179.   1.   2.   2.   4.]\n",
      " [  0.   3.   4.   1. 173.   0.   0.   3.]\n",
      " [  0.   1.   0.   2.   0. 195.   1.   2.]\n",
      " [  4.   9.   0.   6.   0.   6. 179.   1.]\n",
      " [  5.   4.   1.   1.   1.   1.   1. 188.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:03:57.290348\n",
      "ep 351, loss: 9.40, 6400 train 86.92%, 1600 test 88.12%\n",
      "ep 352, loss: 9.49, 6400 train 86.41%, 1600 test 87.94%\n",
      "ep 353, loss: 9.81, 6400 train 86.38%, 1600 test 87.50%\n",
      "ep 354, loss: 9.84, 6400 train 85.80%, 1600 test 86.56%\n",
      "ep 355, loss: 9.51, 6400 train 86.77%, 1600 test 87.81%\n",
      "ep 356, loss: 9.69, 6400 train 86.69%, 1600 test 87.69%\n",
      "ep 357, loss: 8.92, 6400 train 87.17%, 1600 test 88.25%\n",
      "ep 358, loss: 9.44, 6400 train 87.00%, 1600 test 87.69%\n",
      "ep 359, loss: 9.11, 6400 train 86.72%, 1600 test 88.62%\n",
      "ep 360, loss: 9.45, 6400 train 86.92%, 1600 test 87.81%\n",
      "[[173.   3.   1.   6.   0.   0.  10.   3.]\n",
      " [  0. 170.   1.   0.   4.  11.   7.   5.]\n",
      " [  4.   7. 159.   7.  19.  13.   4.   3.]\n",
      " [  5.   2.   3. 171.   7.   5.   2.   3.]\n",
      " [  2.   2.   3.   1. 173.   0.   1.   2.]\n",
      " [  0.   3.   0.   0.   0. 197.   1.   0.]\n",
      " [  3.  13.   2.   4.   1.   2. 179.   1.]\n",
      " [  3.  10.   2.   0.   2.   1.   1. 183.]]\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:07:45.012711\n",
      "ep 361, loss: 9.79, 6400 train 86.28%, 1600 test 87.25%\n",
      "ep 362, loss: 9.41, 6400 train 86.67%, 1600 test 87.38%\n",
      "ep 363, loss: 9.53, 6400 train 86.53%, 1600 test 85.81%\n",
      "ep 364, loss: 9.39, 6400 train 87.16%, 1600 test 88.12%\n",
      "ep 365, loss: 9.19, 6400 train 86.86%, 1600 test 87.06%\n",
      "ep 366, loss: 9.14, 6400 train 87.55%, 1600 test 87.75%\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Tracking training time ###\n",
    "##############################\n",
    "start_time = time.time() ## Added\n",
    "time_elapsed = 0  ## Added Line\n",
    "##############################\n",
    "\n",
    "###############################\n",
    "### Tracking nn performance ###\n",
    "###############################\n",
    "minibatch_loss_list, train_accuracy_list, test_accuracy_list = [], [], [] ## Added\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "print(\"Using device: {}\"\n",
    "      \"\\n\".format(str(device)))\n",
    "########################################################################\n",
    "#######                      Loading Data                        #######\n",
    "########################################################################\n",
    "data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "\n",
    "if train_val_split == 1:\n",
    "    # Train on the entire dataset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset,\n",
    "                        transform=transform('train'))\n",
    "    trainloader = torch.utils.data.DataLoader(data,\n",
    "                        batch_size=batch_size, shuffle=True);\n",
    "else:\n",
    "    # Split the dataset into trainset and testset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "    data.len=len(data)\n",
    "    train_len = int((train_val_split)*data.len)\n",
    "    test_len = data.len - train_len\n",
    "    train_subset, test_subset = random_split(data, [train_len, test_len])\n",
    "    trainset = DatasetFromSubset(\n",
    "        train_subset, transform=transform('train'))\n",
    "    testset = DatasetFromSubset(\n",
    "        test_subset, transform=transform('test'))\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "    testloader = torch.utils.data.DataLoader(testset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Get model, loss criterion and optimizer from student\n",
    "net = net.to(device)\n",
    "criterion = loss_func\n",
    "optimizer = optimizer\n",
    "# get weight initialization and lr scheduler, if appropriate\n",
    "weights_init = weights_init\n",
    "scheduler = scheduler\n",
    "\n",
    "# apply custom weight initialization, if it exists\n",
    "net.apply(weights_init)\n",
    "\n",
    "########################################################################\n",
    "#######                        Training                          #######\n",
    "########################################################################\n",
    "print(\"Start training...\")\n",
    "for epoch in range(1,epochs+1):\n",
    "    total_loss = 0\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in trainloader:           # Load batch\n",
    "        images, labels = batch \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = net(images)             # Process batch\n",
    "\n",
    "        loss = criterion(preds, labels) # Calculate loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                 # Calculate gradients\n",
    "        optimizer.step()                # Update weights\n",
    "\n",
    "        output = preds.argmax(dim=1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_images += labels.size(0)\n",
    "        total_correct += output.eq(labels).sum().item()\n",
    "        minibatch_loss_list.append(loss.item())  ## Added\n",
    "\n",
    "    # apply lr schedule, if it exists\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100 \n",
    "    train_accuracy_list.append(model_accuracy)  ## Added\n",
    "    print('ep {0}, loss: {1:.2f}, {2} train {3:.2f}%'.format(\n",
    "           epoch, total_loss, total_images, model_accuracy), end='')\n",
    "\n",
    "    if train_val_split < 1:\n",
    "        test_network(net,testloader, test_accuracy_list,\n",
    "                     print_confusion=(epoch % 10 == 0)) ## Added\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "   \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.state_dict(),'v3_check.pth')\n",
    "        print(\"   Model saved to checkModel.pth\")\n",
    "        time_elapsed = time.time() - start_time  ## Added Line\n",
    "        print(f'Time elapsed: {str(datetime.timedelta(seconds = time_elapsed))}') ## TIME\n",
    "    \n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "torch.save(net.state_dict(),'v3_saved.pth')\n",
    "print(\"   Model saved to savedModel.pth\")\n",
    "time_elapsed = time.time() - start_time ## Added Line\n",
    "print(f'total time needed to train network: \\\n",
    "        {str(datetime.timedelta(seconds = time_elapsed))}\\ntotal time in seconds: {time_elapsed}') ## TIME\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#**        Data Information     **#\n",
    "###################################\n",
    "print(f'batch size: {batch_size}')\n",
    "print(f'learning rate: {learning_rate}')\n",
    "print(f'train_val_split: {train_val_split}')\n",
    "print(f'epochs: {epochs}')\n",
    "\n",
    "\n",
    "#############################\n",
    "#**         END           **#\n",
    "#############################\n",
    "\n",
    "\n",
    "# Getting count of each cat breed, should be close to 8*0.8*1000 initially..\n",
    "train_data_distribution = get_cat_count(trainloader, 'training data')\n",
    "# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\n",
    "test_data_distribution = get_cat_count(testloader, 'test data')\n",
    "\n",
    "print(f'training data distribution - {train_data_distribution}')\n",
    "print(f'test data distribution - {test_data_distribution}')\n",
    "\n",
    "plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                   num_epochs=epochs,\n",
    "                   iter_per_epoch=len(trainloader),\n",
    "                   results_dir=None,\n",
    "                   averaging_iterations=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_accuracy(train_acc_list=train_accuracy_list,\n",
    "              test_acc_list=test_accuracy_list,\n",
    "              results_dir=None)\n",
    "plt.show()\n",
    "\n",
    "net.cpu()\n",
    "show_examples(model=net, data_loader=testloader, class_dict=cat_dict)\n",
    "\n",
    "conf_matrix = compute_confusion_matrix(model=net, data_loader=testloader, device=torch.device('cpu'))\n",
    "print(conf_matrix)\n",
    "plot_confusion_matrix(conf_matrix, class_names=cat_dict.values(), test_data_distribution=test_data_distribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
