{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from helper import get_cat_count, count_parameters, compute_confusion_matrix, show_examples, plot_training_loss, plot_accuracy, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9444 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat breed classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**student.py**\n",
    "\n",
    "UNSW COMP9444 Neural Networks and Deep Learning\n",
    "\n",
    "You may modify this file however you wish, including creating additional\n",
    "variables, functions, classes, etc., so long as your code runs with the\n",
    "hw2main.py file unmodified, and you are only using the approved packages.\n",
    "\n",
    "You have been given some default values for the variables train_val_split,\n",
    "batch_size as well as the transform function.\n",
    "You are encouraged to modify these to improve the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question:**\n",
    "\n",
    "Briefly describe how your program works, and explain any design and training\n",
    "decisions you made along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "######     Specify transform(s) to be applied to the input images     ######\n",
    "############################################################################\n",
    "\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    # channel size = 3\n",
    "\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "    \"\"\"\n",
    "    # Data Augmentation\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                transforms.RandomResizedCrop(size=80, scale=(0.45, 1.0), ratio=(0.70, 1.4)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomPerspective(p=0.2),\n",
    "                transforms.RandomAffine(degrees=(-15, 15), translate=(0.0, 0.5)),\n",
    "                transforms.RandomRotation((-10,10)),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.1, hue=0.02),\n",
    "                transforms.RandomPosterize(bits=3, p=0.3),\n",
    "                transforms.RandomEqualize(p=0.1),\n",
    "                transforms.RandomGrayscale(p=0.01),\n",
    "                transforms.RandomPerspective(distortion_scale=0.05, p=0.15, fill=0),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )\n",
    "    # Keep the testing data original to ensure accuracy\n",
    "    elif mode == 'test':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomResizedCrop(size=(80, 80), scale=(0.45, 1.0), ratio=(0.7, 1.4), interpolation=bilinear)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomPerspective(p=0.2)\n",
      "    RandomAffine(degrees=[-15.0, 15.0], translate=(0.0, 0.5))\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.9, 1.1], hue=[-0.02, 0.02])\n",
      "    RandomPosterize(bits=3,p=0.3)\n",
      "    RandomEqualize(p=0.1)\n",
      "    RandomGrayscale(p=0.01)\n",
      "    RandomPerspective(p=0.15)\n",
      "    RandomAdjustSharpness(sharpness_factor=2,p=0.5)\n",
      "    ToTensor()\n",
      ")\n",
      "Network(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0, inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ELU(alpha=1.0, inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ELU(alpha=1.0, inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ELU(alpha=1.0, inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ELU(alpha=1.0, inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ELU(alpha=1.0, inplace=True)\n",
      "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ELU(alpha=1.0, inplace=True)\n",
      "    (27): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ELU(alpha=1.0, inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): Linear(in_features=3072, out_features=2400, bias=True)\n",
      "    (3): BatchNorm1d(2400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.6, inplace=False)\n",
      "    (6): Linear(in_features=2400, out_features=1024, bias=True)\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Dropout(p=0.4, inplace=False)\n",
      "    (10): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "+-----------------------+------------+\n",
      "|        Modules        | Parameters |\n",
      "+-----------------------+------------+\n",
      "|  conv_layers.0.weight |    1728    |\n",
      "|   conv_layers.0.bias  |     64     |\n",
      "|  conv_layers.1.weight |     64     |\n",
      "|   conv_layers.1.bias  |     64     |\n",
      "|  conv_layers.3.weight |   36864    |\n",
      "|   conv_layers.3.bias  |     64     |\n",
      "|  conv_layers.4.weight |     64     |\n",
      "|   conv_layers.4.bias  |     64     |\n",
      "|  conv_layers.7.weight |   73728    |\n",
      "|   conv_layers.7.bias  |    128     |\n",
      "|  conv_layers.8.weight |    128     |\n",
      "|   conv_layers.8.bias  |    128     |\n",
      "| conv_layers.10.weight |   147456   |\n",
      "|  conv_layers.10.bias  |    128     |\n",
      "| conv_layers.11.weight |    128     |\n",
      "|  conv_layers.11.bias  |    128     |\n",
      "| conv_layers.14.weight |   294912   |\n",
      "|  conv_layers.14.bias  |    256     |\n",
      "| conv_layers.15.weight |    256     |\n",
      "|  conv_layers.15.bias  |    256     |\n",
      "| conv_layers.17.weight |   589824   |\n",
      "|  conv_layers.17.bias  |    256     |\n",
      "| conv_layers.18.weight |    256     |\n",
      "|  conv_layers.18.bias  |    256     |\n",
      "| conv_layers.21.weight |   589824   |\n",
      "|  conv_layers.21.bias  |    256     |\n",
      "| conv_layers.22.weight |    256     |\n",
      "|  conv_layers.22.bias  |    256     |\n",
      "| conv_layers.24.weight |   589824   |\n",
      "|  conv_layers.24.bias  |    256     |\n",
      "| conv_layers.25.weight |    256     |\n",
      "|  conv_layers.25.bias  |    256     |\n",
      "| conv_layers.27.weight |   442368   |\n",
      "|  conv_layers.27.bias  |    192     |\n",
      "| conv_layers.28.weight |    192     |\n",
      "|  conv_layers.28.bias  |    192     |\n",
      "|   fc_layers.2.weight  |  7372800   |\n",
      "|    fc_layers.2.bias   |    2400    |\n",
      "|   fc_layers.3.weight  |    2400    |\n",
      "|    fc_layers.3.bias   |    2400    |\n",
      "|   fc_layers.6.weight  |  2457600   |\n",
      "|    fc_layers.6.bias   |    1024    |\n",
      "|   fc_layers.7.weight  |    1024    |\n",
      "|    fc_layers.7.bias   |    1024    |\n",
      "|  fc_layers.10.weight  |    8192    |\n",
      "|   fc_layers.10.bias   |     8      |\n",
      "+-----------------------+------------+\n",
      "Total Trainable Params: 12620200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12620200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "#####                      Specify NN to be used                           ######\n",
    "#################################################################################\n",
    "\n",
    "### Simplified implementation of VGG16 with 12 layers instead of 16.\n",
    "### Cut layer = 256 - 256 conv layer. 512-512 * 3 conv layers at the end.\n",
    "### Reduced number of nodes on FC layer from 4096 to 1024.\n",
    "vgg_12 = [64, 64, 'maxpool', 128, 128, 'maxpool', 256, 256, 'maxpool', 512, 512, 512, 'maxpool', 'avgpool', 'fc1', 'fc2', 'fc3']    \n",
    "##########################################################################################\n",
    "# trying to take some inspirations from vgg16 but with less channels and fc layer nodes. #\n",
    "##########################################################################################\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ######### block 1 #########\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            \n",
    "            ######### block 2 #########\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            ######### block 3 #########   \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "        \n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            \n",
    "            ######### block 4 #########\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 192, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # shrink final conv layer width to 4\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4,4))\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten from conv layers\n",
    "\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(192*4*4, 2400),\n",
    "            nn.BatchNorm1d(2400),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Linear(2400, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(1024, 8)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.avgpool(x)       \n",
    "        x = self.fc_layers(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Network()\n",
    "\n",
    "############################################################################\n",
    "######      Specify the optimizer and loss function                   ######\n",
    "############################################################################\n",
    "learning_rate = 0.0005\n",
    "# optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=learning_rate)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# loss_func = F.nll_loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "############################################################################\n",
    "######  Custom weight initialization and lr scheduling are optional   ######\n",
    "############################################################################\n",
    "\n",
    "# Normally, the default weight initialization and fixed learing rate\n",
    "# should work fine. But, we have made it possible for you to define\n",
    "# your own custom weight initialization and lr scheduler, if you wish.\n",
    "def weights_init(m):\n",
    "    return\n",
    "\n",
    "scheduler = None\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#######              Metaparameters and training options              ######\n",
    "############################################################################\n",
    "dataset = \"./data\"\n",
    "train_val_split = 1\n",
    "batch_size = 256 \n",
    "epochs = 1500\n",
    "\n",
    "\n",
    "###############################################\n",
    "#**          Print Network Information      **#\n",
    "###############################################\n",
    "print(transform('train'))\n",
    "print(net)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###########################\n",
    "## Cat breed dictionary  ##\n",
    "###########################\n",
    "cat_dict = {\n",
    "    0: 'bombay',\n",
    "    1: 'calico',\n",
    "    2: 'persian',\n",
    "    3: 'russianblue',\n",
    "    4: 'siamese',\n",
    "    5: 'tiger',\n",
    "    6: 'tortoiseshell',\n",
    "    7: 'tuxedo'\n",
    "}\n",
    "\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "# Test network on validation set, if it exists.\n",
    "## Added params\n",
    "def test_network(net,testloader,test_accuracy_list,print_confusion=False):\n",
    "    net.eval()\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "    conf_matrix = np.zeros((8,8))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            conf_matrix = conf_matrix + metrics.confusion_matrix(\n",
    "                labels.cpu(),predicted.cpu(),labels=[0,1,2,3,4,5,6,7])\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    test_accuracy_list.append(model_accuracy)\n",
    "    print(', {0} test {1:.2f}%'.format(total_images,model_accuracy))\n",
    "    if print_confusion:\n",
    "        np.set_printoptions(precision=2, suppress=True)\n",
    "        print(conf_matrix)\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Start training...\n",
      "ep 1, loss: 63.80, 8000 train 23.71%\n",
      "ep 2, loss: 59.27, 8000 train 29.90%\n",
      "ep 3, loss: 56.98, 8000 train 31.71%\n",
      "ep 4, loss: 55.36, 8000 train 34.88%\n",
      "ep 5, loss: 53.14, 8000 train 37.70%\n",
      "ep 6, loss: 52.08, 8000 train 38.94%\n",
      "ep 7, loss: 51.09, 8000 train 39.84%\n",
      "ep 8, loss: 50.31, 8000 train 40.77%\n",
      "ep 9, loss: 50.23, 8000 train 41.36%\n",
      "ep 10, loss: 48.29, 8000 train 44.30%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:03:50.805036\n",
      "ep 11, loss: 47.82, 8000 train 44.39%\n",
      "ep 12, loss: 47.18, 8000 train 45.75%\n",
      "ep 13, loss: 46.06, 8000 train 47.42%\n",
      "ep 14, loss: 46.27, 8000 train 47.36%\n",
      "ep 15, loss: 45.08, 8000 train 49.06%\n",
      "ep 16, loss: 44.74, 8000 train 49.36%\n",
      "ep 17, loss: 44.66, 8000 train 49.38%\n",
      "ep 18, loss: 45.29, 8000 train 48.20%\n",
      "ep 19, loss: 43.40, 8000 train 50.58%\n",
      "ep 20, loss: 42.62, 8000 train 51.04%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:07:44.648464\n",
      "ep 21, loss: 41.35, 8000 train 53.05%\n",
      "ep 22, loss: 42.11, 8000 train 52.95%\n",
      "ep 23, loss: 41.12, 8000 train 54.04%\n",
      "ep 24, loss: 40.83, 8000 train 53.80%\n",
      "ep 25, loss: 40.10, 8000 train 55.05%\n",
      "ep 26, loss: 39.68, 8000 train 55.70%\n",
      "ep 27, loss: 39.82, 8000 train 55.51%\n",
      "ep 28, loss: 39.07, 8000 train 55.62%\n",
      "ep 29, loss: 38.84, 8000 train 56.15%\n",
      "ep 30, loss: 38.35, 8000 train 56.97%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:12:01.181283\n",
      "ep 31, loss: 37.69, 8000 train 57.95%\n",
      "ep 32, loss: 37.94, 8000 train 57.24%\n",
      "ep 33, loss: 37.44, 8000 train 57.64%\n",
      "ep 34, loss: 37.22, 8000 train 58.19%\n",
      "ep 35, loss: 36.35, 8000 train 58.66%\n",
      "ep 36, loss: 36.66, 8000 train 58.90%\n",
      "ep 37, loss: 36.66, 8000 train 59.00%\n",
      "ep 38, loss: 36.18, 8000 train 59.79%\n",
      "ep 39, loss: 35.88, 8000 train 60.39%\n",
      "ep 40, loss: 36.15, 8000 train 59.56%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:16:14.580973\n",
      "ep 41, loss: 35.76, 8000 train 60.30%\n",
      "ep 42, loss: 34.58, 8000 train 61.42%\n",
      "ep 43, loss: 35.06, 8000 train 60.79%\n",
      "ep 44, loss: 34.93, 8000 train 61.61%\n",
      "ep 45, loss: 34.47, 8000 train 61.94%\n",
      "ep 46, loss: 34.40, 8000 train 61.96%\n",
      "ep 47, loss: 33.80, 8000 train 62.35%\n",
      "ep 48, loss: 33.74, 8000 train 62.62%\n",
      "ep 49, loss: 33.47, 8000 train 62.36%\n",
      "ep 50, loss: 33.19, 8000 train 63.09%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:20:34.757981\n",
      "   Model saved to v4_50_saved.pth\n",
      "ep 51, loss: 32.83, 8000 train 64.11%\n",
      "ep 52, loss: 32.43, 8000 train 64.08%\n",
      "ep 53, loss: 32.35, 8000 train 64.60%\n",
      "ep 54, loss: 32.05, 8000 train 64.21%\n",
      "ep 55, loss: 32.48, 8000 train 64.35%\n",
      "ep 56, loss: 31.78, 8000 train 65.00%\n",
      "ep 57, loss: 31.62, 8000 train 65.34%\n",
      "ep 58, loss: 31.62, 8000 train 65.34%\n",
      "ep 59, loss: 30.68, 8000 train 66.31%\n",
      "ep 60, loss: 31.00, 8000 train 66.00%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:24:43.540248\n",
      "ep 61, loss: 30.77, 8000 train 66.21%\n",
      "ep 62, loss: 30.85, 8000 train 66.24%\n",
      "ep 63, loss: 30.40, 8000 train 66.15%\n",
      "ep 64, loss: 29.66, 8000 train 66.75%\n",
      "ep 65, loss: 30.03, 8000 train 66.47%\n",
      "ep 66, loss: 30.04, 8000 train 67.06%\n",
      "ep 67, loss: 29.54, 8000 train 67.65%\n",
      "ep 68, loss: 29.05, 8000 train 68.36%\n",
      "ep 69, loss: 28.64, 8000 train 68.92%\n",
      "ep 70, loss: 29.13, 8000 train 68.55%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:29:01.766438\n",
      "ep 71, loss: 29.20, 8000 train 68.30%\n",
      "ep 72, loss: 28.54, 8000 train 69.04%\n",
      "ep 73, loss: 29.01, 8000 train 68.47%\n",
      "ep 74, loss: 28.50, 8000 train 68.36%\n",
      "ep 75, loss: 28.45, 8000 train 68.94%\n",
      "ep 76, loss: 28.28, 8000 train 69.29%\n",
      "ep 77, loss: 28.73, 8000 train 68.54%\n",
      "ep 78, loss: 27.73, 8000 train 69.80%\n",
      "ep 79, loss: 27.39, 8000 train 70.21%\n",
      "ep 80, loss: 27.63, 8000 train 69.31%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:33:19.403802\n",
      "ep 81, loss: 27.80, 8000 train 69.50%\n",
      "ep 82, loss: 27.58, 8000 train 70.44%\n",
      "ep 83, loss: 27.55, 8000 train 70.20%\n",
      "ep 84, loss: 27.13, 8000 train 70.24%\n",
      "ep 85, loss: 27.46, 8000 train 70.08%\n",
      "ep 86, loss: 27.52, 8000 train 69.96%\n",
      "ep 87, loss: 26.32, 8000 train 71.33%\n",
      "ep 88, loss: 26.15, 8000 train 71.21%\n",
      "ep 89, loss: 26.26, 8000 train 71.04%\n",
      "ep 90, loss: 26.40, 8000 train 70.99%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:37:38.280191\n",
      "ep 91, loss: 26.79, 8000 train 70.74%\n",
      "ep 92, loss: 25.99, 8000 train 71.65%\n",
      "ep 93, loss: 25.71, 8000 train 71.43%\n",
      "ep 94, loss: 25.93, 8000 train 72.05%\n",
      "ep 95, loss: 26.12, 8000 train 72.00%\n",
      "ep 96, loss: 25.81, 8000 train 71.31%\n",
      "ep 97, loss: 24.94, 8000 train 72.84%\n",
      "ep 98, loss: 25.33, 8000 train 72.12%\n",
      "ep 99, loss: 25.86, 8000 train 71.80%\n",
      "ep 100, loss: 25.46, 8000 train 72.60%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:42:01.076148\n",
      "   Model saved to v4_100_saved.pth\n",
      "ep 101, loss: 25.27, 8000 train 72.26%\n",
      "ep 102, loss: 25.00, 8000 train 72.40%\n",
      "ep 103, loss: 24.40, 8000 train 73.15%\n",
      "ep 104, loss: 25.00, 8000 train 72.71%\n",
      "ep 105, loss: 25.01, 8000 train 73.11%\n",
      "ep 106, loss: 25.06, 8000 train 72.26%\n",
      "ep 107, loss: 24.23, 8000 train 73.17%\n",
      "ep 108, loss: 24.96, 8000 train 73.30%\n",
      "ep 109, loss: 23.60, 8000 train 74.12%\n",
      "ep 110, loss: 24.56, 8000 train 73.64%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:46:28.328441\n",
      "ep 111, loss: 24.71, 8000 train 73.32%\n",
      "ep 112, loss: 23.83, 8000 train 73.74%\n",
      "ep 113, loss: 23.84, 8000 train 73.85%\n",
      "ep 114, loss: 24.13, 8000 train 73.76%\n",
      "ep 115, loss: 23.99, 8000 train 73.69%\n",
      "ep 116, loss: 23.58, 8000 train 74.24%\n",
      "ep 117, loss: 23.68, 8000 train 74.33%\n",
      "ep 118, loss: 23.60, 8000 train 73.84%\n",
      "ep 119, loss: 23.37, 8000 train 74.29%\n",
      "ep 120, loss: 22.82, 8000 train 75.22%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:50:47.795964\n",
      "ep 121, loss: 23.63, 8000 train 74.02%\n",
      "ep 122, loss: 23.45, 8000 train 74.25%\n",
      "ep 123, loss: 22.73, 8000 train 75.14%\n",
      "ep 124, loss: 24.00, 8000 train 74.11%\n",
      "ep 125, loss: 23.45, 8000 train 74.42%\n",
      "ep 126, loss: 22.88, 8000 train 74.96%\n",
      "ep 127, loss: 22.49, 8000 train 75.42%\n",
      "ep 128, loss: 22.40, 8000 train 75.65%\n",
      "ep 129, loss: 22.69, 8000 train 75.50%\n",
      "ep 130, loss: 22.54, 8000 train 75.94%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:55:17.311783\n",
      "ep 131, loss: 22.39, 8000 train 75.71%\n",
      "ep 132, loss: 22.05, 8000 train 75.62%\n",
      "ep 133, loss: 22.32, 8000 train 75.09%\n",
      "ep 134, loss: 22.28, 8000 train 75.92%\n",
      "ep 135, loss: 22.37, 8000 train 75.29%\n",
      "ep 136, loss: 21.62, 8000 train 76.41%\n",
      "ep 137, loss: 21.93, 8000 train 75.66%\n",
      "ep 138, loss: 22.33, 8000 train 75.22%\n",
      "ep 139, loss: 21.82, 8000 train 76.33%\n",
      "ep 140, loss: 21.79, 8000 train 75.72%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:59:26.174742\n",
      "ep 141, loss: 21.43, 8000 train 76.72%\n",
      "ep 142, loss: 21.99, 8000 train 76.17%\n",
      "ep 143, loss: 21.45, 8000 train 76.36%\n",
      "ep 144, loss: 21.83, 8000 train 76.19%\n",
      "ep 145, loss: 21.41, 8000 train 76.51%\n",
      "ep 146, loss: 21.39, 8000 train 76.76%\n",
      "ep 147, loss: 21.03, 8000 train 77.38%\n",
      "ep 148, loss: 21.29, 8000 train 76.61%\n",
      "ep 149, loss: 21.77, 8000 train 76.12%\n",
      "ep 150, loss: 21.06, 8000 train 77.03%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:03:47.670505\n",
      "   Model saved to v4_150_saved.pth\n",
      "ep 151, loss: 20.40, 8000 train 77.60%\n",
      "ep 152, loss: 21.37, 8000 train 76.44%\n",
      "ep 153, loss: 21.71, 8000 train 76.42%\n",
      "ep 154, loss: 20.78, 8000 train 77.48%\n",
      "ep 155, loss: 20.78, 8000 train 77.25%\n",
      "ep 156, loss: 20.33, 8000 train 77.59%\n",
      "ep 157, loss: 20.46, 8000 train 78.03%\n",
      "ep 158, loss: 21.59, 8000 train 76.39%\n",
      "ep 159, loss: 21.35, 8000 train 76.69%\n",
      "ep 160, loss: 20.85, 8000 train 77.05%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:08:21.946198\n",
      "ep 161, loss: 20.64, 8000 train 77.60%\n",
      "ep 162, loss: 20.85, 8000 train 77.46%\n",
      "ep 163, loss: 20.47, 8000 train 77.84%\n",
      "ep 164, loss: 20.11, 8000 train 78.75%\n",
      "ep 165, loss: 20.56, 8000 train 77.55%\n",
      "ep 166, loss: 20.29, 8000 train 78.75%\n",
      "ep 167, loss: 20.46, 8000 train 77.92%\n",
      "ep 168, loss: 20.12, 8000 train 78.49%\n",
      "ep 169, loss: 19.95, 8000 train 78.30%\n",
      "ep 170, loss: 19.21, 8000 train 78.70%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:12:25.134669\n",
      "ep 171, loss: 19.73, 8000 train 78.15%\n",
      "ep 172, loss: 20.16, 8000 train 78.40%\n",
      "ep 173, loss: 20.67, 8000 train 77.89%\n",
      "ep 174, loss: 19.86, 8000 train 78.20%\n",
      "ep 175, loss: 19.15, 8000 train 79.30%\n",
      "ep 176, loss: 19.75, 8000 train 78.61%\n",
      "ep 177, loss: 20.67, 8000 train 77.38%\n",
      "ep 178, loss: 20.20, 8000 train 77.54%\n",
      "ep 179, loss: 19.52, 8000 train 78.59%\n",
      "ep 180, loss: 19.12, 8000 train 78.88%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:16:52.065296\n",
      "ep 181, loss: 19.39, 8000 train 79.05%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 182, loss: 19.84, 8000 train 78.81%\n",
      "ep 183, loss: 19.66, 8000 train 78.67%\n",
      "ep 184, loss: 19.75, 8000 train 78.50%\n",
      "ep 185, loss: 18.99, 8000 train 79.35%\n",
      "ep 186, loss: 18.74, 8000 train 80.09%\n",
      "ep 187, loss: 19.36, 8000 train 78.92%\n",
      "ep 188, loss: 19.33, 8000 train 79.04%\n",
      "ep 189, loss: 19.68, 8000 train 78.80%\n",
      "ep 190, loss: 20.83, 8000 train 77.42%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:21:20.818719\n",
      "ep 191, loss: 18.93, 8000 train 79.09%\n",
      "ep 192, loss: 18.80, 8000 train 79.71%\n",
      "ep 193, loss: 19.00, 8000 train 79.01%\n",
      "ep 194, loss: 18.93, 8000 train 79.33%\n",
      "ep 195, loss: 18.81, 8000 train 79.90%\n",
      "ep 196, loss: 18.69, 8000 train 79.11%\n",
      "ep 197, loss: 18.42, 8000 train 80.04%\n",
      "ep 198, loss: 19.12, 8000 train 78.92%\n",
      "ep 199, loss: 18.90, 8000 train 79.24%\n",
      "ep 200, loss: 17.85, 8000 train 80.80%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:25:37.234249\n",
      "   Model saved to v4_200_saved.pth\n",
      "ep 201, loss: 18.61, 8000 train 79.86%\n",
      "ep 202, loss: 19.18, 8000 train 79.04%\n",
      "ep 203, loss: 18.44, 8000 train 80.17%\n",
      "ep 204, loss: 18.82, 8000 train 79.64%\n",
      "ep 205, loss: 18.57, 8000 train 80.03%\n",
      "ep 206, loss: 18.55, 8000 train 80.16%\n",
      "ep 207, loss: 17.49, 8000 train 81.21%\n",
      "ep 208, loss: 18.51, 8000 train 79.53%\n",
      "ep 209, loss: 17.75, 8000 train 80.60%\n",
      "ep 210, loss: 17.81, 8000 train 80.64%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:30:01.369411\n",
      "ep 211, loss: 17.80, 8000 train 80.53%\n",
      "ep 212, loss: 17.73, 8000 train 80.29%\n",
      "ep 213, loss: 18.10, 8000 train 80.56%\n",
      "ep 214, loss: 18.21, 8000 train 80.38%\n",
      "ep 215, loss: 18.43, 8000 train 79.70%\n",
      "ep 216, loss: 17.92, 8000 train 80.81%\n",
      "ep 217, loss: 18.39, 8000 train 79.80%\n",
      "ep 218, loss: 17.60, 8000 train 80.62%\n",
      "ep 219, loss: 17.85, 8000 train 80.67%\n",
      "ep 220, loss: 17.36, 8000 train 80.84%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:34:08.663249\n",
      "ep 221, loss: 18.00, 8000 train 80.53%\n",
      "ep 222, loss: 17.77, 8000 train 80.55%\n",
      "ep 223, loss: 17.36, 8000 train 81.53%\n",
      "ep 224, loss: 18.20, 8000 train 79.97%\n",
      "ep 225, loss: 17.86, 8000 train 80.62%\n",
      "ep 226, loss: 17.53, 8000 train 81.03%\n",
      "ep 227, loss: 17.80, 8000 train 80.95%\n",
      "ep 228, loss: 18.29, 8000 train 80.10%\n",
      "ep 229, loss: 17.41, 8000 train 80.96%\n",
      "ep 230, loss: 17.20, 8000 train 80.97%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:38:35.691929\n",
      "ep 231, loss: 16.96, 8000 train 81.53%\n",
      "ep 232, loss: 17.22, 8000 train 81.06%\n",
      "ep 233, loss: 17.22, 8000 train 81.17%\n",
      "ep 234, loss: 17.53, 8000 train 80.27%\n",
      "ep 235, loss: 17.23, 8000 train 81.20%\n",
      "ep 236, loss: 17.19, 8000 train 81.20%\n",
      "ep 237, loss: 17.29, 8000 train 81.01%\n",
      "ep 238, loss: 17.39, 8000 train 80.53%\n",
      "ep 239, loss: 16.67, 8000 train 81.92%\n",
      "ep 240, loss: 17.30, 8000 train 81.41%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:43:05.675833\n",
      "ep 241, loss: 16.91, 8000 train 81.56%\n",
      "ep 242, loss: 17.18, 8000 train 80.90%\n",
      "ep 243, loss: 17.00, 8000 train 81.60%\n",
      "ep 244, loss: 16.65, 8000 train 82.14%\n",
      "ep 245, loss: 16.53, 8000 train 81.36%\n",
      "ep 246, loss: 17.14, 8000 train 81.41%\n",
      "ep 247, loss: 16.60, 8000 train 81.99%\n",
      "ep 248, loss: 16.79, 8000 train 81.84%\n",
      "ep 249, loss: 16.34, 8000 train 82.06%\n",
      "ep 250, loss: 17.06, 8000 train 80.62%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:47:24.572445\n",
      "   Model saved to v4_250_saved.pth\n",
      "ep 251, loss: 16.21, 8000 train 81.64%\n",
      "ep 252, loss: 16.53, 8000 train 82.08%\n",
      "ep 253, loss: 16.03, 8000 train 82.94%\n",
      "ep 254, loss: 16.91, 8000 train 81.96%\n",
      "ep 255, loss: 16.28, 8000 train 82.14%\n",
      "ep 256, loss: 16.78, 8000 train 81.70%\n",
      "ep 257, loss: 17.12, 8000 train 81.17%\n",
      "ep 258, loss: 16.49, 8000 train 81.84%\n",
      "ep 259, loss: 16.53, 8000 train 82.27%\n",
      "ep 260, loss: 16.35, 8000 train 82.76%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:51:40.943445\n",
      "ep 261, loss: 16.24, 8000 train 82.15%\n",
      "ep 262, loss: 16.24, 8000 train 82.34%\n",
      "ep 263, loss: 16.58, 8000 train 81.97%\n",
      "ep 264, loss: 15.94, 8000 train 82.50%\n",
      "ep 265, loss: 16.51, 8000 train 81.83%\n",
      "ep 266, loss: 15.78, 8000 train 82.78%\n",
      "ep 267, loss: 15.52, 8000 train 82.65%\n",
      "ep 268, loss: 16.24, 8000 train 82.69%\n",
      "ep 269, loss: 15.63, 8000 train 82.83%\n",
      "ep 270, loss: 15.35, 8000 train 83.25%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:56:03.429765\n",
      "ep 271, loss: 15.36, 8000 train 83.30%\n",
      "ep 272, loss: 16.06, 8000 train 82.78%\n",
      "ep 273, loss: 15.49, 8000 train 82.96%\n",
      "ep 274, loss: 15.66, 8000 train 83.00%\n",
      "ep 275, loss: 15.91, 8000 train 82.34%\n",
      "ep 276, loss: 15.90, 8000 train 82.59%\n",
      "ep 277, loss: 15.76, 8000 train 82.54%\n",
      "ep 278, loss: 15.75, 8000 train 82.86%\n",
      "ep 279, loss: 15.40, 8000 train 83.21%\n",
      "ep 280, loss: 15.77, 8000 train 82.73%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:00:23.941540\n",
      "ep 281, loss: 15.55, 8000 train 83.12%\n",
      "ep 282, loss: 15.07, 8000 train 84.19%\n",
      "ep 283, loss: 15.82, 8000 train 82.71%\n",
      "ep 284, loss: 15.65, 8000 train 83.17%\n",
      "ep 285, loss: 15.72, 8000 train 82.79%\n",
      "ep 286, loss: 15.54, 8000 train 83.01%\n",
      "ep 287, loss: 15.86, 8000 train 82.95%\n",
      "ep 288, loss: 15.16, 8000 train 83.73%\n",
      "ep 289, loss: 15.84, 8000 train 83.30%\n",
      "ep 290, loss: 15.52, 8000 train 83.44%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:04:38.049917\n",
      "ep 291, loss: 15.50, 8000 train 83.51%\n",
      "ep 292, loss: 16.15, 8000 train 82.53%\n",
      "ep 293, loss: 15.30, 8000 train 83.21%\n",
      "ep 294, loss: 15.59, 8000 train 83.40%\n",
      "ep 295, loss: 14.87, 8000 train 83.91%\n",
      "ep 296, loss: 15.20, 8000 train 83.50%\n",
      "ep 297, loss: 15.49, 8000 train 83.43%\n",
      "ep 298, loss: 14.97, 8000 train 83.67%\n",
      "ep 299, loss: 15.19, 8000 train 83.75%\n",
      "ep 300, loss: 15.66, 8000 train 82.55%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:08:59.385375\n",
      "   Model saved to v4_300_saved.pth\n",
      "ep 301, loss: 15.16, 8000 train 83.49%\n",
      "ep 302, loss: 15.57, 8000 train 83.28%\n",
      "ep 303, loss: 14.93, 8000 train 84.44%\n",
      "ep 304, loss: 15.52, 8000 train 83.09%\n",
      "ep 305, loss: 15.13, 8000 train 83.12%\n",
      "ep 306, loss: 14.87, 8000 train 83.49%\n",
      "ep 307, loss: 14.99, 8000 train 83.83%\n",
      "ep 308, loss: 14.84, 8000 train 84.19%\n",
      "ep 309, loss: 15.03, 8000 train 83.41%\n",
      "ep 310, loss: 15.20, 8000 train 83.29%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:13:03.793059\n",
      "ep 311, loss: 14.62, 8000 train 84.23%\n",
      "ep 312, loss: 14.66, 8000 train 84.11%\n",
      "ep 313, loss: 14.38, 8000 train 84.09%\n",
      "ep 314, loss: 14.58, 8000 train 84.70%\n",
      "ep 315, loss: 15.00, 8000 train 83.55%\n",
      "ep 316, loss: 14.61, 8000 train 84.11%\n",
      "ep 317, loss: 14.78, 8000 train 83.96%\n",
      "ep 318, loss: 14.78, 8000 train 84.40%\n",
      "ep 319, loss: 14.49, 8000 train 83.80%\n",
      "ep 320, loss: 14.70, 8000 train 84.11%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:17:21.830040\n",
      "ep 321, loss: 15.32, 8000 train 83.55%\n",
      "ep 322, loss: 14.53, 8000 train 84.14%\n",
      "ep 323, loss: 14.07, 8000 train 84.72%\n",
      "ep 324, loss: 14.46, 8000 train 83.86%\n",
      "ep 325, loss: 14.57, 8000 train 84.12%\n",
      "ep 326, loss: 14.64, 8000 train 84.05%\n",
      "ep 327, loss: 14.31, 8000 train 84.10%\n",
      "ep 328, loss: 14.83, 8000 train 84.10%\n",
      "ep 329, loss: 14.08, 8000 train 84.55%\n",
      "ep 330, loss: 14.38, 8000 train 84.23%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:21:50.004575\n",
      "ep 331, loss: 13.68, 8000 train 84.79%\n",
      "ep 332, loss: 14.45, 8000 train 84.09%\n",
      "ep 333, loss: 13.89, 8000 train 84.75%\n",
      "ep 334, loss: 13.54, 8000 train 85.10%\n",
      "ep 335, loss: 14.22, 8000 train 84.33%\n",
      "ep 336, loss: 14.45, 8000 train 84.23%\n",
      "ep 337, loss: 14.65, 8000 train 84.33%\n",
      "ep 338, loss: 14.04, 8000 train 84.39%\n",
      "ep 339, loss: 14.01, 8000 train 84.41%\n",
      "ep 340, loss: 13.96, 8000 train 84.59%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:26:21.978881\n",
      "ep 341, loss: 13.74, 8000 train 85.08%\n",
      "ep 342, loss: 13.42, 8000 train 85.42%\n",
      "ep 343, loss: 14.16, 8000 train 84.46%\n",
      "ep 344, loss: 13.77, 8000 train 84.95%\n",
      "ep 345, loss: 13.49, 8000 train 85.36%\n",
      "ep 346, loss: 14.22, 8000 train 84.86%\n",
      "ep 347, loss: 13.84, 8000 train 84.09%\n",
      "ep 348, loss: 13.40, 8000 train 85.74%\n",
      "ep 349, loss: 13.74, 8000 train 85.10%\n",
      "ep 350, loss: 13.59, 8000 train 84.85%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:30:38.278246\n",
      "   Model saved to v4_350_saved.pth\n",
      "ep 351, loss: 13.68, 8000 train 84.84%\n",
      "ep 352, loss: 13.43, 8000 train 85.36%\n",
      "ep 353, loss: 13.76, 8000 train 85.10%\n",
      "ep 354, loss: 13.24, 8000 train 85.28%\n",
      "ep 355, loss: 13.22, 8000 train 85.11%\n",
      "ep 356, loss: 13.51, 8000 train 85.41%\n",
      "ep 357, loss: 13.56, 8000 train 85.59%\n",
      "ep 358, loss: 13.52, 8000 train 85.21%\n",
      "ep 359, loss: 13.56, 8000 train 85.52%\n",
      "ep 360, loss: 13.42, 8000 train 85.84%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:34:52.640516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 361, loss: 13.26, 8000 train 85.28%\n",
      "ep 362, loss: 12.98, 8000 train 86.12%\n",
      "ep 363, loss: 13.63, 8000 train 84.92%\n",
      "ep 364, loss: 13.65, 8000 train 85.25%\n",
      "ep 365, loss: 13.50, 8000 train 85.54%\n",
      "ep 366, loss: 13.98, 8000 train 84.97%\n",
      "ep 367, loss: 13.86, 8000 train 84.97%\n",
      "ep 368, loss: 13.56, 8000 train 85.54%\n",
      "ep 369, loss: 13.46, 8000 train 85.36%\n",
      "ep 370, loss: 13.36, 8000 train 85.80%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:39:23.514740\n",
      "ep 371, loss: 13.54, 8000 train 85.08%\n",
      "ep 372, loss: 13.62, 8000 train 85.20%\n",
      "ep 373, loss: 13.16, 8000 train 85.55%\n",
      "ep 374, loss: 13.37, 8000 train 85.49%\n",
      "ep 375, loss: 13.65, 8000 train 84.96%\n",
      "ep 376, loss: 13.17, 8000 train 85.46%\n",
      "ep 377, loss: 13.15, 8000 train 86.10%\n",
      "ep 378, loss: 12.80, 8000 train 86.26%\n",
      "ep 379, loss: 13.40, 8000 train 85.75%\n",
      "ep 380, loss: 13.14, 8000 train 85.54%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:43:39.575371\n",
      "ep 381, loss: 12.88, 8000 train 86.21%\n",
      "ep 382, loss: 12.89, 8000 train 85.86%\n",
      "ep 383, loss: 13.38, 8000 train 85.61%\n",
      "ep 384, loss: 13.03, 8000 train 85.24%\n",
      "ep 385, loss: 12.62, 8000 train 86.05%\n",
      "ep 386, loss: 13.57, 8000 train 85.35%\n",
      "ep 387, loss: 13.81, 8000 train 85.15%\n",
      "ep 388, loss: 12.88, 8000 train 86.20%\n",
      "ep 389, loss: 12.94, 8000 train 86.20%\n",
      "ep 390, loss: 13.70, 8000 train 85.34%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:48:18.799743\n",
      "ep 391, loss: 13.27, 8000 train 85.45%\n",
      "ep 392, loss: 12.72, 8000 train 86.22%\n",
      "ep 393, loss: 12.72, 8000 train 86.34%\n",
      "ep 394, loss: 13.02, 8000 train 85.84%\n",
      "ep 395, loss: 12.50, 8000 train 86.20%\n",
      "ep 396, loss: 12.36, 8000 train 87.04%\n",
      "ep 397, loss: 12.43, 8000 train 86.28%\n",
      "ep 398, loss: 12.66, 8000 train 85.84%\n",
      "ep 399, loss: 12.50, 8000 train 86.34%\n",
      "ep 400, loss: 12.63, 8000 train 86.58%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:52:35.499303\n",
      "   Model saved to v4_400_saved.pth\n",
      "ep 401, loss: 12.17, 8000 train 86.67%\n",
      "ep 402, loss: 12.14, 8000 train 86.74%\n",
      "ep 403, loss: 12.78, 8000 train 85.66%\n",
      "ep 404, loss: 12.78, 8000 train 86.20%\n",
      "ep 405, loss: 12.40, 8000 train 86.15%\n",
      "ep 406, loss: 12.45, 8000 train 86.29%\n",
      "ep 407, loss: 12.07, 8000 train 86.94%\n",
      "ep 408, loss: 12.62, 8000 train 85.95%\n",
      "ep 409, loss: 12.54, 8000 train 86.28%\n",
      "ep 410, loss: 12.90, 8000 train 86.26%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:57:07.722884\n",
      "ep 411, loss: 12.24, 8000 train 86.74%\n",
      "ep 412, loss: 12.36, 8000 train 86.24%\n",
      "ep 413, loss: 12.48, 8000 train 86.34%\n",
      "ep 414, loss: 11.93, 8000 train 87.04%\n",
      "ep 415, loss: 12.50, 8000 train 85.95%\n",
      "ep 416, loss: 12.02, 8000 train 86.65%\n",
      "ep 417, loss: 12.22, 8000 train 86.49%\n",
      "ep 418, loss: 12.17, 8000 train 87.04%\n",
      "ep 419, loss: 12.25, 8000 train 86.71%\n",
      "ep 420, loss: 11.92, 8000 train 87.38%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:01:33.967486\n",
      "ep 421, loss: 12.35, 8000 train 86.50%\n",
      "ep 422, loss: 12.06, 8000 train 86.78%\n",
      "ep 423, loss: 12.69, 8000 train 86.28%\n",
      "ep 424, loss: 11.99, 8000 train 86.74%\n",
      "ep 425, loss: 11.79, 8000 train 87.09%\n",
      "ep 426, loss: 12.02, 8000 train 86.75%\n",
      "ep 427, loss: 11.92, 8000 train 86.76%\n",
      "ep 428, loss: 11.41, 8000 train 87.31%\n",
      "ep 429, loss: 11.78, 8000 train 87.17%\n",
      "ep 430, loss: 11.66, 8000 train 87.54%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:06:18.810634\n",
      "ep 431, loss: 11.78, 8000 train 87.35%\n",
      "ep 432, loss: 12.40, 8000 train 86.40%\n",
      "ep 433, loss: 12.56, 8000 train 86.70%\n",
      "ep 434, loss: 11.67, 8000 train 87.06%\n",
      "ep 435, loss: 12.06, 8000 train 86.96%\n",
      "ep 436, loss: 12.02, 8000 train 86.84%\n",
      "ep 437, loss: 11.71, 8000 train 86.84%\n",
      "ep 438, loss: 12.36, 8000 train 86.81%\n",
      "ep 439, loss: 12.56, 8000 train 86.44%\n",
      "ep 440, loss: 12.61, 8000 train 85.91%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:10:44.251590\n",
      "ep 441, loss: 11.74, 8000 train 87.09%\n",
      "ep 442, loss: 12.35, 8000 train 86.49%\n",
      "ep 443, loss: 12.05, 8000 train 87.22%\n",
      "ep 444, loss: 11.86, 8000 train 87.22%\n",
      "ep 445, loss: 11.42, 8000 train 87.80%\n",
      "ep 446, loss: 11.45, 8000 train 87.15%\n",
      "ep 447, loss: 11.74, 8000 train 86.79%\n",
      "ep 448, loss: 11.92, 8000 train 86.98%\n",
      "ep 449, loss: 12.26, 8000 train 87.09%\n",
      "ep 450, loss: 11.53, 8000 train 87.30%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:15:21.135774\n",
      "   Model saved to v4_450_saved.pth\n",
      "ep 451, loss: 11.90, 8000 train 87.10%\n",
      "ep 452, loss: 11.23, 8000 train 87.34%\n",
      "ep 453, loss: 12.05, 8000 train 87.46%\n",
      "ep 454, loss: 12.38, 8000 train 86.48%\n",
      "ep 455, loss: 11.68, 8000 train 87.44%\n",
      "ep 456, loss: 11.96, 8000 train 86.75%\n",
      "ep 457, loss: 11.69, 8000 train 87.51%\n",
      "ep 458, loss: 11.79, 8000 train 87.20%\n",
      "ep 459, loss: 11.38, 8000 train 87.49%\n",
      "ep 460, loss: 11.58, 8000 train 87.86%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:19:54.916055\n",
      "ep 461, loss: 11.68, 8000 train 86.72%\n",
      "ep 462, loss: 10.98, 8000 train 88.09%\n",
      "ep 463, loss: 11.47, 8000 train 87.29%\n",
      "ep 464, loss: 11.45, 8000 train 87.01%\n",
      "ep 465, loss: 11.64, 8000 train 86.90%\n",
      "ep 466, loss: 11.49, 8000 train 87.40%\n",
      "ep 467, loss: 11.12, 8000 train 87.88%\n",
      "ep 468, loss: 11.52, 8000 train 87.26%\n",
      "ep 469, loss: 11.43, 8000 train 87.17%\n",
      "ep 470, loss: 11.29, 8000 train 87.51%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:24:29.278088\n",
      "ep 471, loss: 11.24, 8000 train 87.83%\n",
      "ep 472, loss: 11.84, 8000 train 87.30%\n",
      "ep 473, loss: 10.85, 8000 train 88.02%\n",
      "ep 474, loss: 12.04, 8000 train 86.79%\n",
      "ep 475, loss: 11.52, 8000 train 87.22%\n",
      "ep 476, loss: 11.73, 8000 train 87.28%\n",
      "ep 477, loss: 11.21, 8000 train 87.99%\n",
      "ep 478, loss: 11.31, 8000 train 87.96%\n",
      "ep 479, loss: 11.19, 8000 train 88.41%\n",
      "ep 480, loss: 11.47, 8000 train 87.55%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:28:49.526616\n",
      "ep 481, loss: 11.50, 8000 train 87.35%\n",
      "ep 482, loss: 11.23, 8000 train 87.54%\n",
      "ep 483, loss: 10.88, 8000 train 88.55%\n",
      "ep 484, loss: 11.57, 8000 train 87.15%\n",
      "ep 485, loss: 11.61, 8000 train 87.55%\n",
      "ep 486, loss: 11.38, 8000 train 87.48%\n",
      "ep 487, loss: 11.47, 8000 train 87.58%\n",
      "ep 488, loss: 10.67, 8000 train 88.15%\n",
      "ep 489, loss: 10.72, 8000 train 87.70%\n",
      "ep 490, loss: 11.01, 8000 train 88.25%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:33:12.220469\n",
      "ep 491, loss: 11.34, 8000 train 87.81%\n",
      "ep 492, loss: 11.65, 8000 train 87.61%\n",
      "ep 493, loss: 10.97, 8000 train 88.10%\n",
      "ep 494, loss: 11.13, 8000 train 87.86%\n",
      "ep 495, loss: 11.22, 8000 train 87.91%\n",
      "ep 496, loss: 10.81, 8000 train 88.25%\n",
      "ep 497, loss: 10.60, 8000 train 88.72%\n",
      "ep 498, loss: 11.78, 8000 train 87.16%\n",
      "ep 499, loss: 10.88, 8000 train 88.26%\n",
      "ep 500, loss: 10.88, 8000 train 88.25%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:38:07.357543\n",
      "   Model saved to v4_500_saved.pth\n",
      "ep 501, loss: 10.71, 8000 train 88.17%\n",
      "ep 502, loss: 10.89, 8000 train 88.60%\n",
      "ep 503, loss: 11.17, 8000 train 87.70%\n",
      "ep 504, loss: 11.42, 8000 train 87.41%\n",
      "ep 505, loss: 10.50, 8000 train 88.33%\n",
      "ep 506, loss: 11.04, 8000 train 87.90%\n",
      "ep 507, loss: 11.12, 8000 train 87.72%\n",
      "ep 508, loss: 10.62, 8000 train 88.48%\n",
      "ep 509, loss: 9.92, 8000 train 89.41%\n",
      "ep 510, loss: 10.32, 8000 train 88.33%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:42:55.586404\n",
      "ep 511, loss: 10.53, 8000 train 88.71%\n",
      "ep 512, loss: 10.74, 8000 train 88.42%\n",
      "ep 513, loss: 10.55, 8000 train 88.26%\n",
      "ep 514, loss: 11.11, 8000 train 87.38%\n",
      "ep 515, loss: 11.03, 8000 train 88.01%\n",
      "ep 516, loss: 11.15, 8000 train 87.80%\n",
      "ep 517, loss: 11.08, 8000 train 87.76%\n",
      "ep 518, loss: 10.88, 8000 train 88.00%\n",
      "ep 519, loss: 11.16, 8000 train 88.20%\n",
      "ep 520, loss: 10.93, 8000 train 88.48%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:47:41.309233\n",
      "ep 521, loss: 11.31, 8000 train 87.90%\n",
      "ep 522, loss: 10.69, 8000 train 88.49%\n",
      "ep 523, loss: 10.69, 8000 train 88.19%\n",
      "ep 524, loss: 10.80, 8000 train 88.33%\n",
      "ep 525, loss: 10.52, 8000 train 88.51%\n",
      "ep 526, loss: 10.57, 8000 train 88.74%\n",
      "ep 527, loss: 10.90, 8000 train 88.52%\n",
      "ep 528, loss: 10.41, 8000 train 88.85%\n",
      "ep 529, loss: 10.38, 8000 train 89.14%\n",
      "ep 530, loss: 10.74, 8000 train 88.41%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:52:36.300667\n",
      "ep 531, loss: 11.18, 8000 train 88.05%\n",
      "ep 532, loss: 10.68, 8000 train 88.65%\n",
      "ep 533, loss: 10.51, 8000 train 88.98%\n",
      "ep 534, loss: 10.38, 8000 train 88.45%\n",
      "ep 535, loss: 10.56, 8000 train 88.69%\n",
      "ep 536, loss: 10.96, 8000 train 88.24%\n",
      "ep 537, loss: 10.76, 8000 train 88.46%\n",
      "ep 538, loss: 10.14, 8000 train 88.85%\n",
      "ep 539, loss: 10.05, 8000 train 89.19%\n",
      "ep 540, loss: 10.10, 8000 train 88.91%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:57:34.796822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 541, loss: 11.18, 8000 train 87.71%\n",
      "ep 542, loss: 10.66, 8000 train 88.29%\n",
      "ep 543, loss: 10.37, 8000 train 88.52%\n",
      "ep 544, loss: 9.90, 8000 train 89.08%\n",
      "ep 545, loss: 10.68, 8000 train 88.42%\n",
      "ep 546, loss: 10.52, 8000 train 88.52%\n",
      "ep 547, loss: 10.43, 8000 train 88.64%\n",
      "ep 548, loss: 10.28, 8000 train 88.96%\n",
      "ep 549, loss: 10.59, 8000 train 88.79%\n",
      "ep 550, loss: 11.31, 8000 train 87.83%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:02:34.758165\n",
      "   Model saved to v4_550_saved.pth\n",
      "ep 551, loss: 10.50, 8000 train 89.16%\n",
      "ep 552, loss: 9.88, 8000 train 89.42%\n",
      "ep 553, loss: 9.63, 8000 train 89.36%\n",
      "ep 554, loss: 10.29, 8000 train 89.12%\n",
      "ep 555, loss: 9.93, 8000 train 89.21%\n",
      "ep 556, loss: 10.68, 8000 train 88.33%\n",
      "ep 557, loss: 10.24, 8000 train 88.83%\n",
      "ep 558, loss: 9.81, 8000 train 89.28%\n",
      "ep 559, loss: 10.46, 8000 train 89.10%\n",
      "ep 560, loss: 9.63, 8000 train 89.46%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:07:33.488529\n",
      "ep 561, loss: 10.25, 8000 train 88.89%\n",
      "ep 562, loss: 10.79, 8000 train 88.81%\n",
      "ep 563, loss: 10.08, 8000 train 89.61%\n",
      "ep 564, loss: 10.66, 8000 train 89.04%\n",
      "ep 565, loss: 10.53, 8000 train 88.50%\n",
      "ep 566, loss: 10.12, 8000 train 88.92%\n",
      "ep 567, loss: 10.61, 8000 train 88.26%\n",
      "ep 568, loss: 9.87, 8000 train 89.35%\n",
      "ep 569, loss: 10.04, 8000 train 88.72%\n",
      "ep 570, loss: 10.17, 8000 train 89.16%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:12:26.014760\n",
      "ep 571, loss: 9.89, 8000 train 89.00%\n",
      "ep 572, loss: 10.03, 8000 train 89.50%\n",
      "ep 573, loss: 9.75, 8000 train 89.45%\n",
      "ep 574, loss: 10.28, 8000 train 89.24%\n",
      "ep 575, loss: 9.78, 8000 train 89.25%\n",
      "ep 576, loss: 10.34, 8000 train 88.51%\n",
      "ep 577, loss: 10.36, 8000 train 88.86%\n",
      "ep 578, loss: 10.22, 8000 train 88.99%\n",
      "ep 579, loss: 9.57, 8000 train 89.86%\n",
      "ep 580, loss: 9.75, 8000 train 89.41%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:17:17.891726\n",
      "ep 581, loss: 10.08, 8000 train 89.22%\n",
      "ep 582, loss: 9.61, 8000 train 89.28%\n",
      "ep 583, loss: 9.95, 8000 train 89.24%\n",
      "ep 584, loss: 9.78, 8000 train 89.40%\n",
      "ep 585, loss: 9.86, 8000 train 89.53%\n",
      "ep 586, loss: 9.32, 8000 train 90.15%\n",
      "ep 587, loss: 9.77, 8000 train 89.81%\n",
      "ep 588, loss: 9.25, 8000 train 90.14%\n",
      "ep 589, loss: 9.91, 8000 train 88.86%\n",
      "ep 590, loss: 9.43, 8000 train 89.65%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:22:12.927591\n",
      "ep 591, loss: 10.28, 8000 train 89.04%\n",
      "ep 592, loss: 10.58, 8000 train 88.55%\n",
      "ep 593, loss: 10.31, 8000 train 88.86%\n",
      "ep 594, loss: 10.07, 8000 train 88.75%\n",
      "ep 595, loss: 9.54, 8000 train 89.85%\n",
      "ep 596, loss: 10.03, 8000 train 89.15%\n",
      "ep 597, loss: 9.48, 8000 train 89.44%\n",
      "ep 598, loss: 9.95, 8000 train 89.42%\n",
      "ep 599, loss: 9.36, 8000 train 89.42%\n",
      "ep 600, loss: 9.51, 8000 train 89.54%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:27:14.503123\n",
      "   Model saved to v4_600_saved.pth\n",
      "ep 601, loss: 9.82, 8000 train 89.29%\n",
      "ep 602, loss: 9.74, 8000 train 89.28%\n",
      "ep 603, loss: 9.48, 8000 train 89.58%\n",
      "ep 604, loss: 9.13, 8000 train 90.19%\n",
      "ep 605, loss: 10.04, 8000 train 89.09%\n",
      "ep 606, loss: 9.98, 8000 train 89.22%\n",
      "ep 607, loss: 9.48, 8000 train 89.64%\n",
      "ep 608, loss: 9.09, 8000 train 89.81%\n",
      "ep 609, loss: 9.11, 8000 train 90.18%\n",
      "ep 610, loss: 9.96, 8000 train 89.18%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:32:17.152517\n",
      "ep 611, loss: 9.09, 8000 train 89.76%\n",
      "ep 612, loss: 9.45, 8000 train 89.56%\n",
      "ep 613, loss: 9.95, 8000 train 89.01%\n",
      "ep 614, loss: 9.22, 8000 train 89.72%\n",
      "ep 615, loss: 9.36, 8000 train 89.86%\n",
      "ep 616, loss: 9.48, 8000 train 89.91%\n",
      "ep 617, loss: 9.17, 8000 train 89.91%\n",
      "ep 618, loss: 9.84, 8000 train 89.38%\n",
      "ep 619, loss: 9.71, 8000 train 89.41%\n",
      "ep 620, loss: 9.97, 8000 train 89.54%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:37:15.567397\n",
      "ep 621, loss: 9.22, 8000 train 89.41%\n",
      "ep 622, loss: 9.42, 8000 train 89.21%\n",
      "ep 623, loss: 9.28, 8000 train 90.14%\n",
      "ep 624, loss: 9.25, 8000 train 89.81%\n",
      "ep 625, loss: 9.33, 8000 train 90.05%\n",
      "ep 626, loss: 9.56, 8000 train 89.70%\n",
      "ep 627, loss: 9.33, 8000 train 89.55%\n",
      "ep 628, loss: 9.50, 8000 train 89.60%\n",
      "ep 629, loss: 8.94, 8000 train 90.26%\n",
      "ep 630, loss: 9.13, 8000 train 89.81%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:42:26.414117\n",
      "ep 631, loss: 9.80, 8000 train 89.30%\n",
      "ep 632, loss: 9.67, 8000 train 89.50%\n",
      "ep 633, loss: 9.56, 8000 train 89.91%\n",
      "ep 634, loss: 9.90, 8000 train 89.24%\n",
      "ep 635, loss: 8.89, 8000 train 90.28%\n",
      "ep 636, loss: 9.03, 8000 train 90.42%\n",
      "ep 637, loss: 9.32, 8000 train 90.00%\n",
      "ep 638, loss: 9.62, 8000 train 89.46%\n",
      "ep 639, loss: 9.41, 8000 train 89.72%\n",
      "ep 640, loss: 9.65, 8000 train 89.41%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:47:20.925366\n",
      "ep 641, loss: 9.38, 8000 train 89.76%\n",
      "ep 642, loss: 8.91, 8000 train 90.40%\n",
      "ep 643, loss: 9.09, 8000 train 90.56%\n",
      "ep 644, loss: 9.24, 8000 train 90.05%\n",
      "ep 645, loss: 8.61, 8000 train 90.66%\n",
      "ep 646, loss: 9.67, 8000 train 89.61%\n",
      "ep 647, loss: 9.16, 8000 train 90.59%\n",
      "ep 648, loss: 8.48, 8000 train 90.81%\n",
      "ep 649, loss: 9.16, 8000 train 89.69%\n",
      "ep 650, loss: 9.98, 8000 train 89.24%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:52:02.142697\n",
      "   Model saved to v4_650_saved.pth\n",
      "ep 651, loss: 9.57, 8000 train 89.31%\n",
      "ep 652, loss: 9.28, 8000 train 89.86%\n",
      "ep 653, loss: 8.99, 8000 train 90.04%\n",
      "ep 654, loss: 9.35, 8000 train 89.96%\n",
      "ep 655, loss: 9.25, 8000 train 90.22%\n",
      "ep 656, loss: 8.98, 8000 train 90.28%\n",
      "ep 657, loss: 8.96, 8000 train 90.41%\n",
      "ep 658, loss: 9.02, 8000 train 90.24%\n",
      "ep 659, loss: 8.88, 8000 train 90.55%\n",
      "ep 660, loss: 9.06, 8000 train 90.35%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 4:56:59.298339\n",
      "ep 661, loss: 8.83, 8000 train 90.30%\n",
      "ep 662, loss: 8.95, 8000 train 89.95%\n",
      "ep 663, loss: 9.26, 8000 train 90.06%\n",
      "ep 664, loss: 8.95, 8000 train 90.35%\n",
      "ep 665, loss: 8.51, 8000 train 90.65%\n",
      "ep 666, loss: 8.76, 8000 train 90.75%\n",
      "ep 667, loss: 9.29, 8000 train 89.96%\n",
      "ep 668, loss: 8.86, 8000 train 90.44%\n",
      "ep 669, loss: 9.32, 8000 train 90.18%\n",
      "ep 670, loss: 9.21, 8000 train 90.14%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:02:05.408795\n",
      "ep 671, loss: 9.08, 8000 train 90.35%\n",
      "ep 672, loss: 8.97, 8000 train 90.34%\n",
      "ep 673, loss: 8.94, 8000 train 90.49%\n",
      "ep 674, loss: 9.27, 8000 train 89.95%\n",
      "ep 675, loss: 8.69, 8000 train 90.69%\n",
      "ep 676, loss: 9.38, 8000 train 89.80%\n",
      "ep 677, loss: 9.16, 8000 train 89.92%\n",
      "ep 678, loss: 8.95, 8000 train 90.11%\n",
      "ep 679, loss: 8.98, 8000 train 90.35%\n",
      "ep 680, loss: 8.47, 8000 train 91.39%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:06:55.541957\n",
      "ep 681, loss: 8.97, 8000 train 90.20%\n",
      "ep 682, loss: 8.63, 8000 train 90.74%\n",
      "ep 683, loss: 8.42, 8000 train 90.67%\n",
      "ep 684, loss: 8.85, 8000 train 90.29%\n",
      "ep 685, loss: 8.62, 8000 train 90.45%\n",
      "ep 686, loss: 9.42, 8000 train 90.19%\n",
      "ep 687, loss: 8.49, 8000 train 90.85%\n",
      "ep 688, loss: 9.11, 8000 train 90.14%\n",
      "ep 689, loss: 8.59, 8000 train 90.71%\n",
      "ep 690, loss: 8.72, 8000 train 90.41%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:11:47.191290\n",
      "ep 691, loss: 8.99, 8000 train 90.09%\n",
      "ep 692, loss: 8.66, 8000 train 90.56%\n",
      "ep 693, loss: 8.87, 8000 train 90.38%\n",
      "ep 694, loss: 8.18, 8000 train 91.24%\n",
      "ep 695, loss: 8.67, 8000 train 90.56%\n",
      "ep 696, loss: 8.79, 8000 train 90.42%\n",
      "ep 697, loss: 8.71, 8000 train 90.64%\n",
      "ep 698, loss: 9.07, 8000 train 89.90%\n",
      "ep 699, loss: 8.60, 8000 train 90.62%\n",
      "ep 700, loss: 8.53, 8000 train 90.90%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:16:38.870213\n",
      "   Model saved to v4_700_saved.pth\n",
      "ep 701, loss: 9.06, 8000 train 90.16%\n",
      "ep 702, loss: 8.49, 8000 train 90.90%\n",
      "ep 703, loss: 9.09, 8000 train 90.51%\n",
      "ep 704, loss: 8.54, 8000 train 90.75%\n",
      "ep 705, loss: 8.97, 8000 train 90.11%\n",
      "ep 706, loss: 8.30, 8000 train 90.88%\n",
      "ep 707, loss: 8.70, 8000 train 90.65%\n",
      "ep 708, loss: 8.29, 8000 train 91.09%\n",
      "ep 709, loss: 8.47, 8000 train 90.70%\n",
      "ep 710, loss: 8.60, 8000 train 90.65%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:21:34.360358\n",
      "ep 711, loss: 8.32, 8000 train 90.69%\n",
      "ep 712, loss: 8.44, 8000 train 90.69%\n",
      "ep 713, loss: 8.53, 8000 train 90.90%\n",
      "ep 714, loss: 8.79, 8000 train 90.08%\n",
      "ep 715, loss: 8.64, 8000 train 90.81%\n",
      "ep 716, loss: 8.60, 8000 train 90.96%\n",
      "ep 717, loss: 8.40, 8000 train 90.67%\n",
      "ep 718, loss: 8.11, 8000 train 91.61%\n",
      "ep 719, loss: 8.34, 8000 train 90.61%\n",
      "ep 720, loss: 8.32, 8000 train 91.31%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:26:24.916915\n",
      "ep 721, loss: 8.36, 8000 train 90.56%\n",
      "ep 722, loss: 8.48, 8000 train 90.85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 723, loss: 8.34, 8000 train 91.15%\n",
      "ep 724, loss: 8.19, 8000 train 90.96%\n",
      "ep 725, loss: 8.46, 8000 train 90.40%\n",
      "ep 726, loss: 8.76, 8000 train 90.70%\n",
      "ep 727, loss: 8.45, 8000 train 90.99%\n",
      "ep 728, loss: 9.01, 8000 train 90.48%\n",
      "ep 729, loss: 8.77, 8000 train 90.48%\n",
      "ep 730, loss: 8.37, 8000 train 90.94%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:31:06.568544\n",
      "ep 731, loss: 8.84, 8000 train 90.79%\n",
      "ep 732, loss: 8.00, 8000 train 91.07%\n",
      "ep 733, loss: 8.10, 8000 train 91.21%\n",
      "ep 734, loss: 7.86, 8000 train 91.49%\n",
      "ep 735, loss: 8.20, 8000 train 91.03%\n",
      "ep 736, loss: 8.45, 8000 train 91.05%\n",
      "ep 737, loss: 8.51, 8000 train 90.85%\n",
      "ep 738, loss: 8.52, 8000 train 90.76%\n",
      "ep 739, loss: 8.81, 8000 train 90.39%\n",
      "ep 740, loss: 8.29, 8000 train 91.05%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:36:02.599024\n",
      "ep 741, loss: 7.97, 8000 train 91.29%\n",
      "ep 742, loss: 7.99, 8000 train 91.20%\n",
      "ep 743, loss: 8.23, 8000 train 91.19%\n",
      "ep 744, loss: 8.01, 8000 train 91.09%\n",
      "ep 745, loss: 8.38, 8000 train 90.97%\n",
      "ep 746, loss: 8.50, 8000 train 91.05%\n",
      "ep 747, loss: 8.44, 8000 train 90.79%\n",
      "ep 748, loss: 8.30, 8000 train 91.15%\n",
      "ep 749, loss: 8.08, 8000 train 90.96%\n",
      "ep 750, loss: 8.38, 8000 train 91.11%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:40:54.106676\n",
      "   Model saved to v4_750_saved.pth\n",
      "ep 751, loss: 8.11, 8000 train 91.50%\n",
      "ep 752, loss: 8.71, 8000 train 90.71%\n",
      "ep 753, loss: 9.00, 8000 train 90.62%\n",
      "ep 754, loss: 8.26, 8000 train 91.22%\n",
      "ep 755, loss: 8.27, 8000 train 90.91%\n",
      "ep 756, loss: 8.74, 8000 train 90.33%\n",
      "ep 757, loss: 8.01, 8000 train 91.57%\n",
      "ep 758, loss: 8.38, 8000 train 91.26%\n",
      "ep 759, loss: 8.41, 8000 train 91.11%\n",
      "ep 760, loss: 7.60, 8000 train 91.76%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:45:43.641616\n",
      "ep 761, loss: 7.68, 8000 train 91.70%\n",
      "ep 762, loss: 8.24, 8000 train 91.09%\n",
      "ep 763, loss: 8.31, 8000 train 91.11%\n",
      "ep 764, loss: 8.10, 8000 train 91.49%\n",
      "ep 765, loss: 8.57, 8000 train 90.90%\n",
      "ep 766, loss: 8.32, 8000 train 91.09%\n",
      "ep 767, loss: 8.04, 8000 train 91.35%\n",
      "ep 768, loss: 8.34, 8000 train 91.35%\n",
      "ep 769, loss: 8.19, 8000 train 90.79%\n",
      "ep 770, loss: 8.17, 8000 train 91.42%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:50:24.906138\n",
      "ep 771, loss: 8.61, 8000 train 90.60%\n",
      "ep 772, loss: 8.38, 8000 train 90.64%\n",
      "ep 773, loss: 7.91, 8000 train 91.76%\n",
      "ep 774, loss: 7.87, 8000 train 91.30%\n",
      "ep 775, loss: 8.04, 8000 train 91.56%\n",
      "ep 776, loss: 8.28, 8000 train 91.16%\n",
      "ep 777, loss: 7.82, 8000 train 91.55%\n",
      "ep 778, loss: 7.95, 8000 train 91.57%\n",
      "ep 779, loss: 8.25, 8000 train 91.38%\n",
      "ep 780, loss: 8.62, 8000 train 90.84%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 5:55:12.957188\n",
      "ep 781, loss: 7.97, 8000 train 91.19%\n",
      "ep 782, loss: 8.31, 8000 train 91.21%\n",
      "ep 783, loss: 7.84, 8000 train 91.53%\n",
      "ep 784, loss: 8.51, 8000 train 90.77%\n",
      "ep 785, loss: 8.31, 8000 train 91.20%\n",
      "ep 786, loss: 7.96, 8000 train 91.41%\n",
      "ep 787, loss: 7.76, 8000 train 91.67%\n",
      "ep 788, loss: 8.07, 8000 train 91.17%\n",
      "ep 789, loss: 7.97, 8000 train 91.61%\n",
      "ep 790, loss: 8.04, 8000 train 91.14%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:00:12.243702\n",
      "ep 791, loss: 7.82, 8000 train 91.39%\n",
      "ep 792, loss: 7.89, 8000 train 91.59%\n",
      "ep 793, loss: 7.91, 8000 train 91.30%\n",
      "ep 794, loss: 7.81, 8000 train 91.67%\n",
      "ep 795, loss: 8.77, 8000 train 90.49%\n",
      "ep 796, loss: 8.11, 8000 train 91.27%\n",
      "ep 797, loss: 8.36, 8000 train 91.03%\n",
      "ep 798, loss: 8.33, 8000 train 90.95%\n",
      "ep 799, loss: 7.70, 8000 train 91.61%\n",
      "ep 800, loss: 8.45, 8000 train 91.27%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:05:01.958330\n",
      "   Model saved to v4_800_saved.pth\n",
      "ep 801, loss: 7.90, 8000 train 91.42%\n",
      "ep 802, loss: 7.89, 8000 train 91.55%\n",
      "ep 803, loss: 8.16, 8000 train 90.96%\n",
      "ep 804, loss: 7.86, 8000 train 91.41%\n",
      "ep 805, loss: 7.94, 8000 train 91.21%\n",
      "ep 806, loss: 8.01, 8000 train 91.39%\n",
      "ep 807, loss: 8.02, 8000 train 91.40%\n",
      "ep 808, loss: 7.37, 8000 train 92.41%\n",
      "ep 809, loss: 7.88, 8000 train 91.80%\n",
      "ep 810, loss: 7.80, 8000 train 91.26%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:10:05.318706\n",
      "ep 811, loss: 8.21, 8000 train 91.20%\n",
      "ep 812, loss: 7.99, 8000 train 91.10%\n",
      "ep 813, loss: 7.94, 8000 train 91.57%\n",
      "ep 814, loss: 7.96, 8000 train 91.33%\n",
      "ep 815, loss: 7.96, 8000 train 91.22%\n",
      "ep 816, loss: 7.57, 8000 train 91.89%\n",
      "ep 817, loss: 7.92, 8000 train 91.57%\n",
      "ep 818, loss: 8.28, 8000 train 91.21%\n",
      "ep 819, loss: 8.14, 8000 train 91.26%\n",
      "ep 820, loss: 7.94, 8000 train 91.47%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:15:05.490353\n",
      "ep 821, loss: 7.35, 8000 train 92.26%\n",
      "ep 822, loss: 8.00, 8000 train 91.44%\n",
      "ep 823, loss: 7.92, 8000 train 91.20%\n",
      "ep 824, loss: 7.44, 8000 train 91.83%\n",
      "ep 825, loss: 7.52, 8000 train 91.72%\n",
      "ep 826, loss: 7.70, 8000 train 92.03%\n",
      "ep 827, loss: 7.55, 8000 train 92.04%\n",
      "ep 828, loss: 7.98, 8000 train 91.39%\n",
      "ep 829, loss: 8.42, 8000 train 90.88%\n",
      "ep 830, loss: 8.05, 8000 train 91.27%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:20:10.903379\n",
      "ep 831, loss: 7.70, 8000 train 91.57%\n",
      "ep 832, loss: 7.71, 8000 train 91.81%\n",
      "ep 833, loss: 7.79, 8000 train 91.80%\n",
      "ep 834, loss: 7.66, 8000 train 92.06%\n",
      "ep 835, loss: 8.08, 8000 train 91.20%\n",
      "ep 836, loss: 7.60, 8000 train 91.80%\n",
      "ep 837, loss: 7.93, 8000 train 91.64%\n",
      "ep 838, loss: 8.33, 8000 train 90.85%\n",
      "ep 839, loss: 7.65, 8000 train 91.99%\n",
      "ep 840, loss: 7.59, 8000 train 91.81%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:25:32.517259\n",
      "ep 841, loss: 7.16, 8000 train 92.31%\n",
      "ep 842, loss: 7.78, 8000 train 91.47%\n",
      "ep 843, loss: 7.74, 8000 train 91.67%\n",
      "ep 844, loss: 7.81, 8000 train 91.53%\n",
      "ep 845, loss: 7.62, 8000 train 91.84%\n",
      "ep 846, loss: 7.57, 8000 train 91.90%\n",
      "ep 847, loss: 7.45, 8000 train 91.96%\n",
      "ep 848, loss: 7.83, 8000 train 91.04%\n",
      "ep 849, loss: 8.12, 8000 train 91.38%\n",
      "ep 850, loss: 8.14, 8000 train 91.24%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:30:36.058109\n",
      "   Model saved to v4_850_saved.pth\n",
      "ep 851, loss: 7.27, 8000 train 91.94%\n",
      "ep 852, loss: 7.46, 8000 train 92.11%\n",
      "ep 853, loss: 7.39, 8000 train 91.90%\n",
      "ep 854, loss: 7.37, 8000 train 91.90%\n",
      "ep 855, loss: 7.34, 8000 train 92.20%\n",
      "ep 856, loss: 7.91, 8000 train 91.38%\n",
      "ep 857, loss: 7.83, 8000 train 91.72%\n",
      "ep 858, loss: 7.68, 8000 train 91.64%\n",
      "ep 859, loss: 7.75, 8000 train 91.72%\n",
      "ep 860, loss: 7.82, 8000 train 91.51%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:35:41.558537\n",
      "ep 861, loss: 7.19, 8000 train 92.10%\n",
      "ep 862, loss: 7.37, 8000 train 91.95%\n",
      "ep 863, loss: 7.44, 8000 train 91.90%\n",
      "ep 864, loss: 7.91, 8000 train 91.54%\n",
      "ep 865, loss: 7.57, 8000 train 91.74%\n",
      "ep 866, loss: 7.34, 8000 train 92.35%\n",
      "ep 867, loss: 7.76, 8000 train 91.96%\n",
      "ep 868, loss: 7.74, 8000 train 91.62%\n",
      "ep 869, loss: 7.36, 8000 train 92.01%\n",
      "ep 870, loss: 7.53, 8000 train 91.81%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:40:41.936658\n",
      "ep 871, loss: 7.45, 8000 train 91.65%\n",
      "ep 872, loss: 7.33, 8000 train 91.89%\n",
      "ep 873, loss: 7.15, 8000 train 92.22%\n",
      "ep 874, loss: 7.35, 8000 train 92.11%\n",
      "ep 875, loss: 7.14, 8000 train 92.10%\n",
      "ep 876, loss: 7.22, 8000 train 92.24%\n",
      "ep 877, loss: 7.47, 8000 train 92.22%\n",
      "ep 878, loss: 7.64, 8000 train 91.92%\n",
      "ep 879, loss: 7.85, 8000 train 91.67%\n",
      "ep 880, loss: 7.57, 8000 train 91.74%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:45:59.732883\n",
      "ep 881, loss: 6.94, 8000 train 92.55%\n",
      "ep 882, loss: 7.37, 8000 train 92.20%\n",
      "ep 883, loss: 7.47, 8000 train 92.10%\n",
      "ep 884, loss: 7.24, 8000 train 92.09%\n",
      "ep 885, loss: 7.37, 8000 train 92.04%\n",
      "ep 886, loss: 7.77, 8000 train 91.76%\n",
      "ep 887, loss: 7.24, 8000 train 91.97%\n",
      "ep 888, loss: 7.37, 8000 train 91.91%\n",
      "ep 889, loss: 7.24, 8000 train 92.11%\n",
      "ep 890, loss: 7.42, 8000 train 92.20%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:50:48.421430\n",
      "ep 891, loss: 7.52, 8000 train 91.62%\n",
      "ep 892, loss: 7.06, 8000 train 92.33%\n",
      "ep 893, loss: 7.21, 8000 train 92.16%\n",
      "ep 894, loss: 7.30, 8000 train 92.12%\n",
      "ep 895, loss: 7.88, 8000 train 91.55%\n",
      "ep 896, loss: 7.19, 8000 train 91.92%\n",
      "ep 897, loss: 7.20, 8000 train 92.35%\n",
      "ep 898, loss: 7.24, 8000 train 92.10%\n",
      "ep 899, loss: 7.32, 8000 train 92.04%\n",
      "ep 900, loss: 6.87, 8000 train 92.46%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 6:55:49.723350\n",
      "   Model saved to v4_900_saved.pth\n",
      "ep 901, loss: 7.61, 8000 train 91.75%\n",
      "ep 902, loss: 7.35, 8000 train 92.01%\n",
      "ep 903, loss: 7.30, 8000 train 92.04%\n",
      "ep 904, loss: 6.95, 8000 train 92.55%\n",
      "ep 905, loss: 7.34, 8000 train 91.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 906, loss: 7.28, 8000 train 92.24%\n",
      "ep 907, loss: 7.27, 8000 train 92.27%\n",
      "ep 908, loss: 7.47, 8000 train 91.77%\n",
      "ep 909, loss: 6.90, 8000 train 92.65%\n",
      "ep 910, loss: 7.02, 8000 train 92.25%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:00:44.175190\n",
      "ep 911, loss: 6.86, 8000 train 92.31%\n",
      "ep 912, loss: 7.28, 8000 train 92.45%\n",
      "ep 913, loss: 7.61, 8000 train 92.05%\n",
      "ep 914, loss: 7.56, 8000 train 91.66%\n",
      "ep 915, loss: 7.06, 8000 train 92.69%\n",
      "ep 916, loss: 7.46, 8000 train 92.44%\n",
      "ep 917, loss: 7.72, 8000 train 91.76%\n",
      "ep 918, loss: 7.02, 8000 train 92.71%\n",
      "ep 919, loss: 7.46, 8000 train 92.10%\n",
      "ep 920, loss: 7.30, 8000 train 92.22%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:05:57.043371\n",
      "ep 921, loss: 6.88, 8000 train 92.67%\n",
      "ep 922, loss: 6.89, 8000 train 92.56%\n",
      "ep 923, loss: 7.48, 8000 train 91.85%\n",
      "ep 924, loss: 7.04, 8000 train 92.49%\n",
      "ep 925, loss: 7.27, 8000 train 92.33%\n",
      "ep 926, loss: 7.41, 8000 train 91.92%\n",
      "ep 927, loss: 7.00, 8000 train 92.29%\n",
      "ep 928, loss: 7.21, 8000 train 91.94%\n",
      "ep 929, loss: 7.39, 8000 train 92.03%\n",
      "ep 930, loss: 7.17, 8000 train 92.06%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:11:05.570777\n",
      "ep 931, loss: 6.94, 8000 train 92.30%\n",
      "ep 932, loss: 7.25, 8000 train 92.21%\n",
      "ep 933, loss: 7.53, 8000 train 91.81%\n",
      "ep 934, loss: 7.43, 8000 train 92.26%\n",
      "ep 935, loss: 6.99, 8000 train 92.62%\n",
      "ep 936, loss: 6.88, 8000 train 92.33%\n",
      "ep 937, loss: 6.92, 8000 train 92.41%\n",
      "ep 938, loss: 6.74, 8000 train 92.77%\n",
      "ep 939, loss: 7.14, 8000 train 92.29%\n",
      "ep 940, loss: 6.75, 8000 train 92.54%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:16:10.109970\n",
      "ep 941, loss: 7.16, 8000 train 92.11%\n",
      "ep 942, loss: 7.00, 8000 train 92.46%\n",
      "ep 943, loss: 7.26, 8000 train 92.66%\n",
      "ep 944, loss: 7.18, 8000 train 91.96%\n",
      "ep 945, loss: 7.12, 8000 train 92.21%\n",
      "ep 946, loss: 6.76, 8000 train 92.80%\n",
      "ep 947, loss: 6.98, 8000 train 92.59%\n",
      "ep 948, loss: 6.70, 8000 train 92.76%\n",
      "ep 949, loss: 7.32, 8000 train 91.94%\n",
      "ep 950, loss: 7.35, 8000 train 92.07%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:20:47.368987\n",
      "   Model saved to v4_950_saved.pth\n",
      "ep 951, loss: 7.24, 8000 train 91.92%\n",
      "ep 952, loss: 6.93, 8000 train 92.22%\n",
      "ep 953, loss: 7.40, 8000 train 92.17%\n",
      "ep 954, loss: 7.02, 8000 train 92.27%\n",
      "ep 955, loss: 7.10, 8000 train 92.60%\n",
      "ep 956, loss: 7.58, 8000 train 92.03%\n",
      "ep 957, loss: 6.98, 8000 train 92.62%\n",
      "ep 958, loss: 6.90, 8000 train 92.75%\n",
      "ep 959, loss: 7.22, 8000 train 92.34%\n",
      "ep 960, loss: 7.14, 8000 train 92.47%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:25:49.487234\n",
      "ep 961, loss: 7.07, 8000 train 92.80%\n",
      "ep 962, loss: 7.12, 8000 train 92.20%\n",
      "ep 963, loss: 6.79, 8000 train 92.89%\n",
      "ep 964, loss: 6.70, 8000 train 92.75%\n",
      "ep 965, loss: 7.19, 8000 train 92.27%\n",
      "ep 966, loss: 6.75, 8000 train 92.90%\n",
      "ep 967, loss: 6.63, 8000 train 92.70%\n",
      "ep 968, loss: 7.32, 8000 train 92.11%\n",
      "ep 969, loss: 6.90, 8000 train 92.47%\n",
      "ep 970, loss: 6.87, 8000 train 92.84%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:30:39.803304\n",
      "ep 971, loss: 6.98, 8000 train 92.61%\n",
      "ep 972, loss: 6.71, 8000 train 92.66%\n",
      "ep 973, loss: 6.59, 8000 train 93.03%\n",
      "ep 974, loss: 6.91, 8000 train 92.42%\n",
      "ep 975, loss: 7.08, 8000 train 92.40%\n",
      "ep 976, loss: 6.48, 8000 train 92.85%\n",
      "ep 977, loss: 6.89, 8000 train 92.71%\n",
      "ep 978, loss: 7.08, 8000 train 92.69%\n",
      "ep 979, loss: 6.53, 8000 train 92.77%\n",
      "ep 980, loss: 7.41, 8000 train 92.11%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:35:44.621254\n",
      "ep 981, loss: 6.96, 8000 train 92.79%\n",
      "ep 982, loss: 6.33, 8000 train 93.12%\n",
      "ep 983, loss: 6.72, 8000 train 92.38%\n",
      "ep 984, loss: 6.65, 8000 train 92.84%\n",
      "ep 985, loss: 6.48, 8000 train 92.83%\n",
      "ep 986, loss: 6.87, 8000 train 93.08%\n",
      "ep 987, loss: 7.27, 8000 train 92.27%\n",
      "ep 988, loss: 7.02, 8000 train 92.25%\n",
      "ep 989, loss: 7.44, 8000 train 91.86%\n",
      "ep 990, loss: 6.85, 8000 train 92.50%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:40:33.983840\n",
      "ep 991, loss: 6.66, 8000 train 92.65%\n",
      "ep 992, loss: 7.32, 8000 train 92.17%\n",
      "ep 993, loss: 6.72, 8000 train 92.67%\n",
      "ep 994, loss: 6.59, 8000 train 92.62%\n",
      "ep 995, loss: 6.67, 8000 train 92.96%\n",
      "ep 996, loss: 6.57, 8000 train 92.94%\n",
      "ep 997, loss: 6.71, 8000 train 92.97%\n",
      "ep 998, loss: 7.22, 8000 train 92.33%\n",
      "ep 999, loss: 6.88, 8000 train 92.71%\n",
      "ep 1000, loss: 6.76, 8000 train 92.58%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:45:40.152148\n",
      "   Model saved to v4_1000_saved.pth\n",
      "ep 1001, loss: 7.29, 8000 train 92.04%\n",
      "ep 1002, loss: 6.76, 8000 train 92.81%\n",
      "ep 1003, loss: 7.03, 8000 train 92.33%\n",
      "ep 1004, loss: 6.89, 8000 train 92.38%\n",
      "ep 1005, loss: 6.26, 8000 train 93.44%\n",
      "ep 1006, loss: 7.04, 8000 train 92.55%\n",
      "ep 1007, loss: 7.00, 8000 train 92.55%\n",
      "ep 1008, loss: 7.34, 8000 train 92.24%\n",
      "ep 1009, loss: 6.84, 8000 train 92.86%\n",
      "ep 1010, loss: 7.29, 8000 train 92.21%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:50:54.442409\n",
      "ep 1011, loss: 6.63, 8000 train 93.10%\n",
      "ep 1012, loss: 6.96, 8000 train 92.50%\n",
      "ep 1013, loss: 6.91, 8000 train 92.64%\n",
      "ep 1014, loss: 6.90, 8000 train 92.60%\n",
      "ep 1015, loss: 6.69, 8000 train 92.89%\n",
      "ep 1016, loss: 6.77, 8000 train 92.88%\n",
      "ep 1017, loss: 6.72, 8000 train 92.75%\n",
      "ep 1018, loss: 6.55, 8000 train 92.69%\n",
      "ep 1019, loss: 6.36, 8000 train 93.30%\n",
      "ep 1020, loss: 6.74, 8000 train 92.74%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 7:55:45.562453\n",
      "ep 1021, loss: 7.05, 8000 train 92.58%\n",
      "ep 1022, loss: 6.87, 8000 train 92.62%\n",
      "ep 1023, loss: 6.64, 8000 train 92.84%\n",
      "ep 1024, loss: 6.52, 8000 train 93.25%\n",
      "ep 1025, loss: 6.82, 8000 train 92.80%\n",
      "ep 1026, loss: 6.34, 8000 train 93.10%\n",
      "ep 1027, loss: 6.59, 8000 train 92.96%\n",
      "ep 1028, loss: 6.69, 8000 train 93.21%\n",
      "ep 1029, loss: 6.64, 8000 train 92.73%\n",
      "ep 1030, loss: 6.38, 8000 train 92.69%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:00:51.253085\n",
      "ep 1031, loss: 6.30, 8000 train 93.24%\n",
      "ep 1032, loss: 6.88, 8000 train 92.77%\n",
      "ep 1033, loss: 6.30, 8000 train 93.58%\n",
      "ep 1034, loss: 6.90, 8000 train 92.64%\n",
      "ep 1035, loss: 6.78, 8000 train 92.88%\n",
      "ep 1036, loss: 7.03, 8000 train 92.47%\n",
      "ep 1037, loss: 6.86, 8000 train 92.59%\n",
      "ep 1038, loss: 6.87, 8000 train 92.94%\n",
      "ep 1039, loss: 6.28, 8000 train 93.17%\n",
      "ep 1040, loss: 6.59, 8000 train 93.06%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:05:56.779228\n",
      "ep 1041, loss: 6.16, 8000 train 93.26%\n",
      "ep 1042, loss: 6.82, 8000 train 92.54%\n",
      "ep 1043, loss: 6.95, 8000 train 92.42%\n",
      "ep 1044, loss: 6.23, 8000 train 93.39%\n",
      "ep 1045, loss: 7.18, 8000 train 91.99%\n",
      "ep 1046, loss: 6.23, 8000 train 93.25%\n",
      "ep 1047, loss: 6.18, 8000 train 93.23%\n",
      "ep 1048, loss: 6.65, 8000 train 93.08%\n",
      "ep 1049, loss: 6.75, 8000 train 92.69%\n",
      "ep 1050, loss: 6.39, 8000 train 92.91%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:11:04.427655\n",
      "   Model saved to v4_1050_saved.pth\n",
      "ep 1051, loss: 5.92, 8000 train 93.54%\n",
      "ep 1052, loss: 6.59, 8000 train 92.95%\n",
      "ep 1053, loss: 5.76, 8000 train 93.65%\n",
      "ep 1054, loss: 6.47, 8000 train 93.19%\n",
      "ep 1055, loss: 6.76, 8000 train 92.64%\n",
      "ep 1056, loss: 6.63, 8000 train 92.51%\n",
      "ep 1057, loss: 6.88, 8000 train 92.59%\n",
      "ep 1058, loss: 6.61, 8000 train 92.97%\n",
      "ep 1059, loss: 6.98, 8000 train 92.56%\n",
      "ep 1060, loss: 6.17, 8000 train 93.64%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:16:13.891941\n",
      "ep 1061, loss: 5.94, 8000 train 93.74%\n",
      "ep 1062, loss: 6.72, 8000 train 92.77%\n",
      "ep 1063, loss: 6.52, 8000 train 92.80%\n",
      "ep 1064, loss: 6.63, 8000 train 92.81%\n",
      "ep 1065, loss: 6.32, 8000 train 93.11%\n",
      "ep 1066, loss: 6.62, 8000 train 92.83%\n",
      "ep 1067, loss: 6.60, 8000 train 93.29%\n",
      "ep 1068, loss: 6.51, 8000 train 92.77%\n",
      "ep 1069, loss: 6.92, 8000 train 92.65%\n",
      "ep 1070, loss: 6.59, 8000 train 92.89%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:21:08.923483\n",
      "ep 1071, loss: 6.36, 8000 train 93.34%\n",
      "ep 1072, loss: 6.06, 8000 train 93.39%\n",
      "ep 1073, loss: 6.46, 8000 train 92.95%\n",
      "ep 1074, loss: 6.25, 8000 train 93.27%\n",
      "ep 1075, loss: 6.88, 8000 train 92.73%\n",
      "ep 1076, loss: 6.65, 8000 train 92.74%\n",
      "ep 1077, loss: 6.61, 8000 train 93.05%\n",
      "ep 1078, loss: 6.48, 8000 train 92.80%\n",
      "ep 1079, loss: 6.76, 8000 train 92.90%\n",
      "ep 1080, loss: 6.11, 8000 train 93.26%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:25:55.502773\n",
      "ep 1081, loss: 6.62, 8000 train 92.96%\n",
      "ep 1082, loss: 6.40, 8000 train 93.25%\n",
      "ep 1083, loss: 5.95, 8000 train 93.55%\n",
      "ep 1084, loss: 6.56, 8000 train 93.15%\n",
      "ep 1085, loss: 6.57, 8000 train 93.04%\n",
      "ep 1086, loss: 6.64, 8000 train 92.85%\n",
      "ep 1087, loss: 6.55, 8000 train 93.36%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1088, loss: 6.50, 8000 train 93.16%\n",
      "ep 1089, loss: 6.76, 8000 train 92.50%\n",
      "ep 1090, loss: 6.82, 8000 train 92.70%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:30:52.615350\n",
      "ep 1091, loss: 6.48, 8000 train 93.03%\n",
      "ep 1092, loss: 6.26, 8000 train 93.17%\n",
      "ep 1093, loss: 6.57, 8000 train 93.01%\n",
      "ep 1094, loss: 6.82, 8000 train 92.73%\n",
      "ep 1095, loss: 6.76, 8000 train 92.99%\n",
      "ep 1096, loss: 6.06, 8000 train 93.54%\n",
      "ep 1097, loss: 6.40, 8000 train 93.24%\n",
      "ep 1098, loss: 6.60, 8000 train 93.01%\n",
      "ep 1099, loss: 6.36, 8000 train 93.24%\n",
      "ep 1100, loss: 6.40, 8000 train 93.17%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:35:54.739519\n",
      "   Model saved to v4_1100_saved.pth\n",
      "ep 1101, loss: 5.94, 8000 train 93.61%\n",
      "ep 1102, loss: 6.27, 8000 train 92.94%\n",
      "ep 1103, loss: 6.65, 8000 train 93.14%\n",
      "ep 1104, loss: 6.66, 8000 train 92.89%\n",
      "ep 1105, loss: 6.39, 8000 train 92.79%\n",
      "ep 1106, loss: 5.86, 8000 train 93.45%\n",
      "ep 1107, loss: 6.55, 8000 train 93.08%\n",
      "ep 1108, loss: 6.70, 8000 train 92.77%\n",
      "ep 1109, loss: 6.12, 8000 train 93.40%\n",
      "ep 1110, loss: 6.53, 8000 train 93.03%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:40:58.512868\n",
      "ep 1111, loss: 6.50, 8000 train 92.70%\n",
      "ep 1112, loss: 6.20, 8000 train 93.16%\n",
      "ep 1113, loss: 6.65, 8000 train 92.81%\n",
      "ep 1114, loss: 6.60, 8000 train 93.00%\n",
      "ep 1115, loss: 6.44, 8000 train 93.39%\n",
      "ep 1116, loss: 6.71, 8000 train 92.51%\n",
      "ep 1117, loss: 6.36, 8000 train 93.61%\n",
      "ep 1118, loss: 7.05, 8000 train 92.89%\n",
      "ep 1119, loss: 6.09, 8000 train 93.50%\n",
      "ep 1120, loss: 5.88, 8000 train 93.51%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:46:08.518385\n",
      "ep 1121, loss: 6.20, 8000 train 93.39%\n",
      "ep 1122, loss: 6.15, 8000 train 93.30%\n",
      "ep 1123, loss: 6.18, 8000 train 93.59%\n",
      "ep 1124, loss: 6.46, 8000 train 92.95%\n",
      "ep 1125, loss: 6.41, 8000 train 93.19%\n",
      "ep 1126, loss: 6.46, 8000 train 93.06%\n",
      "ep 1127, loss: 6.31, 8000 train 93.16%\n",
      "ep 1128, loss: 6.41, 8000 train 93.66%\n",
      "ep 1129, loss: 6.04, 8000 train 93.69%\n",
      "ep 1130, loss: 6.14, 8000 train 93.64%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:51:09.126517\n",
      "ep 1131, loss: 6.48, 8000 train 93.03%\n",
      "ep 1132, loss: 5.95, 8000 train 93.84%\n",
      "ep 1133, loss: 6.14, 8000 train 93.27%\n",
      "ep 1134, loss: 6.70, 8000 train 92.64%\n",
      "ep 1135, loss: 6.06, 8000 train 93.27%\n",
      "ep 1136, loss: 6.52, 8000 train 92.84%\n",
      "ep 1137, loss: 6.51, 8000 train 93.00%\n",
      "ep 1138, loss: 6.06, 8000 train 93.51%\n",
      "ep 1139, loss: 6.16, 8000 train 93.35%\n",
      "ep 1140, loss: 6.26, 8000 train 93.30%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 8:56:05.813910\n",
      "ep 1141, loss: 6.03, 8000 train 93.49%\n",
      "ep 1142, loss: 6.11, 8000 train 93.14%\n",
      "ep 1143, loss: 6.18, 8000 train 93.39%\n",
      "ep 1144, loss: 5.76, 8000 train 93.81%\n",
      "ep 1145, loss: 6.27, 8000 train 93.38%\n",
      "ep 1146, loss: 6.30, 8000 train 93.33%\n",
      "ep 1147, loss: 5.42, 8000 train 94.34%\n",
      "ep 1148, loss: 5.98, 8000 train 93.83%\n",
      "ep 1149, loss: 6.24, 8000 train 93.46%\n",
      "ep 1150, loss: 5.72, 8000 train 93.58%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:01:28.287820\n",
      "   Model saved to v4_1150_saved.pth\n",
      "ep 1151, loss: 6.22, 8000 train 93.36%\n",
      "ep 1152, loss: 6.16, 8000 train 93.25%\n",
      "ep 1153, loss: 5.75, 8000 train 93.67%\n",
      "ep 1154, loss: 6.10, 8000 train 93.58%\n",
      "ep 1155, loss: 5.98, 8000 train 93.51%\n",
      "ep 1156, loss: 6.21, 8000 train 93.30%\n",
      "ep 1157, loss: 6.09, 8000 train 93.41%\n",
      "ep 1158, loss: 6.03, 8000 train 93.53%\n",
      "ep 1159, loss: 6.00, 8000 train 93.33%\n",
      "ep 1160, loss: 6.24, 8000 train 93.11%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:06:43.804755\n",
      "ep 1161, loss: 5.63, 8000 train 93.64%\n",
      "ep 1162, loss: 6.06, 8000 train 93.25%\n",
      "ep 1163, loss: 6.16, 8000 train 93.15%\n",
      "ep 1164, loss: 5.99, 8000 train 93.77%\n",
      "ep 1165, loss: 5.90, 8000 train 93.80%\n",
      "ep 1166, loss: 6.40, 8000 train 93.08%\n",
      "ep 1167, loss: 6.04, 8000 train 93.79%\n",
      "ep 1168, loss: 6.40, 8000 train 93.20%\n",
      "ep 1169, loss: 6.18, 8000 train 93.54%\n",
      "ep 1170, loss: 6.36, 8000 train 93.64%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:11:46.110679\n",
      "ep 1171, loss: 6.84, 8000 train 92.67%\n",
      "ep 1172, loss: 6.57, 8000 train 92.88%\n",
      "ep 1173, loss: 6.25, 8000 train 93.38%\n",
      "ep 1174, loss: 6.20, 8000 train 93.30%\n",
      "ep 1175, loss: 6.28, 8000 train 93.33%\n",
      "ep 1176, loss: 6.27, 8000 train 93.05%\n",
      "ep 1177, loss: 6.42, 8000 train 93.11%\n",
      "ep 1178, loss: 6.07, 8000 train 93.47%\n",
      "ep 1179, loss: 6.26, 8000 train 93.34%\n",
      "ep 1180, loss: 5.78, 8000 train 93.76%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:16:48.948997\n",
      "ep 1181, loss: 5.69, 8000 train 93.88%\n",
      "ep 1182, loss: 6.32, 8000 train 93.14%\n",
      "ep 1183, loss: 5.96, 8000 train 93.80%\n",
      "ep 1184, loss: 5.60, 8000 train 93.99%\n",
      "ep 1185, loss: 5.94, 8000 train 93.74%\n",
      "ep 1186, loss: 6.45, 8000 train 92.97%\n",
      "ep 1187, loss: 6.43, 8000 train 93.51%\n",
      "ep 1188, loss: 6.44, 8000 train 93.09%\n",
      "ep 1189, loss: 6.27, 8000 train 93.45%\n",
      "ep 1190, loss: 6.14, 8000 train 93.38%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:22:05.216346\n",
      "ep 1191, loss: 6.04, 8000 train 93.88%\n",
      "ep 1192, loss: 6.01, 8000 train 93.56%\n",
      "ep 1193, loss: 5.88, 8000 train 93.70%\n",
      "ep 1194, loss: 6.16, 8000 train 93.54%\n",
      "ep 1195, loss: 5.87, 8000 train 93.64%\n",
      "ep 1196, loss: 6.01, 8000 train 93.61%\n",
      "ep 1197, loss: 5.72, 8000 train 93.90%\n",
      "ep 1198, loss: 6.08, 8000 train 93.66%\n",
      "ep 1199, loss: 5.59, 8000 train 94.26%\n",
      "ep 1200, loss: 5.82, 8000 train 93.36%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:27:08.660147\n",
      "   Model saved to v4_1200_saved.pth\n",
      "ep 1201, loss: 5.94, 8000 train 93.80%\n",
      "ep 1202, loss: 5.67, 8000 train 93.65%\n",
      "ep 1203, loss: 5.71, 8000 train 93.90%\n",
      "ep 1204, loss: 5.72, 8000 train 93.70%\n",
      "ep 1205, loss: 5.68, 8000 train 93.83%\n",
      "ep 1206, loss: 5.20, 8000 train 94.39%\n",
      "ep 1207, loss: 5.93, 8000 train 93.69%\n",
      "ep 1208, loss: 6.08, 8000 train 93.39%\n",
      "ep 1209, loss: 5.77, 8000 train 93.73%\n",
      "ep 1210, loss: 5.89, 8000 train 93.70%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:32:11.647644\n",
      "ep 1211, loss: 6.08, 8000 train 93.44%\n",
      "ep 1212, loss: 6.01, 8000 train 93.62%\n",
      "ep 1213, loss: 5.86, 8000 train 93.75%\n",
      "ep 1214, loss: 6.39, 8000 train 93.31%\n",
      "ep 1215, loss: 5.94, 8000 train 93.77%\n",
      "ep 1216, loss: 6.02, 8000 train 93.46%\n",
      "ep 1217, loss: 5.56, 8000 train 93.96%\n",
      "ep 1218, loss: 5.51, 8000 train 93.92%\n",
      "ep 1219, loss: 6.01, 8000 train 93.51%\n",
      "ep 1220, loss: 6.15, 8000 train 93.53%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:36:32.319225\n",
      "ep 1221, loss: 6.23, 8000 train 93.69%\n",
      "ep 1222, loss: 6.02, 8000 train 93.56%\n",
      "ep 1223, loss: 5.24, 8000 train 94.38%\n",
      "ep 1224, loss: 6.01, 8000 train 93.70%\n",
      "ep 1225, loss: 6.13, 8000 train 93.51%\n",
      "ep 1226, loss: 6.04, 8000 train 93.71%\n",
      "ep 1227, loss: 5.84, 8000 train 93.61%\n",
      "ep 1228, loss: 5.43, 8000 train 94.14%\n",
      "ep 1229, loss: 6.20, 8000 train 93.25%\n",
      "ep 1230, loss: 5.58, 8000 train 93.81%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:41:42.461112\n",
      "ep 1231, loss: 5.47, 8000 train 93.89%\n",
      "ep 1232, loss: 5.80, 8000 train 93.66%\n",
      "ep 1233, loss: 5.62, 8000 train 94.12%\n",
      "ep 1234, loss: 6.40, 8000 train 93.21%\n",
      "ep 1235, loss: 6.06, 8000 train 93.55%\n",
      "ep 1236, loss: 6.12, 8000 train 93.41%\n",
      "ep 1237, loss: 5.66, 8000 train 93.96%\n",
      "ep 1238, loss: 5.97, 8000 train 93.59%\n",
      "ep 1239, loss: 5.93, 8000 train 93.71%\n",
      "ep 1240, loss: 5.98, 8000 train 93.65%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:46:56.216373\n",
      "ep 1241, loss: 5.72, 8000 train 93.95%\n",
      "ep 1242, loss: 6.21, 8000 train 93.27%\n",
      "ep 1243, loss: 5.90, 8000 train 93.46%\n",
      "ep 1244, loss: 5.66, 8000 train 93.94%\n",
      "ep 1245, loss: 5.80, 8000 train 94.04%\n",
      "ep 1246, loss: 5.64, 8000 train 94.23%\n",
      "ep 1247, loss: 5.51, 8000 train 94.55%\n",
      "ep 1248, loss: 5.93, 8000 train 93.65%\n",
      "ep 1249, loss: 5.95, 8000 train 93.54%\n",
      "ep 1250, loss: 5.75, 8000 train 93.74%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:52:07.079072\n",
      "   Model saved to v4_1250_saved.pth\n",
      "ep 1251, loss: 5.62, 8000 train 93.90%\n",
      "ep 1252, loss: 6.41, 8000 train 93.27%\n",
      "ep 1253, loss: 5.84, 8000 train 93.62%\n",
      "ep 1254, loss: 5.85, 8000 train 93.69%\n",
      "ep 1255, loss: 5.27, 8000 train 94.35%\n",
      "ep 1256, loss: 5.60, 8000 train 93.76%\n",
      "ep 1257, loss: 5.80, 8000 train 93.75%\n",
      "ep 1258, loss: 5.74, 8000 train 93.74%\n",
      "ep 1259, loss: 5.26, 8000 train 94.11%\n",
      "ep 1260, loss: 5.89, 8000 train 93.94%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 9:57:07.432572\n",
      "ep 1261, loss: 5.93, 8000 train 93.70%\n",
      "ep 1262, loss: 5.63, 8000 train 93.97%\n",
      "ep 1263, loss: 5.39, 8000 train 94.36%\n",
      "ep 1264, loss: 5.76, 8000 train 93.60%\n",
      "ep 1265, loss: 5.56, 8000 train 93.89%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1266, loss: 5.50, 8000 train 93.76%\n",
      "ep 1267, loss: 5.54, 8000 train 94.24%\n",
      "ep 1268, loss: 5.84, 8000 train 93.86%\n",
      "ep 1269, loss: 6.27, 8000 train 93.09%\n",
      "ep 1270, loss: 5.23, 8000 train 94.30%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:02:07.675100\n",
      "ep 1271, loss: 5.96, 8000 train 93.67%\n",
      "ep 1272, loss: 5.69, 8000 train 93.99%\n",
      "ep 1273, loss: 6.02, 8000 train 93.47%\n",
      "ep 1274, loss: 5.71, 8000 train 93.64%\n",
      "ep 1275, loss: 5.71, 8000 train 94.11%\n",
      "ep 1276, loss: 5.66, 8000 train 94.10%\n",
      "ep 1277, loss: 5.66, 8000 train 93.97%\n",
      "ep 1278, loss: 5.90, 8000 train 93.62%\n",
      "ep 1279, loss: 5.82, 8000 train 93.81%\n",
      "ep 1280, loss: 5.99, 8000 train 93.46%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:07:08.472795\n",
      "ep 1281, loss: 5.60, 8000 train 94.20%\n",
      "ep 1282, loss: 5.66, 8000 train 93.95%\n",
      "ep 1283, loss: 5.54, 8000 train 94.04%\n",
      "ep 1284, loss: 5.66, 8000 train 93.91%\n",
      "ep 1285, loss: 5.62, 8000 train 93.88%\n",
      "ep 1286, loss: 5.85, 8000 train 93.80%\n",
      "ep 1287, loss: 6.18, 8000 train 93.60%\n",
      "ep 1288, loss: 6.03, 8000 train 93.38%\n",
      "ep 1289, loss: 5.72, 8000 train 94.21%\n",
      "ep 1290, loss: 5.63, 8000 train 93.94%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:12:19.610125\n",
      "ep 1291, loss: 5.73, 8000 train 93.59%\n",
      "ep 1292, loss: 5.53, 8000 train 94.06%\n",
      "ep 1293, loss: 5.65, 8000 train 94.31%\n",
      "ep 1294, loss: 5.43, 8000 train 93.97%\n",
      "ep 1295, loss: 5.42, 8000 train 94.31%\n",
      "ep 1296, loss: 6.20, 8000 train 93.79%\n",
      "ep 1297, loss: 5.61, 8000 train 94.16%\n",
      "ep 1298, loss: 5.96, 8000 train 93.56%\n",
      "ep 1299, loss: 5.74, 8000 train 93.92%\n",
      "ep 1300, loss: 5.89, 8000 train 93.56%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:17:30.633263\n",
      "   Model saved to v4_1300_saved.pth\n",
      "ep 1301, loss: 5.94, 8000 train 93.45%\n",
      "ep 1302, loss: 5.54, 8000 train 93.75%\n",
      "ep 1303, loss: 5.50, 8000 train 94.10%\n",
      "ep 1304, loss: 5.25, 8000 train 94.73%\n",
      "ep 1305, loss: 5.60, 8000 train 93.81%\n",
      "ep 1306, loss: 5.48, 8000 train 94.12%\n",
      "ep 1307, loss: 5.90, 8000 train 93.64%\n",
      "ep 1308, loss: 6.15, 8000 train 93.24%\n",
      "ep 1309, loss: 5.63, 8000 train 93.70%\n",
      "ep 1310, loss: 5.24, 8000 train 94.16%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:22:20.178268\n",
      "ep 1311, loss: 5.54, 8000 train 94.14%\n",
      "ep 1312, loss: 6.15, 8000 train 93.62%\n",
      "ep 1313, loss: 5.79, 8000 train 93.58%\n",
      "ep 1314, loss: 5.95, 8000 train 94.06%\n",
      "ep 1315, loss: 6.15, 8000 train 93.67%\n",
      "ep 1316, loss: 5.73, 8000 train 93.89%\n",
      "ep 1317, loss: 5.59, 8000 train 93.91%\n",
      "ep 1318, loss: 5.59, 8000 train 93.97%\n",
      "ep 1319, loss: 5.41, 8000 train 94.06%\n",
      "ep 1320, loss: 5.11, 8000 train 94.60%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:27:26.572244\n",
      "ep 1321, loss: 5.37, 8000 train 94.24%\n",
      "ep 1322, loss: 5.61, 8000 train 94.24%\n",
      "ep 1323, loss: 5.66, 8000 train 93.86%\n",
      "ep 1324, loss: 5.67, 8000 train 93.92%\n",
      "ep 1325, loss: 5.90, 8000 train 93.89%\n",
      "ep 1326, loss: 5.30, 8000 train 94.19%\n",
      "ep 1327, loss: 5.45, 8000 train 93.96%\n",
      "ep 1328, loss: 5.28, 8000 train 94.36%\n",
      "ep 1329, loss: 5.65, 8000 train 93.89%\n",
      "ep 1330, loss: 5.11, 8000 train 94.73%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:32:25.480415\n",
      "ep 1331, loss: 5.51, 8000 train 94.03%\n",
      "ep 1332, loss: 6.12, 8000 train 93.40%\n",
      "ep 1333, loss: 5.74, 8000 train 94.09%\n",
      "ep 1334, loss: 5.65, 8000 train 93.85%\n",
      "ep 1335, loss: 5.30, 8000 train 94.29%\n",
      "ep 1336, loss: 5.48, 8000 train 94.11%\n",
      "ep 1337, loss: 5.65, 8000 train 94.09%\n",
      "ep 1338, loss: 5.08, 8000 train 94.55%\n",
      "ep 1339, loss: 5.42, 8000 train 93.97%\n",
      "ep 1340, loss: 5.62, 8000 train 93.86%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:37:19.749817\n",
      "ep 1341, loss: 5.47, 8000 train 94.01%\n",
      "ep 1342, loss: 5.38, 8000 train 94.21%\n",
      "ep 1343, loss: 5.63, 8000 train 93.85%\n",
      "ep 1344, loss: 5.93, 8000 train 93.74%\n",
      "ep 1345, loss: 5.66, 8000 train 94.15%\n",
      "ep 1346, loss: 5.41, 8000 train 94.10%\n",
      "ep 1347, loss: 5.37, 8000 train 93.96%\n",
      "ep 1348, loss: 5.48, 8000 train 94.29%\n",
      "ep 1349, loss: 5.47, 8000 train 94.05%\n",
      "ep 1350, loss: 5.82, 8000 train 93.85%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:42:10.327314\n",
      "   Model saved to v4_1350_saved.pth\n",
      "ep 1351, loss: 5.95, 8000 train 93.90%\n",
      "ep 1352, loss: 5.55, 8000 train 94.00%\n",
      "ep 1353, loss: 5.61, 8000 train 93.90%\n",
      "ep 1354, loss: 5.59, 8000 train 93.80%\n",
      "ep 1355, loss: 5.27, 8000 train 94.47%\n",
      "ep 1356, loss: 5.34, 8000 train 94.00%\n",
      "ep 1357, loss: 5.64, 8000 train 94.14%\n",
      "ep 1358, loss: 5.71, 8000 train 93.92%\n",
      "ep 1359, loss: 5.51, 8000 train 94.21%\n",
      "ep 1360, loss: 5.67, 8000 train 93.76%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:46:56.399029\n",
      "ep 1361, loss: 5.51, 8000 train 94.05%\n",
      "ep 1362, loss: 5.78, 8000 train 93.56%\n",
      "ep 1363, loss: 5.29, 8000 train 94.33%\n",
      "ep 1364, loss: 4.98, 8000 train 94.50%\n",
      "ep 1365, loss: 5.40, 8000 train 94.23%\n",
      "ep 1366, loss: 5.51, 8000 train 94.27%\n",
      "ep 1367, loss: 5.46, 8000 train 94.21%\n",
      "ep 1368, loss: 5.54, 8000 train 93.83%\n",
      "ep 1369, loss: 5.31, 8000 train 94.06%\n",
      "ep 1370, loss: 5.45, 8000 train 94.11%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:51:45.924591\n",
      "ep 1371, loss: 5.16, 8000 train 94.50%\n",
      "ep 1372, loss: 5.98, 8000 train 93.61%\n",
      "ep 1373, loss: 5.65, 8000 train 94.23%\n",
      "ep 1374, loss: 6.23, 8000 train 93.59%\n",
      "ep 1375, loss: 5.24, 8000 train 94.42%\n",
      "ep 1376, loss: 5.52, 8000 train 94.03%\n",
      "ep 1377, loss: 5.27, 8000 train 94.38%\n",
      "ep 1378, loss: 5.43, 8000 train 94.23%\n",
      "ep 1379, loss: 5.62, 8000 train 94.03%\n",
      "ep 1380, loss: 5.43, 8000 train 94.20%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 10:56:38.817792\n",
      "ep 1381, loss: 5.82, 8000 train 93.94%\n",
      "ep 1382, loss: 5.72, 8000 train 94.19%\n",
      "ep 1383, loss: 5.83, 8000 train 93.66%\n",
      "ep 1384, loss: 5.54, 8000 train 94.09%\n",
      "ep 1385, loss: 5.34, 8000 train 94.41%\n",
      "ep 1386, loss: 5.27, 8000 train 94.45%\n",
      "ep 1387, loss: 6.14, 8000 train 93.54%\n",
      "ep 1388, loss: 5.47, 8000 train 94.10%\n",
      "ep 1389, loss: 5.69, 8000 train 93.95%\n",
      "ep 1390, loss: 5.53, 8000 train 94.16%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:01:25.328720\n",
      "ep 1391, loss: 5.33, 8000 train 94.35%\n",
      "ep 1392, loss: 5.28, 8000 train 94.25%\n",
      "ep 1393, loss: 5.71, 8000 train 93.85%\n",
      "ep 1394, loss: 5.13, 8000 train 94.49%\n",
      "ep 1395, loss: 4.87, 8000 train 94.70%\n",
      "ep 1396, loss: 5.26, 8000 train 94.09%\n",
      "ep 1397, loss: 5.50, 8000 train 94.14%\n",
      "ep 1398, loss: 5.69, 8000 train 93.91%\n",
      "ep 1399, loss: 5.41, 8000 train 94.06%\n",
      "ep 1400, loss: 5.28, 8000 train 94.20%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:06:03.366259\n",
      "   Model saved to v4_1400_saved.pth\n",
      "ep 1401, loss: 5.18, 8000 train 94.35%\n",
      "ep 1402, loss: 5.21, 8000 train 94.64%\n",
      "ep 1403, loss: 5.27, 8000 train 94.11%\n",
      "ep 1404, loss: 5.10, 8000 train 94.61%\n",
      "ep 1405, loss: 5.25, 8000 train 94.58%\n",
      "ep 1406, loss: 5.44, 8000 train 94.30%\n",
      "ep 1407, loss: 5.09, 8000 train 94.41%\n",
      "ep 1408, loss: 5.12, 8000 train 94.40%\n",
      "ep 1409, loss: 5.16, 8000 train 94.34%\n",
      "ep 1410, loss: 5.14, 8000 train 94.55%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:11:01.243877\n",
      "ep 1411, loss: 5.40, 8000 train 94.09%\n",
      "ep 1412, loss: 5.27, 8000 train 94.38%\n",
      "ep 1413, loss: 5.49, 8000 train 94.06%\n",
      "ep 1414, loss: 5.32, 8000 train 94.34%\n",
      "ep 1415, loss: 5.06, 8000 train 94.51%\n",
      "ep 1416, loss: 5.78, 8000 train 93.90%\n",
      "ep 1417, loss: 5.85, 8000 train 93.70%\n",
      "ep 1418, loss: 5.70, 8000 train 93.80%\n",
      "ep 1419, loss: 5.61, 8000 train 94.15%\n",
      "ep 1420, loss: 5.29, 8000 train 94.26%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:15:43.450745\n",
      "ep 1421, loss: 5.07, 8000 train 94.58%\n",
      "ep 1422, loss: 5.01, 8000 train 94.62%\n",
      "ep 1423, loss: 5.52, 8000 train 94.00%\n",
      "ep 1424, loss: 5.95, 8000 train 93.42%\n",
      "ep 1425, loss: 5.65, 8000 train 93.99%\n",
      "ep 1426, loss: 5.17, 8000 train 94.49%\n",
      "ep 1427, loss: 5.19, 8000 train 94.47%\n",
      "ep 1428, loss: 4.95, 8000 train 94.66%\n",
      "ep 1429, loss: 5.63, 8000 train 94.01%\n",
      "ep 1430, loss: 5.18, 8000 train 94.19%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:20:42.359730\n",
      "ep 1431, loss: 5.08, 8000 train 94.36%\n",
      "ep 1432, loss: 5.27, 8000 train 94.40%\n",
      "ep 1433, loss: 5.30, 8000 train 94.25%\n",
      "ep 1434, loss: 5.46, 8000 train 94.35%\n",
      "ep 1435, loss: 5.55, 8000 train 93.89%\n",
      "ep 1436, loss: 5.81, 8000 train 93.83%\n",
      "ep 1437, loss: 5.65, 8000 train 94.06%\n",
      "ep 1438, loss: 5.50, 8000 train 93.94%\n",
      "ep 1439, loss: 4.87, 8000 train 94.62%\n",
      "ep 1440, loss: 5.19, 8000 train 94.47%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:25:25.112272\n",
      "ep 1441, loss: 5.51, 8000 train 94.10%\n",
      "ep 1442, loss: 5.06, 8000 train 94.39%\n",
      "ep 1443, loss: 5.45, 8000 train 94.24%\n",
      "ep 1444, loss: 5.23, 8000 train 94.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1445, loss: 5.59, 8000 train 93.92%\n",
      "ep 1446, loss: 5.93, 8000 train 93.70%\n",
      "ep 1447, loss: 5.19, 8000 train 94.40%\n",
      "ep 1448, loss: 5.60, 8000 train 94.14%\n",
      "ep 1449, loss: 5.17, 8000 train 94.36%\n",
      "ep 1450, loss: 4.86, 8000 train 94.59%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:30:06.485070\n",
      "   Model saved to v4_1450_saved.pth\n",
      "ep 1451, loss: 5.09, 8000 train 94.65%\n",
      "ep 1452, loss: 4.82, 8000 train 94.95%\n",
      "ep 1453, loss: 4.95, 8000 train 95.00%\n",
      "ep 1454, loss: 5.60, 8000 train 94.12%\n",
      "ep 1455, loss: 5.09, 8000 train 94.61%\n",
      "ep 1456, loss: 4.97, 8000 train 94.27%\n",
      "ep 1457, loss: 5.35, 8000 train 94.17%\n",
      "ep 1458, loss: 4.96, 8000 train 94.44%\n",
      "ep 1459, loss: 5.05, 8000 train 94.64%\n",
      "ep 1460, loss: 5.26, 8000 train 94.50%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:34:41.833165\n",
      "ep 1461, loss: 5.44, 8000 train 94.15%\n",
      "ep 1462, loss: 4.77, 8000 train 95.11%\n",
      "ep 1463, loss: 5.18, 8000 train 94.26%\n",
      "ep 1464, loss: 5.53, 8000 train 94.46%\n",
      "ep 1465, loss: 5.00, 8000 train 94.67%\n",
      "ep 1466, loss: 5.11, 8000 train 94.56%\n",
      "ep 1467, loss: 5.84, 8000 train 93.97%\n",
      "ep 1468, loss: 5.52, 8000 train 94.15%\n",
      "ep 1469, loss: 5.79, 8000 train 93.96%\n",
      "ep 1470, loss: 4.93, 8000 train 94.96%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:39:36.569226\n",
      "ep 1471, loss: 5.16, 8000 train 94.40%\n",
      "ep 1472, loss: 5.52, 8000 train 94.11%\n",
      "ep 1473, loss: 5.18, 8000 train 94.29%\n",
      "ep 1474, loss: 4.86, 8000 train 94.73%\n",
      "ep 1475, loss: 4.67, 8000 train 94.95%\n",
      "ep 1476, loss: 5.27, 8000 train 94.46%\n",
      "ep 1477, loss: 5.23, 8000 train 94.61%\n",
      "ep 1478, loss: 5.13, 8000 train 94.69%\n",
      "ep 1479, loss: 5.09, 8000 train 94.34%\n",
      "ep 1480, loss: 5.31, 8000 train 94.40%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:44:14.063865\n",
      "ep 1481, loss: 5.22, 8000 train 94.51%\n",
      "ep 1482, loss: 5.03, 8000 train 94.75%\n",
      "ep 1483, loss: 5.10, 8000 train 94.59%\n",
      "ep 1484, loss: 5.07, 8000 train 94.50%\n",
      "ep 1485, loss: 5.24, 8000 train 94.47%\n",
      "ep 1486, loss: 5.15, 8000 train 94.69%\n",
      "ep 1487, loss: 5.26, 8000 train 94.35%\n",
      "ep 1488, loss: 4.99, 8000 train 94.44%\n",
      "ep 1489, loss: 5.21, 8000 train 94.50%\n",
      "ep 1490, loss: 5.74, 8000 train 94.06%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:49:05.393477\n",
      "ep 1491, loss: 5.06, 8000 train 94.77%\n",
      "ep 1492, loss: 4.95, 8000 train 94.76%\n",
      "ep 1493, loss: 5.04, 8000 train 94.41%\n",
      "ep 1494, loss: 5.02, 8000 train 94.46%\n",
      "ep 1495, loss: 4.94, 8000 train 94.45%\n",
      "ep 1496, loss: 5.08, 8000 train 94.61%\n",
      "ep 1497, loss: 4.96, 8000 train 94.66%\n",
      "ep 1498, loss: 5.12, 8000 train 94.41%\n",
      "ep 1499, loss: 5.28, 8000 train 94.12%\n",
      "ep 1500, loss: 5.24, 8000 train 94.44%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 11:53:41.512867\n",
      "   Model saved to v4_1500_saved.pth\n",
      "   Model saved to savedModel.pth\n",
      "total time needed to train network:         11:53:41.648867\n",
      "total time in seconds: 42821.64886713028\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Tracking training time ###\n",
    "##############################\n",
    "start_time = time.time() ## Added\n",
    "time_elapsed = 0  ## Added Line\n",
    "##############################\n",
    "\n",
    "###############################\n",
    "### Tracking nn performance ###\n",
    "###############################\n",
    "minibatch_loss_list, train_accuracy_list, test_accuracy_list = [], [], [] ## Added\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "print(\"Using device: {}\"\n",
    "      \"\\n\".format(str(device)))\n",
    "########################################################################\n",
    "#######                      Loading Data                        #######\n",
    "########################################################################\n",
    "data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "\n",
    "if train_val_split == 1:\n",
    "    # Train on the entire dataset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset,\n",
    "                        transform=transform('train'))\n",
    "    trainloader = torch.utils.data.DataLoader(data,\n",
    "                        batch_size=batch_size, shuffle=True);\n",
    "else:\n",
    "    # Split the dataset into trainset and testset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "    data.len=len(data)\n",
    "    train_len = int((train_val_split)*data.len)\n",
    "    test_len = data.len - train_len\n",
    "    train_subset, test_subset = random_split(data, [train_len, test_len])\n",
    "    trainset = DatasetFromSubset(\n",
    "        train_subset, transform=transform('train'))\n",
    "    testset = DatasetFromSubset(\n",
    "        test_subset, transform=transform('test'))\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "    testloader = torch.utils.data.DataLoader(testset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Get model, loss criterion and optimizer from student\n",
    "net = net.to(device)\n",
    "criterion = loss_func\n",
    "optimizer = optimizer\n",
    "# get weight initialization and lr scheduler, if appropriate\n",
    "weights_init = weights_init\n",
    "scheduler = scheduler\n",
    "\n",
    "# apply custom weight initialization, if it exists\n",
    "net.apply(weights_init)\n",
    "\n",
    "########################################################################\n",
    "#######                        Training                          #######\n",
    "########################################################################\n",
    "print(\"Start training...\")\n",
    "for epoch in range(1,epochs+1):\n",
    "    total_loss = 0\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in trainloader:           # Load batch\n",
    "        images, labels = batch \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = net(images)             # Process batch\n",
    "\n",
    "        loss = criterion(preds, labels) # Calculate loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                 # Calculate gradients\n",
    "        optimizer.step()                # Update weights\n",
    "\n",
    "        output = preds.argmax(dim=1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_images += labels.size(0)\n",
    "        total_correct += output.eq(labels).sum().item()\n",
    "        minibatch_loss_list.append(loss.item())  ## Added\n",
    "\n",
    "    # apply lr schedule, if it exists\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100 \n",
    "    train_accuracy_list.append(model_accuracy)  ## Added\n",
    "    print('ep {0}, loss: {1:.2f}, {2} train {3:.2f}%'.format(\n",
    "           epoch, total_loss, total_images, model_accuracy), end='')\n",
    "\n",
    "    if train_val_split < 1:\n",
    "        test_network(net,testloader, test_accuracy_list,\n",
    "                     print_confusion=(epoch % 10 == 0)) ## Added\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "   \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.state_dict(),'v4_check.pth')\n",
    "        print(\"   Model saved to checkModel.pth\")\n",
    "        time_elapsed = time.time() - start_time  ## Added Line\n",
    "        print(f'Time elapsed: {str(datetime.timedelta(seconds = time_elapsed))}') ## TIME\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        torch.save(net.state_dict(), f'v4_{model_accuracy}_{epoch}_saved.pth')\n",
    "        print(f\"   Model saved to v4_{epoch}_saved.pth\")\n",
    "    \n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "torch.save(net.state_dict(),'v4_final_saved.pth')\n",
    "print(\"   Model saved to savedModel.pth\")\n",
    "time_elapsed = time.time() - start_time ## Added Line\n",
    "print(f'total time needed to train network: \\\n",
    "        {str(datetime.timedelta(seconds = time_elapsed))}\\ntotal time in seconds: {time_elapsed}') ## TIME\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 256\n",
      "learning rate: 0.0005\n",
      "train_val_split: 1\n",
      "epochs: 1500\n",
      "training data - total instances = 8000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4504/2636902767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtrain_data_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cat_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtest_data_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cat_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'training data distribution - {train_data_distribution}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#**        Data Information     **#\n",
    "###################################\n",
    "print(f'batch size: {batch_size}')\n",
    "print(f'learning rate: {learning_rate}')\n",
    "print(f'train_val_split: {train_val_split}')\n",
    "print(f'epochs: {epochs}')\n",
    "\n",
    "\n",
    "#############################\n",
    "#**         END           **#\n",
    "#############################\n",
    "\n",
    "\n",
    "# Getting count of each cat breed, should be close to 8*0.8*1000 initially..\n",
    "train_data_distribution = get_cat_count(trainloader, 'training data')\n",
    "# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\n",
    "test_data_distribution = get_cat_count(testloader, 'test data')\n",
    "\n",
    "print(f'training data distribution - {train_data_distribution}')\n",
    "print(f'test data distribution - {test_data_distribution}')\n",
    "\n",
    "plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                   num_epochs=epochs,\n",
    "                   iter_per_epoch=len(trainloader),\n",
    "                   results_dir=None,\n",
    "                   averaging_iterations=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_accuracy(train_acc_list=train_accuracy_list,\n",
    "              test_acc_list=test_accuracy_list,\n",
    "              results_dir=None)\n",
    "plt.show()\n",
    "\n",
    "net.cpu()\n",
    "show_examples(model=net, data_loader=testloader, class_dict=cat_dict)\n",
    "\n",
    "conf_matrix = compute_confusion_matrix(model=net, data_loader=testloader, device=torch.device('cpu'))\n",
    "print(conf_matrix)\n",
    "plot_confusion_matrix(conf_matrix, class_names=cat_dict.values(), test_data_distribution=test_data_distribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
