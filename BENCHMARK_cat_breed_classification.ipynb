{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from helper import get_cat_count, count_parameters, compute_confusion_matrix, show_examples, plot_training_loss, plot_accuracy, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9444 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat breed classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**student.py**\n",
    "\n",
    "UNSW COMP9444 Neural Networks and Deep Learning\n",
    "\n",
    "You may modify this file however you wish, including creating additional\n",
    "variables, functions, classes, etc., so long as your code runs with the\n",
    "hw2main.py file unmodified, and you are only using the approved packages.\n",
    "\n",
    "You have been given some default values for the variables train_val_split,\n",
    "batch_size as well as the transform function.\n",
    "You are encouraged to modify these to improve the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question:**\n",
    "\n",
    "Briefly describe how your program works, and explain any design and training\n",
    "decisions you made along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "######     Specify transform(s) to be applied to the input images     ######\n",
    "############################################################################\n",
    "\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    # channel size = 3\n",
    "\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                # transforms.RandomCrop((64, 64)),\n",
    "                transforms.RandomResizedCrop(size=80, \n",
    "                         scale=(0.75, 1.0), ratio=(0.75, 1.3)), # original 80*80, avoid cropping important info\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation((-10,10)),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "                transforms.RandomPosterize(bits=3, p=0.4),\n",
    "                transforms.RandomEqualize(p=0.1),\n",
    "                transforms.RandomGrayscale(p=0.1),\n",
    "                transforms.RandomPerspective(distortion_scale=0.05, p=0.1, fill=0),\n",
    "                ## T.RandomErasing(),\n",
    "                ## T.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
    "                ## T.RandomInvert(p=0.05),\n",
    "                transforms.ToTensor()\n",
    "                ## Standardize each channel of the image\n",
    "                ## T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "    elif mode == 'test':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                # transforms.CenterCrop((64, 64)),\n",
    "                transforms.ToTensor()\n",
    "                ## Standardize each channel of the image\n",
    "                ## transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                ##                                 [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠ Using Last Term's Open Source Deep Learning models for Simpsons classification task (To Benchmark against our model's performance)\n",
    "\n",
    "\n",
    "### https://github.com/hharryyf/COMP9444/blob/main/hw2/student.py\n",
    "\n",
    "#### Do not copt this isn't our work, just wanted to test.\n",
    "\n",
    "https://www.cse.unsw.edu.au/~cs9444/21T2/hw2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Image transformations for Data Augmentation during training.\n",
    "    Only Grayscale and ToTensor are applied during testing to\n",
    "    ensure model classifies the original images.\n",
    "    \"\"\"\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Grayscale(), \n",
    "                transforms.RandomResizedCrop((64, 64), scale=(0.5, 1.0)),\n",
    "                transforms.RandomPerspective(p=0.2),\n",
    "                transforms.RandomAffine(degrees=(-15, 15), translate=(0.0, 0.5)),\n",
    "                transforms.RandomHorizontalFlip(), \n",
    "                transforms.RandomAutocontrast(),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )\n",
    "    elif mode == 'test':\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "############################################################################\n",
    "######   Define the Module to process the images and produce labels   ######\n",
    "############################################################################\n",
    "class hharryyfNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Network class for final model. \n",
    "    Architecture:\n",
    "        1. Convolutional and maxpooling blocks\n",
    "            7 Conv2D layers, 4 MaxPool2D layers with\n",
    "            BatchNorm, ReLU activation\n",
    "        2. Adapted Average Pooling Layer\n",
    "        3. Fully connected layers\n",
    "            3 dense layers with BatchNorm, ReLU activation,\n",
    "            and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), # greyscale operation reduces number of channels to 1\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 192, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(192, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 192, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((5,5))\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(192*5*5, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(256, 9),   #adjusted from 14 to 9..\n",
    "            nn.BatchNorm1d(9)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        x = self.cnn_layers(t)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "net = hharryyfNetwork()\n",
    "optimiser = optim.Adam(net.parameters(), lr=0.0006)\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "batch_size = 200\n",
    "epochs = 100\n",
    "\n",
    "def weights_init(m):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠ Using Last Term's Open Source Deep Learning models for Simpsons classification task (To Benchmark against our model's performance)\n",
    "https://github.com/mrclauderandall/COMP9444-2021T2-Project2/blob/main/student.py\n",
    "Do not copt this isn't our work, just wanted to test.\n",
    "https://www.cse.unsw.edu.au/~cs9444/21T2/hw2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     if isinstance(m, nn.Conv2d):\n",
    "#         nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "\n",
    "        \n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(Block, self).__init__()\n",
    "#         self.activation = nn.ReLU(inplace = True)\n",
    "\n",
    "#         self.c1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "#         self.b1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "#         self.c2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         self.b2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "#         self.d = nn.Dropout(p=0.2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         x = self.activation(self.b1(self.c1(x)))\n",
    "#         x = self.activation(self.b2(self.c2(x)))\n",
    "#         x = self.d(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# ############################################################################\n",
    "# ######   Define the Module to process the images and produce labels   ######\n",
    "# ############################################################################\n",
    "# class mrclauderandallNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Network, self).__init__()\n",
    "\n",
    "#         block = Block\n",
    "\n",
    "#         # define activation and pooling functions\n",
    "#         self.activation = nn.ReLU(inplace = True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "#         # initial convolution\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "#         # block layers\n",
    "#         self.layer1 = nn.Sequential(block(64, 64, 1), block(64, 128, 1))\n",
    "#         self.layer2 = nn.Sequential(block(128, 256, 2), block(256, 512, 2))\n",
    "\n",
    "#         # average pooling and linear output layer\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "#         self.h1 = nn.Linear(512, 14)\n",
    "\n",
    "#         # weight initialization\n",
    "#         for module in self.modules():\n",
    "#             init_weights(module)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.maxpool(x)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.h1(x)\n",
    "#         return x\n",
    "\n",
    "# net = mrclauderandallNetwork()\n",
    "# lossFunc = nn.CrossEntropyLoss()\n",
    "# optimiser = optim.Adam(net.parameters(), lr=0.001)\n",
    "# batch_size = 100\n",
    "# epochs = 100\n",
    "\n",
    "# def weights_init(m):\n",
    "#     if isinstance(m, nn.Conv2d):\n",
    "#         nn.init.kaiming_normal_(m.weight, mode='fan_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Grayscale(num_output_channels=1)\n",
      "    RandomResizedCrop(size=(64, 64), scale=(0.5, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
      "    RandomPerspective(p=0.2)\n",
      "    RandomAffine(degrees=[-15.0, 15.0], translate=(0.0, 0.5))\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomAutocontrast(p=0.5)\n",
      "    RandomAdjustSharpness(sharpness_factor=2,p=0.5)\n",
      "    ToTensor()\n",
      ")\n",
      "hharryyfNetwork(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(5, 5))\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=4800, out_features=2048, bias=True)\n",
      "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Linear(in_features=256, out_features=9, bias=True)\n",
      "    (9): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "+------------------------+------------+\n",
      "|        Modules         | Parameters |\n",
      "+------------------------+------------+\n",
      "|  cnn_layers.0.weight   |    576     |\n",
      "|   cnn_layers.0.bias    |     64     |\n",
      "|  cnn_layers.1.weight   |     64     |\n",
      "|   cnn_layers.1.bias    |     64     |\n",
      "|  cnn_layers.4.weight   |   73728    |\n",
      "|   cnn_layers.4.bias    |    128     |\n",
      "|  cnn_layers.5.weight   |    128     |\n",
      "|   cnn_layers.5.bias    |    128     |\n",
      "|  cnn_layers.7.weight   |   147456   |\n",
      "|   cnn_layers.7.bias    |    128     |\n",
      "|  cnn_layers.8.weight   |    128     |\n",
      "|   cnn_layers.8.bias    |    128     |\n",
      "|  cnn_layers.11.weight  |   221184   |\n",
      "|   cnn_layers.11.bias   |    192     |\n",
      "|  cnn_layers.12.weight  |    192     |\n",
      "|   cnn_layers.12.bias   |    192     |\n",
      "|  cnn_layers.14.weight  |   442368   |\n",
      "|   cnn_layers.14.bias   |    256     |\n",
      "|  cnn_layers.15.weight  |    256     |\n",
      "|   cnn_layers.15.bias   |    256     |\n",
      "|  cnn_layers.18.weight  |   589824   |\n",
      "|   cnn_layers.18.bias   |    256     |\n",
      "|  cnn_layers.19.weight  |    256     |\n",
      "|   cnn_layers.19.bias   |    256     |\n",
      "|  cnn_layers.21.weight  |   442368   |\n",
      "|   cnn_layers.21.bias   |    192     |\n",
      "|  cnn_layers.22.weight  |    192     |\n",
      "|   cnn_layers.22.bias   |    192     |\n",
      "| linear_layers.1.weight |  9830400   |\n",
      "|  linear_layers.1.bias  |    2048    |\n",
      "| linear_layers.2.weight |    2048    |\n",
      "|  linear_layers.2.bias  |    2048    |\n",
      "| linear_layers.5.weight |   524288   |\n",
      "|  linear_layers.5.bias  |    256     |\n",
      "| linear_layers.6.weight |    256     |\n",
      "|  linear_layers.6.bias  |    256     |\n",
      "| linear_layers.8.weight |    2304    |\n",
      "|  linear_layers.8.bias  |     9      |\n",
      "| linear_layers.9.weight |     9      |\n",
      "|  linear_layers.9.bias  |     9      |\n",
      "+------------------------+------------+\n",
      "Total Trainable Params: 12285083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12285083"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################################\n",
    "######      Specify the optimizer and loss function                   ######\n",
    "############################################################################\n",
    "learning_rate = 0.001\n",
    "# optimizer = torch.optim.SGD(net.parameters(), momentum=0.9, lr=learning_rate)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# loss_func = F.nll_loss\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "############################################################################\n",
    "######  Custom weight initialization and lr scheduling are optional   ######\n",
    "############################################################################\n",
    "\n",
    "# Normally, the default weight initialization and fixed learing rate\n",
    "# should work fine. But, we have made it possible for you to define\n",
    "# your own custom weight initialization and lr scheduler, if you wish.\n",
    "# def weights_init(m):\n",
    "#     return\n",
    "\n",
    "scheduler = None\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#######              Metaparameters and training options              ######\n",
    "############################################################################\n",
    "dataset = \"./data\"\n",
    "train_val_split = 0.8\n",
    "\n",
    "\n",
    "###############################################\n",
    "#**          Print Network Information      **#\n",
    "###############################################\n",
    "print(transform('train'))\n",
    "print(net)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###########################\n",
    "## Cat breed dictionary  ##\n",
    "###########################\n",
    "cat_dict = {\n",
    "    0: 'bombay',\n",
    "    1: 'calico',\n",
    "    2: 'persian',\n",
    "    3: 'russianblue',\n",
    "    4: 'siamese',\n",
    "    5: 'tiger',\n",
    "    6: 'tortoiseshell',\n",
    "    7: 'tuxedo'\n",
    "}\n",
    "\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "# Test network on validation set, if it exists.\n",
    "## Added params\n",
    "def test_network(net,testloader,test_accuracy_list,print_confusion=False):\n",
    "    net.eval()\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "    conf_matrix = np.zeros((8,8))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            conf_matrix = conf_matrix + metrics.confusion_matrix(\n",
    "                labels.cpu(),predicted.cpu(),labels=[0,1,2,3,4,5,6,7])\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    test_accuracy_list.append(model_accuracy)\n",
    "    print(', {0} test {1:.2f}%'.format(total_images,model_accuracy))\n",
    "    if print_confusion:\n",
    "        np.set_printoptions(precision=2, suppress=True)\n",
    "        print(conf_matrix)\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrar\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:594: UserWarning: torch.lstsq is deprecated in favor of torch.linalg.lstsq and will be removed in a future PyTorch release.\n",
      "torch.linalg.lstsq has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n",
      "To get the qr decomposition consider using torch.linalg.qr.\n",
      "The returned solution in torch.lstsq stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals in the field 'residuals' of the returned named tuple.\n",
      "The unpacking of the solution, as in\n",
      "X, _ = torch.lstsq(B, A).solution[:A.size(1)]\n",
      "should be replaced with\n",
      "X = torch.linalg.lstsq(A, B).solution (Triggered internally at  ..\\aten\\src\\ATen\\LegacyTHFunctionsCPU.cpp:389.)\n",
      "  res = torch.lstsq(b_matrix, a_matrix)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1, loss: 64.56, 6400 train 21.11%, 1600 test 23.88%\n",
      "Time elapsed: 0:01:33.875743\n",
      "ep 2, loss: 58.85, 6400 train 29.12%, 1600 test 36.06%\n",
      "Time elapsed: 0:04:30.206522\n",
      "ep 3, loss: 55.15, 6400 train 33.94%, 1600 test 31.69%\n",
      "Time elapsed: 0:05:29.164329\n",
      "ep 4, loss: 52.36, 6400 train 37.23%, 1600 test 34.00%\n",
      "Time elapsed: 0:06:29.151180\n",
      "ep 5, loss: 51.27, 6400 train 39.84%, 1600 test 40.12%\n",
      "Time elapsed: 0:07:29.509540\n",
      "ep 6, loss: 49.58, 6400 train 40.77%, 1600 test 34.44%\n",
      "Time elapsed: 0:08:30.567040\n",
      "ep 7, loss: 48.26, 6400 train 43.83%, 1600 test 49.94%\n",
      "Time elapsed: 0:09:30.298446\n",
      "ep 8, loss: 46.38, 6400 train 46.56%, 1600 test 39.81%\n",
      "Time elapsed: 0:10:29.981963\n",
      "ep 9, loss: 45.70, 6400 train 47.70%, 1600 test 28.56%\n",
      "Time elapsed: 0:11:29.592006\n",
      "ep 10, loss: 44.57, 6400 train 48.45%, 1600 test 42.00%\n",
      "[[ 93.   1.   0.   0.   0.   3.  59.  33.]\n",
      " [  0.  99.   1.   1.   4.  22.  56.  18.]\n",
      " [  6.  63.  29.   5.  16.  31.  47.   2.]\n",
      " [  5.   4.   2.  21.   2.  57. 119.  12.]\n",
      " [  0.  91.  24.   1.  18.  10.  49.  10.]\n",
      " [  0.   6.   2.   1.   1. 119.  61.   3.]\n",
      " [  5.   5.   0.   1.   0.   7. 171.  14.]\n",
      " [  8.  31.   0.   0.   0.   1.  28. 122.]]\n",
      "Time elapsed: 0:12:30.104522\n",
      "   Model saved to checkModel.pth\n",
      "ep 11, loss: 44.09, 6400 train 49.22%, 1600 test 34.56%\n",
      "Time elapsed: 0:13:30.237906\n",
      "ep 12, loss: 42.32, 6400 train 51.69%, 1600 test 37.44%\n",
      "Time elapsed: 0:14:31.604199\n",
      "ep 13, loss: 41.37, 6400 train 53.09%, 1600 test 38.38%\n",
      "Time elapsed: 0:15:34.622205\n",
      "ep 14, loss: 40.16, 6400 train 54.45%, 1600 test 54.12%\n",
      "Time elapsed: 0:16:38.417942\n",
      "ep 15, loss: 39.38, 6400 train 55.95%, 1600 test 59.31%\n",
      "Time elapsed: 0:17:41.965343\n",
      "ep 16, loss: 39.00, 6400 train 56.47%, 1600 test 57.94%\n",
      "Time elapsed: 0:18:45.111633\n",
      "ep 17, loss: 37.60, 6400 train 57.44%, 1600 test 48.62%\n",
      "Time elapsed: 0:19:49.101671\n",
      "ep 18, loss: 37.30, 6400 train 58.44%, 1600 test 51.62%\n",
      "Time elapsed: 0:20:50.057776\n",
      "ep 19, loss: 36.22, 6400 train 59.34%, 1600 test 56.75%\n",
      "Time elapsed: 0:21:51.213987\n",
      "ep 20, loss: 35.64, 6400 train 60.20%, 1600 test 57.44%\n",
      "[[121.   0.   0.  33.   0.   3.   8.  24.]\n",
      " [  0.  97.   0.   4.   3.  58.   9.  30.]\n",
      " [  5.  15.  56.  41.  31.  40.   6.   5.]\n",
      " [  5.   2.   2. 159.   0.  41.   8.   5.]\n",
      " [  1.  40.  21.  36.  67.  20.   5.  13.]\n",
      " [  0.   2.   3.  16.   0. 170.   2.   0.]\n",
      " [ 10.   8.   0.  19.   1.  47. 107.  11.]\n",
      " [  8.   6.   0.  14.   1.  17.   2. 142.]]\n",
      "Time elapsed: 0:22:52.378939\n",
      "   Model saved to checkModel.pth\n",
      "ep 21, loss: 34.64, 6400 train 61.41%, 1600 test 61.06%\n",
      "Time elapsed: 0:23:53.950349\n",
      "ep 22, loss: 34.25, 6400 train 62.53%, 1600 test 44.75%\n",
      "Time elapsed: 0:24:57.762043\n",
      "ep 23, loss: 33.46, 6400 train 62.89%, 1600 test 62.88%\n",
      "Time elapsed: 0:26:01.630509\n",
      "ep 24, loss: 32.32, 6400 train 64.50%, 1600 test 62.25%\n",
      "Time elapsed: 0:27:05.648884\n",
      "ep 25, loss: 31.75, 6400 train 64.59%, 1600 test 59.56%\n",
      "Time elapsed: 0:28:08.125010\n",
      "ep 26, loss: 31.76, 6400 train 64.70%, 1600 test 47.00%\n",
      "Time elapsed: 0:29:09.334310\n",
      "ep 27, loss: 31.40, 6400 train 65.56%, 1600 test 55.00%\n",
      "Time elapsed: 0:30:10.995517\n",
      "ep 28, loss: 30.62, 6400 train 66.58%, 1600 test 67.06%\n",
      "Time elapsed: 0:31:13.379154\n",
      "ep 29, loss: 29.26, 6400 train 68.14%, 1600 test 56.06%\n",
      "Time elapsed: 0:32:13.702850\n",
      "ep 30, loss: 28.97, 6400 train 67.86%, 1600 test 65.62%\n",
      "[[137.   0.   1.  25.   3.   0.   5.  18.]\n",
      " [  0. 149.  13.   0.   0.  16.   8.  15.]\n",
      " [  6.  13. 144.   6.   4.  15.   6.   5.]\n",
      " [  1.   4.  18. 169.   3.  16.   4.   7.]\n",
      " [  2.  50.  82.   9.  48.   4.   4.   4.]\n",
      " [  1.   7.  18.  11.   3. 145.   5.   3.]\n",
      " [ 17.  20.   7.  20.   2.  13. 109.  15.]\n",
      " [  6.  25.   3.   4.   1.   2.   0. 149.]]\n",
      "Time elapsed: 0:32:57.945005\n",
      "   Model saved to checkModel.pth\n",
      "ep 31, loss: 28.61, 6400 train 68.48%, 1600 test 65.31%\n",
      "Time elapsed: 0:33:36.172716\n",
      "ep 32, loss: 28.11, 6400 train 68.97%, 1600 test 61.19%\n",
      "Time elapsed: 0:34:14.088419\n",
      "ep 33, loss: 27.75, 6400 train 69.70%, 1600 test 66.94%\n",
      "Time elapsed: 0:34:51.978363\n",
      "ep 34, loss: 26.80, 6400 train 70.50%, 1600 test 61.94%\n",
      "Time elapsed: 0:35:30.286268\n",
      "ep 35, loss: 26.69, 6400 train 71.23%, 1600 test 63.06%\n",
      "Time elapsed: 0:36:08.406276\n",
      "ep 36, loss: 26.15, 6400 train 71.41%, 1600 test 64.56%\n",
      "Time elapsed: 0:36:46.285105\n",
      "ep 37, loss: 25.83, 6400 train 71.91%, 1600 test 64.00%\n",
      "Time elapsed: 0:37:24.158916\n",
      "ep 38, loss: 25.58, 6400 train 72.72%, 1600 test 69.94%\n",
      "Time elapsed: 0:38:02.099865\n",
      "ep 39, loss: 25.43, 6400 train 72.77%, 1600 test 59.50%\n",
      "Time elapsed: 0:38:39.957840\n",
      "ep 40, loss: 24.35, 6400 train 73.78%, 1600 test 66.19%\n",
      "[[ 99.   3.   2.  16.  12.   0.   6.  51.]\n",
      " [  0. 129.   4.   0.  12.  11.   2.  43.]\n",
      " [  3.  17. 123.   0.  32.   8.   7.   9.]\n",
      " [  2.  12.  15. 145.  25.   9.   1.  13.]\n",
      " [  0.  30.  19.   2. 140.   2.   2.   8.]\n",
      " [  0.   9.  13.   2.   5. 150.   2.  12.]\n",
      " [  3.  42.   6.   7.  10.   9.  94.  32.]\n",
      " [  1.   4.   1.   2.   3.   0.   0. 179.]]\n",
      "Time elapsed: 0:39:17.851945\n",
      "   Model saved to checkModel.pth\n",
      "ep 41, loss: 23.73, 6400 train 73.98%, 1600 test 64.81%\n",
      "Time elapsed: 0:39:55.953000\n",
      "ep 42, loss: 23.85, 6400 train 74.05%, 1600 test 65.00%\n",
      "Time elapsed: 0:40:33.843438\n",
      "ep 43, loss: 23.27, 6400 train 74.38%, 1600 test 71.06%\n",
      "Time elapsed: 0:41:11.868761\n",
      "ep 44, loss: 22.89, 6400 train 75.34%, 1600 test 75.06%\n",
      "Time elapsed: 0:41:49.835960\n",
      "ep 45, loss: 23.11, 6400 train 74.97%, 1600 test 67.75%\n",
      "Time elapsed: 0:42:27.716260\n",
      "ep 46, loss: 21.56, 6400 train 76.36%, 1600 test 70.62%\n",
      "Time elapsed: 0:43:05.629008\n",
      "ep 47, loss: 22.37, 6400 train 75.62%, 1600 test 68.88%\n",
      "Time elapsed: 0:43:43.517567\n",
      "ep 48, loss: 21.11, 6400 train 77.06%, 1600 test 54.87%\n",
      "Time elapsed: 0:44:21.389200\n",
      "ep 49, loss: 20.89, 6400 train 77.33%, 1600 test 74.12%\n",
      "Time elapsed: 0:44:59.318007\n",
      "ep 50, loss: 21.02, 6400 train 77.03%, 1600 test 67.88%\n",
      "[[134.   0.   0.   7.   1.   0.   8.  39.]\n",
      " [  0.  66.   6.   2.  15.  17.  17.  78.]\n",
      " [  4.   3. 122.  14.  13.  10.  19.  14.]\n",
      " [  4.   1.   3. 164.   1.  12.  13.  24.]\n",
      " [  1.   6.  20.  19. 132.   6.  11.   8.]\n",
      " [  1.   0.   5.   8.   2. 144.  11.  22.]\n",
      " [  5.   4.   0.   6.   0.   6. 140.  42.]\n",
      " [  1.   1.   1.   1.   2.   0.   0. 184.]]\n",
      "Time elapsed: 0:45:37.210254\n",
      "   Model saved to checkModel.pth\n",
      "ep 51, loss: 20.95, 6400 train 77.72%, 1600 test 68.88%\n",
      "Time elapsed: 0:46:15.227953\n",
      "ep 52, loss: 20.38, 6400 train 78.11%, 1600 test 72.56%\n",
      "Time elapsed: 0:46:53.082951\n",
      "ep 53, loss: 20.15, 6400 train 77.78%, 1600 test 72.94%\n",
      "Time elapsed: 0:47:31.058702\n",
      "ep 54, loss: 19.27, 6400 train 79.14%, 1600 test 71.75%\n",
      "Time elapsed: 0:48:08.971834\n",
      "ep 55, loss: 19.63, 6400 train 78.62%, 1600 test 67.06%\n",
      "Time elapsed: 0:48:46.798148\n",
      "ep 56, loss: 19.02, 6400 train 79.36%, 1600 test 72.19%\n",
      "Time elapsed: 0:49:24.653946\n",
      "ep 57, loss: 18.99, 6400 train 79.61%, 1600 test 71.38%\n",
      "Time elapsed: 0:50:02.538994\n",
      "ep 58, loss: 18.86, 6400 train 79.91%, 1600 test 70.81%\n",
      "Time elapsed: 0:50:40.387748\n",
      "ep 59, loss: 18.90, 6400 train 79.58%, 1600 test 73.44%\n",
      "Time elapsed: 0:51:18.276426\n",
      "ep 60, loss: 18.11, 6400 train 79.75%, 1600 test 76.75%\n",
      "[[164.   0.   2.   5.   1.   0.   8.   9.]\n",
      " [  1. 113.  11.   0.  13.  16.  15.  32.]\n",
      " [  4.   6. 140.   2.  28.   8.  10.   1.]\n",
      " [ 19.   2.   7. 172.   7.   8.   4.   3.]\n",
      " [  5.   6.  18.   4. 160.   2.   5.   3.]\n",
      " [  1.   5.   9.   6.   4. 162.   4.   2.]\n",
      " [  9.   8.   2.   6.   2.   8. 156.  12.]\n",
      " [ 15.   4.   5.   1.   2.   1.   1. 161.]]\n",
      "Time elapsed: 0:51:56.736679\n",
      "   Model saved to checkModel.pth\n",
      "ep 61, loss: 16.88, 6400 train 81.92%, 1600 test 71.31%\n",
      "Time elapsed: 0:52:34.836552\n",
      "ep 62, loss: 17.40, 6400 train 81.20%, 1600 test 75.12%\n",
      "Time elapsed: 0:53:12.815856\n",
      "ep 63, loss: 17.11, 6400 train 81.34%, 1600 test 69.00%\n",
      "Time elapsed: 0:53:50.761245\n",
      "ep 64, loss: 17.05, 6400 train 81.48%, 1600 test 71.62%\n",
      "Time elapsed: 0:54:28.793338\n",
      "ep 65, loss: 17.57, 6400 train 80.81%, 1600 test 71.94%\n",
      "Time elapsed: 0:55:06.798928\n",
      "ep 66, loss: 16.28, 6400 train 82.52%, 1600 test 66.81%\n",
      "Time elapsed: 0:55:44.934989\n",
      "ep 67, loss: 16.37, 6400 train 82.72%, 1600 test 78.31%\n",
      "Time elapsed: 0:56:23.015886\n",
      "ep 68, loss: 16.24, 6400 train 82.44%, 1600 test 69.38%\n",
      "Time elapsed: 0:57:01.040074\n",
      "ep 69, loss: 15.83, 6400 train 83.06%, 1600 test 72.81%\n",
      "Time elapsed: 0:57:58.004287\n",
      "ep 70, loss: 15.57, 6400 train 83.08%, 1600 test 75.06%\n",
      "[[171.   0.   0.   3.   3.   0.   7.   5.]\n",
      " [  3. 110.   5.   1.  29.  12.  16.  25.]\n",
      " [  7.   5. 121.   1.  51.   5.   6.   3.]\n",
      " [ 22.   1.   5. 165.  20.   2.   2.   5.]\n",
      " [  5.   4.   6.   0. 179.   1.   6.   2.]\n",
      " [  3.   4.  10.   2.  14. 151.   7.   2.]\n",
      " [ 27.   9.   7.   8.   2.   3. 136.  11.]\n",
      " [ 15.   2.   1.   1.   1.   0.   2. 168.]]\n",
      "Time elapsed: 0:59:00.478586\n",
      "   Model saved to checkModel.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 71, loss: 15.53, 6400 train 83.28%, 1600 test 74.38%\n",
      "Time elapsed: 0:59:59.593259\n",
      "ep 72, loss: 15.26, 6400 train 83.34%, 1600 test 70.88%\n",
      "Time elapsed: 1:00:57.937365\n",
      "ep 73, loss: 14.79, 6400 train 84.48%, 1600 test 72.00%\n",
      "Time elapsed: 1:01:57.298565\n",
      "ep 74, loss: 14.98, 6400 train 83.69%, 1600 test 72.50%\n",
      "Time elapsed: 1:02:55.651966\n",
      "ep 75, loss: 14.36, 6400 train 84.39%, 1600 test 73.75%\n",
      "Time elapsed: 1:03:53.808530\n",
      "ep 76, loss: 14.93, 6400 train 84.36%, 1600 test 71.81%\n",
      "Time elapsed: 1:04:51.985709\n",
      "ep 77, loss: 14.88, 6400 train 83.67%, 1600 test 68.88%\n",
      "Time elapsed: 1:05:51.326451\n",
      "ep 78, loss: 14.20, 6400 train 84.66%, 1600 test 75.94%\n",
      "Time elapsed: 1:06:51.723768\n",
      "ep 79, loss: 13.23, 6400 train 85.47%, 1600 test 76.69%\n",
      "Time elapsed: 1:07:51.272219\n",
      "ep 80, loss: 12.89, 6400 train 86.08%, 1600 test 74.69%\n",
      "[[172.   1.   0.   4.   1.   0.   5.   6.]\n",
      " [  0. 106.   3.   3.  15.  39.  14.  21.]\n",
      " [  6.   6. 134.   2.  18.  24.   8.   1.]\n",
      " [ 22.   1.   6. 156.   5.  20.   2.  10.]\n",
      " [  3.   3.  11.   9. 152.  19.   4.   2.]\n",
      " [  1.   1.   1.   4.   2. 182.   2.   0.]\n",
      " [ 19.  10.   1.   6.   2.  16. 133.  16.]\n",
      " [  9.   8.   3.   2.   5.   2.   1. 160.]]\n",
      "Time elapsed: 1:08:49.772948\n",
      "   Model saved to checkModel.pth\n",
      "ep 81, loss: 13.28, 6400 train 85.91%, 1600 test 72.69%\n",
      "Time elapsed: 1:09:48.320880\n",
      "ep 82, loss: 13.41, 6400 train 85.78%, 1600 test 74.62%\n",
      "Time elapsed: 1:10:47.127851\n",
      "ep 83, loss: 13.38, 6400 train 85.84%, 1600 test 77.94%\n",
      "Time elapsed: 1:11:33.131717\n",
      "ep 84, loss: 13.56, 6400 train 85.53%, 1600 test 78.50%\n",
      "Time elapsed: 1:12:11.363130\n",
      "ep 85, loss: 12.94, 6400 train 85.91%, 1600 test 73.19%\n",
      "Time elapsed: 1:12:49.643696\n",
      "ep 86, loss: 12.44, 6400 train 86.22%, 1600 test 72.38%\n",
      "Time elapsed: 1:13:27.704457\n",
      "ep 87, loss: 12.60, 6400 train 86.62%, 1600 test 69.81%\n",
      "Time elapsed: 1:14:05.660867\n",
      "ep 88, loss: 12.17, 6400 train 87.22%, 1600 test 77.50%\n",
      "Time elapsed: 1:14:43.784764\n",
      "ep 89, loss: 12.81, 6400 train 86.45%, 1600 test 72.12%\n",
      "Time elapsed: 1:15:21.759722\n",
      "ep 90, loss: 12.53, 6400 train 86.61%, 1600 test 78.62%\n",
      "[[152.   0.   1.   8.   1.   0.  20.   7.]\n",
      " [  0. 116.   6.   1.  21.  10.  16.  31.]\n",
      " [  3.   6. 134.   3.  25.   9.  15.   4.]\n",
      " [  6.   2.   1. 192.   6.   4.   7.   4.]\n",
      " [  3.   5.   9.   3. 170.   6.   4.   3.]\n",
      " [  0.   6.   5.   6.   3. 160.   8.   5.]\n",
      " [  6.   9.   2.  10.   2.   1. 161.  12.]\n",
      " [  5.   1.   2.   3.   2.   0.   4. 173.]]\n",
      "Time elapsed: 1:15:59.794645\n",
      "   Model saved to checkModel.pth\n",
      "ep 91, loss: 11.45, 6400 train 87.62%, 1600 test 75.00%\n",
      "Time elapsed: 1:16:38.020000\n",
      "ep 92, loss: 12.08, 6400 train 86.89%, 1600 test 76.25%\n",
      "Time elapsed: 1:17:16.050586\n",
      "ep 93, loss: 11.43, 6400 train 87.38%, 1600 test 72.19%\n",
      "Time elapsed: 1:17:54.125233\n",
      "ep 94, loss: 11.24, 6400 train 88.05%, 1600 test 75.69%\n",
      "Time elapsed: 1:18:32.154515\n",
      "ep 95, loss: 11.55, 6400 train 87.09%, 1600 test 77.50%\n",
      "Time elapsed: 1:19:10.183749\n",
      "ep 96, loss: 11.44, 6400 train 88.11%, 1600 test 73.12%\n",
      "Time elapsed: 1:19:48.272150\n",
      "ep 97, loss: 10.67, 6400 train 88.27%, 1600 test 70.69%\n",
      "Time elapsed: 1:20:26.389621\n",
      "ep 98, loss: 10.56, 6400 train 88.28%, 1600 test 73.75%\n",
      "Time elapsed: 1:21:04.444522\n",
      "ep 99, loss: 10.75, 6400 train 88.27%, 1600 test 73.44%\n",
      "Time elapsed: 1:21:42.517413\n",
      "ep 100, loss: 10.82, 6400 train 88.16%, 1600 test 77.00%\n",
      "[[123.   0.   1.  14.   1.   0.  23.  27.]\n",
      " [  0. 147.   1.   5.   4.  15.  10.  19.]\n",
      " [  2.  12. 118.   7.  17.  29.  11.   3.]\n",
      " [  2.   3.   4. 190.   1.   9.   9.   4.]\n",
      " [  1.  15.   5.   9. 150.  13.   6.   4.]\n",
      " [  0.   4.   2.   3.   0. 178.   4.   2.]\n",
      " [  2.  17.   1.   5.   0.  13. 157.   8.]\n",
      " [  2.   8.   1.   4.   2.   3.   1. 169.]]\n",
      "Time elapsed: 1:22:20.583384\n",
      "   Model saved to checkModel.pth\n",
      "ep 101, loss: 10.10, 6400 train 89.41%, 1600 test 72.12%\n",
      "Time elapsed: 1:22:58.738941\n",
      "ep 102, loss: 10.98, 6400 train 88.44%, 1600 test 67.12%\n",
      "Time elapsed: 1:23:36.754594\n",
      "ep 103, loss: 10.09, 6400 train 89.30%, 1600 test 78.06%\n",
      "Time elapsed: 1:24:14.864705\n",
      "ep 104, loss: 10.24, 6400 train 89.00%, 1600 test 75.94%\n",
      "Time elapsed: 1:24:52.925248\n",
      "ep 105, loss: 10.79, 6400 train 88.42%, 1600 test 75.88%\n",
      "Time elapsed: 1:25:30.993424\n",
      "ep 106, loss: 10.15, 6400 train 89.23%, 1600 test 76.00%\n",
      "Time elapsed: 1:26:09.138361\n",
      "ep 107, loss: 9.52, 6400 train 89.33%, 1600 test 75.88%\n",
      "Time elapsed: 1:26:47.247140\n",
      "ep 108, loss: 9.63, 6400 train 90.23%, 1600 test 78.88%\n",
      "Time elapsed: 1:27:25.378770\n",
      "ep 109, loss: 10.11, 6400 train 88.94%, 1600 test 74.06%\n",
      "Time elapsed: 1:28:03.503327\n",
      "ep 110, loss: 9.67, 6400 train 89.64%, 1600 test 77.12%\n",
      "[[138.   0.   3.  16.   2.   1.   7.  22.]\n",
      " [  0. 134.   2.   2.  14.   9.  13.  27.]\n",
      " [  2.  14. 124.  12.  27.  12.   6.   2.]\n",
      " [  0.   1.   1. 207.   2.   1.   3.   7.]\n",
      " [  0.  14.   5.  15. 156.   9.   1.   3.]\n",
      " [  0.  10.   2.  12.   2. 163.   0.   4.]\n",
      " [  5.  18.   1.  19.   2.   4. 139.  15.]\n",
      " [  4.   5.   0.   3.   3.   2.   0. 173.]]\n",
      "Time elapsed: 1:28:41.554685\n",
      "   Model saved to checkModel.pth\n",
      "ep 111, loss: 9.49, 6400 train 89.89%, 1600 test 77.19%\n",
      "Time elapsed: 1:29:19.719651\n",
      "ep 112, loss: 8.59, 6400 train 90.67%, 1600 test 77.81%\n",
      "Time elapsed: 1:29:57.768535\n",
      "ep 113, loss: 9.72, 6400 train 89.81%, 1600 test 75.44%\n",
      "Time elapsed: 1:30:35.812441\n",
      "ep 114, loss: 8.99, 6400 train 90.17%, 1600 test 77.50%\n",
      "Time elapsed: 1:31:13.920584\n",
      "ep 115, loss: 8.50, 6400 train 90.86%, 1600 test 76.12%\n",
      "Time elapsed: 1:31:52.013250\n",
      "ep 116, loss: 9.52, 6400 train 89.48%, 1600 test 76.44%\n",
      "Time elapsed: 1:32:30.106928\n",
      "ep 117, loss: 8.64, 6400 train 90.41%, 1600 test 76.12%\n",
      "Time elapsed: 1:33:08.160001\n",
      "ep 118, loss: 8.20, 6400 train 91.48%, 1600 test 76.44%\n",
      "Time elapsed: 1:33:46.239301\n",
      "ep 119, loss: 9.22, 6400 train 90.33%, 1600 test 77.94%\n",
      "Time elapsed: 1:34:24.315073\n",
      "ep 120, loss: 8.55, 6400 train 90.61%, 1600 test 76.25%\n",
      "[[175.   0.   3.   9.   1.   0.   0.   1.]\n",
      " [  4. 126.   6.   5.  27.  15.   9.   9.]\n",
      " [  6.   4. 130.   7.  39.   8.   4.   1.]\n",
      " [ 11.   1.   5. 193.   9.   1.   0.   2.]\n",
      " [  3.   1.   8.   5. 180.   3.   2.   1.]\n",
      " [  1.   2.   3.  11.   5. 168.   1.   2.]\n",
      " [ 36.  15.   5.  18.   6.   7. 110.   6.]\n",
      " [ 26.  11.   2.   4.   8.   0.   1. 138.]]\n",
      "Time elapsed: 1:35:02.338383\n",
      "   Model saved to checkModel.pth\n",
      "ep 121, loss: 8.57, 6400 train 91.05%, 1600 test 75.19%\n",
      "Time elapsed: 1:35:40.462143\n",
      "ep 122, loss: 7.79, 6400 train 91.42%, 1600 test 77.38%\n",
      "Time elapsed: 1:36:18.541698\n",
      "ep 123, loss: 8.31, 6400 train 91.44%, 1600 test 78.56%\n",
      "Time elapsed: 1:36:56.540061\n",
      "ep 124, loss: 7.83, 6400 train 91.67%, 1600 test 78.00%\n",
      "Time elapsed: 1:37:34.585116\n",
      "ep 125, loss: 8.24, 6400 train 91.61%, 1600 test 74.62%\n",
      "Time elapsed: 1:38:12.609185\n",
      "ep 126, loss: 7.73, 6400 train 91.50%, 1600 test 75.38%\n",
      "Time elapsed: 1:38:50.652329\n",
      "ep 127, loss: 7.51, 6400 train 91.58%, 1600 test 74.56%\n",
      "Time elapsed: 1:39:28.674782\n",
      "ep 128, loss: 7.53, 6400 train 91.84%, 1600 test 78.88%\n",
      "Time elapsed: 1:40:06.706141\n",
      "ep 129, loss: 7.52, 6400 train 92.42%, 1600 test 77.06%\n",
      "Time elapsed: 1:40:44.781914\n",
      "ep 130, loss: 7.36, 6400 train 92.20%, 1600 test 76.62%\n",
      "[[171.   1.   2.   2.   1.   2.   5.   5.]\n",
      " [  2. 116.   2.   2.  17.  16.  24.  22.]\n",
      " [  6.   9. 122.   2.  25.  18.  12.   5.]\n",
      " [ 28.   1.   2. 143.   7.  19.  13.   9.]\n",
      " [  5.   7.   7.   2. 168.   6.   4.   4.]\n",
      " [  1.   1.   2.   4.   2. 180.   1.   2.]\n",
      " [ 18.   6.   0.   3.   3.   8. 155.  10.]\n",
      " [  8.   5.   0.   1.   3.   1.   1. 171.]]\n",
      "Time elapsed: 1:41:22.887132\n",
      "   Model saved to checkModel.pth\n",
      "ep 131, loss: 7.47, 6400 train 92.00%, 1600 test 76.19%\n",
      "Time elapsed: 1:42:01.053569\n",
      "ep 132, loss: 6.75, 6400 train 92.91%, 1600 test 79.88%\n",
      "Time elapsed: 1:42:39.147977\n",
      "ep 133, loss: 7.13, 6400 train 92.62%, 1600 test 75.69%\n",
      "Time elapsed: 1:43:17.208328\n",
      "ep 134, loss: 7.37, 6400 train 92.44%, 1600 test 73.38%\n",
      "Time elapsed: 1:43:55.346100\n",
      "ep 135, loss: 7.46, 6400 train 92.31%, 1600 test 77.88%\n",
      "Time elapsed: 1:44:33.503550\n",
      "ep 136, loss: 6.91, 6400 train 92.86%, 1600 test 75.38%\n",
      "Time elapsed: 1:45:11.601922\n",
      "ep 137, loss: 6.75, 6400 train 93.20%, 1600 test 75.44%\n",
      "Time elapsed: 1:45:49.672135\n",
      "ep 138, loss: 6.88, 6400 train 92.69%, 1600 test 77.12%\n",
      "Time elapsed: 1:46:27.807602\n",
      "ep 139, loss: 7.14, 6400 train 92.58%, 1600 test 76.12%\n",
      "Time elapsed: 1:47:05.915561\n",
      "ep 140, loss: 6.39, 6400 train 93.41%, 1600 test 77.56%\n",
      "[[169.   2.   3.   5.   1.   0.   8.   1.]\n",
      " [  1. 164.   8.   0.   9.   4.  13.   2.]\n",
      " [  3.  11. 158.   1.  17.   1.   7.   1.]\n",
      " [  9.   5.  17. 177.   3.   4.   4.   3.]\n",
      " [  2.  16.  13.   3. 164.   1.   3.   1.]\n",
      " [  1.  16.  19.   2.   2. 143.   8.   2.]\n",
      " [ 14.  20.   9.   5.   4.   2. 147.   2.]\n",
      " [ 18.  36.   5.   1.   5.   0.   6. 119.]]\n",
      "Time elapsed: 1:47:44.052928\n",
      "   Model saved to checkModel.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 141, loss: 6.70, 6400 train 92.91%, 1600 test 73.88%\n",
      "Time elapsed: 1:48:22.295921\n",
      "ep 142, loss: 6.89, 6400 train 92.78%, 1600 test 74.88%\n",
      "Time elapsed: 1:49:00.368311\n",
      "ep 143, loss: 6.40, 6400 train 93.23%, 1600 test 74.88%\n",
      "Time elapsed: 1:49:38.448259\n",
      "ep 144, loss: 7.13, 6400 train 92.73%, 1600 test 78.38%\n",
      "Time elapsed: 1:50:16.589472\n",
      "ep 145, loss: 6.48, 6400 train 93.36%, 1600 test 78.88%\n",
      "Time elapsed: 1:50:54.689154\n",
      "ep 146, loss: 5.73, 6400 train 94.17%, 1600 test 77.50%\n",
      "Time elapsed: 1:51:32.761307\n",
      "ep 147, loss: 6.57, 6400 train 92.78%, 1600 test 77.50%\n",
      "Time elapsed: 1:52:10.891391\n",
      "ep 148, loss: 6.95, 6400 train 92.77%, 1600 test 77.19%\n",
      "Time elapsed: 1:52:48.966409\n",
      "ep 149, loss: 6.43, 6400 train 93.05%, 1600 test 75.12%\n",
      "Time elapsed: 1:53:27.032276\n",
      "ep 150, loss: 6.58, 6400 train 93.11%, 1600 test 75.19%\n",
      "[[175.   0.   1.   4.   1.   1.   2.   5.]\n",
      " [  1. 122.   4.   1.   7.  25.  18.  23.]\n",
      " [  6.  13. 111.   2.  30.  21.  10.   6.]\n",
      " [ 17.   2.   1. 172.   1.  14.   8.   7.]\n",
      " [  3.  10.   3.  10. 153.  11.   9.   4.]\n",
      " [  0.   5.   3.   4.   0. 175.   4.   2.]\n",
      " [ 18.  19.   0.  13.   1.  12. 128.  12.]\n",
      " [ 14.   6.   0.   2.   1.   0.   0. 167.]]\n",
      "Time elapsed: 1:54:05.149736\n",
      "   Model saved to checkModel.pth\n",
      "ep 151, loss: 6.32, 6400 train 93.25%, 1600 test 76.81%\n",
      "Time elapsed: 1:54:43.258584\n",
      "ep 152, loss: 5.56, 6400 train 94.12%, 1600 test 75.75%\n",
      "Time elapsed: 1:55:21.309685\n",
      "ep 153, loss: 6.63, 6400 train 93.25%, 1600 test 78.69%\n",
      "Time elapsed: 1:55:59.368834\n",
      "ep 154, loss: 6.18, 6400 train 93.86%, 1600 test 79.12%\n",
      "Time elapsed: 1:56:37.466037\n",
      "ep 155, loss: 6.30, 6400 train 93.39%, 1600 test 74.56%\n",
      "Time elapsed: 1:57:15.600723\n",
      "ep 156, loss: 6.18, 6400 train 93.77%, 1600 test 77.19%\n",
      "Time elapsed: 1:57:53.659251\n",
      "ep 157, loss: 5.87, 6400 train 93.72%, 1600 test 76.12%\n",
      "Time elapsed: 1:58:31.859403\n",
      "ep 158, loss: 5.61, 6400 train 94.09%, 1600 test 77.25%\n",
      "Time elapsed: 1:59:09.938042\n",
      "ep 159, loss: 5.52, 6400 train 94.03%, 1600 test 78.88%\n",
      "Time elapsed: 1:59:47.951036\n",
      "ep 160, loss: 5.72, 6400 train 93.78%, 1600 test 74.62%\n",
      "[[171.   0.   0.   9.   1.   1.   3.   4.]\n",
      " [  0. 132.   5.   3.  13.  23.  19.   6.]\n",
      " [  8.   8. 139.   5.  11.  24.   3.   1.]\n",
      " [ 11.   4.   4. 178.   1.  21.   1.   2.]\n",
      " [  2.   9.  15.  11. 141.  20.   4.   1.]\n",
      " [  2.   6.   1.   3.   1. 178.   2.   0.]\n",
      " [ 18.  16.   1.  12.   1.  24. 129.   2.]\n",
      " [ 20.  22.   5.   4.   2.   4.   7. 126.]]\n",
      "Time elapsed: 2:00:25.939513\n",
      "   Model saved to checkModel.pth\n",
      "ep 161, loss: 5.93, 6400 train 93.91%, 1600 test 78.00%\n",
      "Time elapsed: 2:01:04.301404\n",
      "ep 162, loss: 5.99, 6400 train 93.89%, 1600 test 77.12%\n",
      "Time elapsed: 2:01:42.676853\n",
      "ep 163, loss: 5.91, 6400 train 93.80%, 1600 test 77.88%\n",
      "Time elapsed: 2:02:21.029111\n",
      "ep 164, loss: 5.81, 6400 train 93.91%, 1600 test 79.12%\n",
      "Time elapsed: 2:02:59.455835\n",
      "ep 165, loss: 5.65, 6400 train 94.14%, 1600 test 75.06%\n",
      "Time elapsed: 2:03:37.701313\n",
      "ep 166, loss: 6.01, 6400 train 93.86%, 1600 test 78.19%\n",
      "Time elapsed: 2:04:16.087632\n",
      "ep 167, loss: 5.57, 6400 train 94.03%, 1600 test 77.31%\n",
      "Time elapsed: 2:04:54.886473\n",
      "ep 168, loss: 5.16, 6400 train 94.61%, 1600 test 76.50%\n",
      "Time elapsed: 2:05:33.776495\n",
      "ep 169, loss: 5.51, 6400 train 94.28%, 1600 test 78.50%\n",
      "Time elapsed: 2:06:12.089765\n",
      "ep 170, loss: 5.64, 6400 train 94.30%, 1600 test 77.00%\n",
      "[[167.   0.   0.   9.   2.   1.   9.   1.]\n",
      " [  0. 105.   2.   8.  45.   7.  28.   6.]\n",
      " [  6.   4. 123.   9.  44.   9.   4.   0.]\n",
      " [  6.   0.   1. 206.   4.   2.   3.   0.]\n",
      " [  1.   0.   2.  12. 184.   1.   2.   1.]\n",
      " [  1.   4.   1.  16.   6. 158.   5.   2.]\n",
      " [ 13.  13.   1.  19.   3.   3. 148.   3.]\n",
      " [ 12.  14.   1.   5.   9.   1.   7. 141.]]\n",
      "Time elapsed: 2:06:50.288360\n",
      "   Model saved to checkModel.pth\n",
      "ep 171, loss: 5.19, 6400 train 94.72%, 1600 test 78.88%\n",
      "Time elapsed: 2:07:28.848693\n",
      "ep 172, loss: 5.30, 6400 train 94.39%, 1600 test 74.50%\n",
      "Time elapsed: 2:08:07.225505\n",
      "ep 173, loss: 4.90, 6400 train 94.98%, 1600 test 75.31%\n",
      "Time elapsed: 2:08:45.539923\n",
      "ep 174, loss: 5.16, 6400 train 94.52%, 1600 test 76.19%\n",
      "Time elapsed: 2:09:24.306841\n",
      "ep 175, loss: 5.08, 6400 train 94.98%, 1600 test 79.31%\n",
      "Time elapsed: 2:10:02.670624\n",
      "ep 176, loss: 5.15, 6400 train 94.55%, 1600 test 78.00%\n",
      "Time elapsed: 2:10:41.101222\n",
      "ep 177, loss: 5.34, 6400 train 94.55%, 1600 test 76.75%\n",
      "Time elapsed: 2:11:19.574620\n",
      "ep 178, loss: 5.62, 6400 train 94.50%, 1600 test 78.62%\n",
      "Time elapsed: 2:11:57.996887\n",
      "ep 179, loss: 5.20, 6400 train 94.66%, 1600 test 77.88%\n",
      "Time elapsed: 2:12:36.387193\n",
      "ep 180, loss: 5.35, 6400 train 94.53%, 1600 test 77.25%\n",
      "[[167.   1.   0.  11.   1.   3.   4.   2.]\n",
      " [  0. 156.   2.   4.  11.   5.  18.   5.]\n",
      " [  5.  22. 103.   6.  38.  17.   5.   3.]\n",
      " [  7.   3.   1. 203.   1.   3.   3.   1.]\n",
      " [  2.  14.   2.   8. 170.   5.   1.   1.]\n",
      " [  1.  13.   2.   7.   3. 163.   0.   4.]\n",
      " [ 10.  21.   1.  23.   3.   8. 137.   0.]\n",
      " [ 12.  24.   1.   5.   3.   2.   6. 137.]]\n",
      "Time elapsed: 2:13:14.438773\n",
      "   Model saved to checkModel.pth\n",
      "ep 181, loss: 4.58, 6400 train 95.38%, 1600 test 76.19%\n",
      "Time elapsed: 2:13:52.908182\n",
      "ep 182, loss: 4.66, 6400 train 95.30%, 1600 test 75.69%\n",
      "Time elapsed: 2:14:31.185805\n",
      "ep 183, loss: 4.67, 6400 train 95.17%, 1600 test 78.50%\n",
      "Time elapsed: 2:15:09.387851\n",
      "ep 184, loss: 4.49, 6400 train 95.31%, 1600 test 77.31%\n",
      "Time elapsed: 2:15:47.912775\n",
      "ep 185, loss: 4.88, 6400 train 95.06%, 1600 test 74.06%\n",
      "Time elapsed: 2:16:26.464236\n",
      "ep 186, loss: 4.62, 6400 train 95.30%, 1600 test 77.00%\n",
      "Time elapsed: 2:17:04.811945\n",
      "ep 187, loss: 5.11, 6400 train 95.03%, 1600 test 78.50%\n",
      "Time elapsed: 2:17:43.098519\n",
      "ep 188, loss: 4.14, 6400 train 95.78%, 1600 test 78.25%\n",
      "Time elapsed: 2:18:21.231877\n",
      "ep 189, loss: 4.61, 6400 train 95.45%, 1600 test 75.00%\n",
      "Time elapsed: 2:18:59.255234\n",
      "ep 190, loss: 4.24, 6400 train 95.62%, 1600 test 74.06%\n",
      "[[148.   2.   0.  28.   1.   4.   6.   0.]\n",
      " [  0. 119.   3.   5.  21.  16.  25.  12.]\n",
      " [  4.  12. 105.   7.  49.  14.   7.   1.]\n",
      " [  5.   0.   2. 199.   7.   5.   2.   2.]\n",
      " [  1.   6.   0.  12. 179.   3.   1.   1.]\n",
      " [  2.   3.   2.   5.   4. 171.   3.   3.]\n",
      " [ 10.  13.   2.  38.   3.  13. 119.   5.]\n",
      " [  8.  18.   1.   7.   5.   3.   3. 145.]]\n",
      "Time elapsed: 2:19:37.297016\n",
      "   Model saved to checkModel.pth\n",
      "ep 191, loss: 4.26, 6400 train 95.33%, 1600 test 75.81%\n",
      "Time elapsed: 2:20:15.442846\n",
      "ep 192, loss: 4.66, 6400 train 95.31%, 1600 test 78.31%\n",
      "Time elapsed: 2:20:53.399408\n",
      "ep 193, loss: 4.56, 6400 train 95.53%, 1600 test 78.00%\n",
      "Time elapsed: 2:21:31.449976\n",
      "ep 194, loss: 4.80, 6400 train 95.03%, 1600 test 76.06%\n",
      "Time elapsed: 2:22:09.528795\n",
      "ep 195, loss: 5.03, 6400 train 94.98%, 1600 test 77.88%\n",
      "Time elapsed: 2:22:47.526493\n",
      "ep 196, loss: 4.71, 6400 train 95.33%, 1600 test 77.50%\n",
      "Time elapsed: 2:23:25.464170\n",
      "ep 197, loss: 4.28, 6400 train 95.34%, 1600 test 73.50%\n",
      "Time elapsed: 2:24:03.468514\n",
      "ep 198, loss: 4.14, 6400 train 95.84%, 1600 test 78.94%\n",
      "Time elapsed: 2:24:41.551684\n",
      "ep 199, loss: 4.88, 6400 train 95.22%, 1600 test 76.69%\n",
      "Time elapsed: 2:25:19.541929\n",
      "ep 200, loss: 4.50, 6400 train 95.38%, 1600 test 76.94%\n",
      "[[149.   1.   1.   5.   1.   1.  25.   6.]\n",
      " [  0. 152.   5.   0.   4.   6.  28.   6.]\n",
      " [  2.  17. 136.   4.  16.   6.  16.   2.]\n",
      " [  7.   5.   8. 169.   0.   7.  23.   3.]\n",
      " [  0.  16.   5.  10. 160.   4.   7.   1.]\n",
      " [  1.  10.   4.   3.   4. 156.  13.   2.]\n",
      " [  3.  22.   1.   3.   0.   1. 173.   0.]\n",
      " [  6.  37.   4.   0.   0.   0.   7. 136.]]\n",
      "Time elapsed: 2:25:57.589864\n",
      "   Model saved to checkModel.pth\n",
      "ep 201, loss: 3.69, 6400 train 96.16%, 1600 test 75.25%\n",
      "Time elapsed: 2:26:35.689032\n",
      "ep 202, loss: 4.48, 6400 train 95.39%, 1600 test 78.94%\n",
      "Time elapsed: 2:27:13.697539\n",
      "ep 203, loss: 4.65, 6400 train 95.53%, 1600 test 77.06%\n",
      "Time elapsed: 2:27:51.762352\n",
      "ep 204, loss: 4.34, 6400 train 95.42%, 1600 test 78.56%\n",
      "Time elapsed: 2:28:29.773055\n",
      "ep 205, loss: 3.91, 6400 train 96.14%, 1600 test 75.38%\n",
      "Time elapsed: 2:29:08.142495\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Tracking training time ###\n",
    "##############################\n",
    "start_time = time.time() ## Added\n",
    "time_elapsed = 0  ## Added Line\n",
    "##############################\n",
    "\n",
    "###############################\n",
    "### Tracking nn performance ###\n",
    "###############################\n",
    "minibatch_loss_list, train_accuracy_list, test_accuracy_list = [], [], [] ## Added\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "print(\"Using device: {}\"\n",
    "      \"\\n\".format(str(device)))\n",
    "########################################################################\n",
    "#######                      Loading Data                        #######\n",
    "########################################################################\n",
    "data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "\n",
    "if train_val_split == 1:\n",
    "    # Train on the entire dataset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset,\n",
    "                        transform=transform('train'))\n",
    "    trainloader = torch.utils.data.DataLoader(data,\n",
    "                        batch_size=batch_size, shuffle=True);\n",
    "else:\n",
    "    # Split the dataset into trainset and testset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "    data.len=len(data)\n",
    "    train_len = int((train_val_split)*data.len)\n",
    "    test_len = data.len - train_len\n",
    "    train_subset, test_subset = random_split(data, [train_len, test_len])\n",
    "    trainset = DatasetFromSubset(\n",
    "        train_subset, transform=transform('train'))\n",
    "    testset = DatasetFromSubset(\n",
    "        test_subset, transform=transform('test'))\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "    testloader = torch.utils.data.DataLoader(testset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Get model, loss criterion and optimizer from student\n",
    "net = net.to(device)\n",
    "criterion = loss_func\n",
    "optimizer = optimizer\n",
    "# get weight initialization and lr scheduler, if appropriate\n",
    "weights_init = weights_init\n",
    "scheduler = scheduler\n",
    "\n",
    "# apply custom weight initialization, if it exists\n",
    "net.apply(weights_init)\n",
    "\n",
    "########################################################################\n",
    "#######                        Training                          #######\n",
    "########################################################################\n",
    "print(\"Start training...\")\n",
    "for epoch in range(1,epochs+1):\n",
    "    total_loss = 0\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in trainloader:           # Load batch\n",
    "        images, labels = batch \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = net(images)             # Process batch\n",
    "\n",
    "        loss = criterion(preds, labels) # Calculate loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                 # Calculate gradients\n",
    "        optimizer.step()                # Update weights\n",
    "\n",
    "        output = preds.argmax(dim=1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_images += labels.size(0)\n",
    "        total_correct += output.eq(labels).sum().item()\n",
    "        minibatch_loss_list.append(loss.item())  ## Added\n",
    "\n",
    "    # apply lr schedule, if it exists\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100 \n",
    "    train_accuracy_list.append(model_accuracy)  ## Added\n",
    "    print('ep {0}, loss: {1:.2f}, {2} train {3:.2f}%'.format(\n",
    "           epoch, total_loss, total_images, model_accuracy), end='')\n",
    "\n",
    "    if train_val_split < 1:\n",
    "        test_network(net,testloader, test_accuracy_list,\n",
    "                     print_confusion=(epoch % 10 == 0)) ## Added\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - start_time  ## Added Line\n",
    "    print(f'Time elapsed: {str(datetime.timedelta(seconds = time_elapsed))}') ## Added\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.state_dict(),'checkModel.pth')\n",
    "        print(\"   Model saved to checkModel.pth\")        \n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "time_elapsed = time.time() - start_time ## Added Line\n",
    "print(f'total time needed to train network: {str(datetime.timedelta(seconds = time_elapsed))}\\ntotal time in seconds: {time_elapsed}') ## Added Line\n",
    "torch.save(net.state_dict(),'savedModel.pth')\n",
    "print(\"   Model saved to savedModel.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 200\n",
      "learning rate: 0.001\n",
      "train_val_split: 0.8\n",
      "epochs: 150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-055fe160caa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Getting count of each cat breed, should be close to 8*0.8*1000 initially..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain_data_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cat_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtest_data_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cat_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\COMP9444\\Cat-Breed-Classification\\helper.py\u001b[0m in \u001b[0;36mget_cat_count\u001b[1;34m(dataloader, dataloadertype)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcounter_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mXs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-c70a18b6cf3c>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhue_factor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \"\"\"\n\u001b[0;32m    843\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HSV'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mnp_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 995\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdither\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#**        Data Information     **#\n",
    "###################################\n",
    "print(f'batch size: {batch_size}')\n",
    "print(f'learning rate: {learning_rate}')\n",
    "print(f'train_val_split: {train_val_split}')\n",
    "print(f'epochs: {epochs}')\n",
    "\n",
    "\n",
    "#############################\n",
    "#**         END           **#\n",
    "#############################\n",
    "\n",
    "\n",
    "# Getting count of each cat breed, should be close to 8*0.8*1000 initially..\n",
    "train_data_distribution = get_cat_count(trainloader, 'training data')\n",
    "# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\n",
    "test_data_distribution = get_cat_count(testloader, 'test data')\n",
    "\n",
    "print(f'training data distribution - {train_data_distribution}')\n",
    "print(f'test data distribution - {test_data_distribution}')\n",
    "\n",
    "plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                   num_epochs=epochs,\n",
    "                   iter_per_epoch=len(trainloader),\n",
    "                   results_dir=None,\n",
    "                   averaging_iterations=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_accuracy(train_acc_list=train_accuracy_list,\n",
    "              test_acc_list=test_accuracy_list,\n",
    "              results_dir=None)\n",
    "plt.show()\n",
    "\n",
    "net.cpu()\n",
    "show_examples(model=net, data_loader=testloader, class_dict=cat_dict)\n",
    "\n",
    "conf_matrix = compute_confusion_matrix(model=net, data_loader=testloader, device=torch.device('cpu'))\n",
    "print(conf_matrix)\n",
    "plot_confusion_matrix(conf_matrix, class_names=cat_dict.values(), test_data_distribution=test_data_distribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
