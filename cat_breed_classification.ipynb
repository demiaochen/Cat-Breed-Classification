{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from helper import get_cat_count, count_parameters, compute_confusion_matrix, show_examples, plot_training_loss, plot_accuracy, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9444 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat breed classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**student.py**\n",
    "\n",
    "UNSW COMP9444 Neural Networks and Deep Learning\n",
    "\n",
    "You may modify this file however you wish, including creating additional\n",
    "variables, functions, classes, etc., so long as your code runs with the\n",
    "hw2main.py file unmodified, and you are only using the approved packages.\n",
    "\n",
    "You have been given some default values for the variables train_val_split,\n",
    "batch_size as well as the transform function.\n",
    "You are encouraged to modify these to improve the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question:**\n",
    "\n",
    "Briefly describe how your program works, and explain any design and training\n",
    "decisions you made along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "######     Specify transform(s) to be applied to the input images     ######\n",
    "############################################################################\n",
    "\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    # channel size = 3\n",
    "\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                # transforms.RandomCrop((64, 64)),\n",
    "                transforms.RandomResizedCrop(size=80, \n",
    "                         scale=(0.75, 1.0), ratio=(0.75, 1.3)), # original 80*80, avoid cropping important info\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation((-10,10)),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "                transforms.RandomPosterize(bits=3, p=0.4),\n",
    "                transforms.RandomEqualize(p=0.1),\n",
    "                transforms.RandomGrayscale(p=0.1),\n",
    "                transforms.RandomPerspective(distortion_scale=0.05, p=0.1, fill=0),\n",
    "                ## T.RandomErasing(),\n",
    "                ## T.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
    "                ## T.RandomInvert(p=0.05),\n",
    "                transforms.ToTensor()\n",
    "                ## Standardize each channel of the image\n",
    "                ## T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "    elif mode == 'test':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                # transforms.CenterCrop((64, 64)),\n",
    "                transforms.ToTensor()\n",
    "                ## Standardize each channel of the image\n",
    "                ## transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                ##                                 [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet with convolutional block\n",
    "\n",
    "> Resizing logic (note this does skip connections after 2 conv layers because figure is ResNet-50, however for ResNet-34 and ResNet-18 we skip after 1 conv block  \n",
    "\n",
    "![conv block resnet](./ResidualBlock.png)\n",
    "\n",
    "image source: https://stackoverflow.com/questions/58200107/stuck-understanding-resnets-identity-block-and-convolutional-blocks\n",
    "\n",
    "Helpful resources (implementations of residual blocks): https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L14/2-resnet-example.ipynb \n",
    "\n",
    "https://stackoverflow.com/questions/60817390/implementing-a-simple-resnet-block-with-pytorch\n",
    "\n",
    "\n",
    "![resnet18andresnet34layers](./resnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#####                      Specify NN to be used                           ######\n",
    "#################################################################################\n",
    "\n",
    "# Trying an implementation of ResNet \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, block_type='convolution'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(output_channel, output_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(output_channel)\n",
    "        )\n",
    "        \n",
    "        # For ResNet18 as input and output dimensions will always match.\n",
    "        self.skip_connection = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(output_channel)\n",
    "        )\n",
    "        \n",
    "        self.block_type = block_type\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.block_type is 'convolution':\n",
    "            shortcut = self.skip_connection(x)\n",
    "        else:\n",
    "            shortcut = x\n",
    "        block = self.block(x)\n",
    "        x = nn.functional.relu(block + shortcut)\n",
    "        return x\n",
    "\n",
    "\n",
    "## Should we try maxpooling after residual blocks?\n",
    "## ResNet18 Architecture: https://www.researchgate.net/figure/Original-ResNet-18-Architecture_fig1_336642248\n",
    "## ResNet18 Architecture: https://www.researchgate.net/profile/Paolo-Napoletano/publication/322476121/figure/tbl1/AS:668726449946625@1536448218498/ResNet-18-Architecture.png\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.initial_cnn_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "        )\n",
    "        \n",
    "        self.residual_block_1 = ResidualBlock(64, 64, block_type='identity')\n",
    "        self.residual_block_2 = ResidualBlock(64, 128)\n",
    "        \n",
    "        self.residual_block_3 = ResidualBlock(128, 128, block_type='identity')\n",
    "        self.residual_block_4 = ResidualBlock(128, 256)\n",
    "        \n",
    "\n",
    "        self.residual_block_5 = ResidualBlock(256, 256, block_type='identity')\n",
    "        self.residual_block_6 = ResidualBlock(256, 512)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.residual_block_7 = ResidualBlock(512, 512, block_type='identity')\n",
    "        \n",
    "        # shrink final conv layer width to 1\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512*1*1, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(1024, 8)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.initial_cnn_layer(x)\n",
    "        x = self.residual_block_1(x)\n",
    "        x = self.residual_block_2(x)\n",
    "        x = self.residual_block_3(x)\n",
    "        x = self.residual_block_4(x)        \n",
    "        x = self.residual_block_5(x)\n",
    "        x = self.residual_block_6(x)        \n",
    "        x = self.residual_block_7(x)        \n",
    "        x = self.avgpool(x)\n",
    "    \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomResizedCrop(size=(80, 80), scale=(0.75, 1.0), ratio=(0.75, 1.3), interpolation=bilinear)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=[0.6, 1.4], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.2, 0.2])\n",
      "    RandomPosterize(bits=3,p=0.4)\n",
      "    RandomEqualize(p=0.1)\n",
      "    RandomGrayscale(p=0.1)\n",
      "    RandomPerspective(p=0.1)\n",
      "    ToTensor()\n",
      ")\n",
      "ResNet18(\n",
      "  (initial_cnn_layer): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (residual_block_1): ResidualBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (skip_connection): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (residual_block_2): ResidualBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (skip_connection): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (residual_block_3): ResidualBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (skip_connection): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (residual_block_4): ResidualBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (skip_connection): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (residual_block_5): ResidualBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (skip_connection): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (residual_block_6): ResidualBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (skip_connection): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (residual_block_7): ResidualBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (skip_connection): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.4, inplace=False)\n",
      "    (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "+-------------------------------------------+------------+\n",
      "|                  Modules                  | Parameters |\n",
      "+-------------------------------------------+------------+\n",
      "|         initial_cnn_layer.0.weight        |    9408    |\n",
      "|          initial_cnn_layer.0.bias         |     64     |\n",
      "|         initial_cnn_layer.1.weight        |     64     |\n",
      "|          initial_cnn_layer.1.bias         |     64     |\n",
      "|      residual_block_1.block.0.weight      |   36864    |\n",
      "|       residual_block_1.block.0.bias       |     64     |\n",
      "|      residual_block_1.block.1.weight      |     64     |\n",
      "|       residual_block_1.block.1.bias       |     64     |\n",
      "|      residual_block_1.block.3.weight      |   36864    |\n",
      "|       residual_block_1.block.3.bias       |     64     |\n",
      "|      residual_block_1.block.4.weight      |     64     |\n",
      "|       residual_block_1.block.4.bias       |     64     |\n",
      "| residual_block_1.skip_connection.0.weight |    4096    |\n",
      "|  residual_block_1.skip_connection.0.bias  |     64     |\n",
      "| residual_block_1.skip_connection.1.weight |     64     |\n",
      "|  residual_block_1.skip_connection.1.bias  |     64     |\n",
      "|      residual_block_2.block.0.weight      |   73728    |\n",
      "|       residual_block_2.block.0.bias       |    128     |\n",
      "|      residual_block_2.block.1.weight      |    128     |\n",
      "|       residual_block_2.block.1.bias       |    128     |\n",
      "|      residual_block_2.block.3.weight      |   147456   |\n",
      "|       residual_block_2.block.3.bias       |    128     |\n",
      "|      residual_block_2.block.4.weight      |    128     |\n",
      "|       residual_block_2.block.4.bias       |    128     |\n",
      "| residual_block_2.skip_connection.0.weight |    8192    |\n",
      "|  residual_block_2.skip_connection.0.bias  |    128     |\n",
      "| residual_block_2.skip_connection.1.weight |    128     |\n",
      "|  residual_block_2.skip_connection.1.bias  |    128     |\n",
      "|      residual_block_3.block.0.weight      |   147456   |\n",
      "|       residual_block_3.block.0.bias       |    128     |\n",
      "|      residual_block_3.block.1.weight      |    128     |\n",
      "|       residual_block_3.block.1.bias       |    128     |\n",
      "|      residual_block_3.block.3.weight      |   147456   |\n",
      "|       residual_block_3.block.3.bias       |    128     |\n",
      "|      residual_block_3.block.4.weight      |    128     |\n",
      "|       residual_block_3.block.4.bias       |    128     |\n",
      "| residual_block_3.skip_connection.0.weight |   16384    |\n",
      "|  residual_block_3.skip_connection.0.bias  |    128     |\n",
      "| residual_block_3.skip_connection.1.weight |    128     |\n",
      "|  residual_block_3.skip_connection.1.bias  |    128     |\n",
      "|      residual_block_4.block.0.weight      |   294912   |\n",
      "|       residual_block_4.block.0.bias       |    256     |\n",
      "|      residual_block_4.block.1.weight      |    256     |\n",
      "|       residual_block_4.block.1.bias       |    256     |\n",
      "|      residual_block_4.block.3.weight      |   589824   |\n",
      "|       residual_block_4.block.3.bias       |    256     |\n",
      "|      residual_block_4.block.4.weight      |    256     |\n",
      "|       residual_block_4.block.4.bias       |    256     |\n",
      "| residual_block_4.skip_connection.0.weight |   32768    |\n",
      "|  residual_block_4.skip_connection.0.bias  |    256     |\n",
      "| residual_block_4.skip_connection.1.weight |    256     |\n",
      "|  residual_block_4.skip_connection.1.bias  |    256     |\n",
      "|      residual_block_5.block.0.weight      |   589824   |\n",
      "|       residual_block_5.block.0.bias       |    256     |\n",
      "|      residual_block_5.block.1.weight      |    256     |\n",
      "|       residual_block_5.block.1.bias       |    256     |\n",
      "|      residual_block_5.block.3.weight      |   589824   |\n",
      "|       residual_block_5.block.3.bias       |    256     |\n",
      "|      residual_block_5.block.4.weight      |    256     |\n",
      "|       residual_block_5.block.4.bias       |    256     |\n",
      "| residual_block_5.skip_connection.0.weight |   65536    |\n",
      "|  residual_block_5.skip_connection.0.bias  |    256     |\n",
      "| residual_block_5.skip_connection.1.weight |    256     |\n",
      "|  residual_block_5.skip_connection.1.bias  |    256     |\n",
      "|      residual_block_6.block.0.weight      |  1179648   |\n",
      "|       residual_block_6.block.0.bias       |    512     |\n",
      "|      residual_block_6.block.1.weight      |    512     |\n",
      "|       residual_block_6.block.1.bias       |    512     |\n",
      "|      residual_block_6.block.3.weight      |  2359296   |\n",
      "|       residual_block_6.block.3.bias       |    512     |\n",
      "|      residual_block_6.block.4.weight      |    512     |\n",
      "|       residual_block_6.block.4.bias       |    512     |\n",
      "| residual_block_6.skip_connection.0.weight |   131072   |\n",
      "|  residual_block_6.skip_connection.0.bias  |    512     |\n",
      "| residual_block_6.skip_connection.1.weight |    512     |\n",
      "|  residual_block_6.skip_connection.1.bias  |    512     |\n",
      "|      residual_block_7.block.0.weight      |  2359296   |\n",
      "|       residual_block_7.block.0.bias       |    512     |\n",
      "|      residual_block_7.block.1.weight      |    512     |\n",
      "|       residual_block_7.block.1.bias       |    512     |\n",
      "|      residual_block_7.block.3.weight      |  2359296   |\n",
      "|       residual_block_7.block.3.bias       |    512     |\n",
      "|      residual_block_7.block.4.weight      |    512     |\n",
      "|       residual_block_7.block.4.bias       |    512     |\n",
      "| residual_block_7.skip_connection.0.weight |   262144   |\n",
      "|  residual_block_7.skip_connection.0.bias  |    512     |\n",
      "| residual_block_7.skip_connection.1.weight |    512     |\n",
      "|  residual_block_7.skip_connection.1.bias  |    512     |\n",
      "|             fc_layers.1.weight            |   524288   |\n",
      "|              fc_layers.1.bias             |    1024    |\n",
      "|             fc_layers.2.weight            |    1024    |\n",
      "|              fc_layers.2.bias             |    1024    |\n",
      "|             fc_layers.5.weight            |  1048576   |\n",
      "|              fc_layers.5.bias             |    1024    |\n",
      "|             fc_layers.6.weight            |    1024    |\n",
      "|              fc_layers.6.bias             |    1024    |\n",
      "|             fc_layers.9.weight            |    8192    |\n",
      "|              fc_layers.9.bias             |     8      |\n",
      "+-------------------------------------------+------------+\n",
      "Total Trainable Params: 13045448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13045448"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "\n",
    "############################################################################\n",
    "######      Specify the optimizer and loss function                   ######\n",
    "############################################################################\n",
    "learning_rate = 0.001\n",
    "# optimizer = torch.optim.SGD(net.parameters(), momentum=0.9, lr=learning_rate)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# loss_func = F.nll_loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "############################################################################\n",
    "######  Custom weight initialization and lr scheduling are optional   ######\n",
    "############################################################################\n",
    "\n",
    "# Normally, the default weight initialization and fixed learing rate\n",
    "# should work fine. But, we have made it possible for you to define\n",
    "# your own custom weight initialization and lr scheduler, if you wish.\n",
    "def weights_init(m):\n",
    "    return\n",
    "\n",
    "scheduler = None\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#######              Metaparameters and training options              ######\n",
    "############################################################################\n",
    "dataset = \"./data\"\n",
    "train_val_split = 0.8\n",
    "batch_size = 200\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "###############################################\n",
    "#**          Print Network Information      **#\n",
    "###############################################\n",
    "print(transform('train'))\n",
    "print(net)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###########################\n",
    "## Cat breed dictionary  ##\n",
    "###########################\n",
    "cat_dict = {\n",
    "    0: 'bombay',\n",
    "    1: 'calico',\n",
    "    2: 'persian',\n",
    "    3: 'russianblue',\n",
    "    4: 'siamese',\n",
    "    5: 'tiger',\n",
    "    6: 'tortoiseshell',\n",
    "    7: 'tuxedo'\n",
    "}\n",
    "\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "# Test network on validation set, if it exists.\n",
    "## Added params\n",
    "def test_network(net,testloader,test_accuracy_list,print_confusion=False):\n",
    "    net.eval()\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "    conf_matrix = np.zeros((8,8))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            conf_matrix = conf_matrix + metrics.confusion_matrix(\n",
    "                labels.cpu(),predicted.cpu(),labels=[0,1,2,3,4,5,6,7])\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    test_accuracy_list.append(model_accuracy)\n",
    "    print(', {0} test {1:.2f}%'.format(total_images,model_accuracy))\n",
    "    if print_confusion:\n",
    "        np.set_printoptions(precision=2, suppress=True)\n",
    "        print(conf_matrix)\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Start training...\n",
      "ep 1, loss: 63.54, 6400 train 23.42%, 1600 test 20.12%\n",
      "Time elapsed: 0:00:43.328747\n",
      "ep 2, loss: 57.76, 6400 train 30.25%, 1600 test 23.19%\n",
      "Time elapsed: 0:01:26.764095\n",
      "ep 3, loss: 52.81, 6400 train 36.86%, 1600 test 29.31%\n",
      "Time elapsed: 0:02:10.534041\n",
      "ep 4, loss: 52.06, 6400 train 38.89%, 1600 test 27.81%\n",
      "Time elapsed: 0:02:54.252197\n",
      "ep 5, loss: 48.88, 6400 train 42.88%, 1600 test 31.31%\n",
      "Time elapsed: 0:03:57.074944\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-9bb771d0e2e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mtotal_images\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Tracking training time ###\n",
    "##############################\n",
    "start_time = time.time() ## Added\n",
    "time_elapsed = 0  ## Added Line\n",
    "##############################\n",
    "\n",
    "###############################\n",
    "### Tracking nn performance ###\n",
    "###############################\n",
    "minibatch_loss_list, train_accuracy_list, test_accuracy_list = [], [], [] ## Added\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "print(\"Using device: {}\"\n",
    "      \"\\n\".format(str(device)))\n",
    "########################################################################\n",
    "#######                      Loading Data                        #######\n",
    "########################################################################\n",
    "data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "\n",
    "if train_val_split == 1:\n",
    "    # Train on the entire dataset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset,\n",
    "                        transform=transform('train'))\n",
    "    trainloader = torch.utils.data.DataLoader(data,\n",
    "                        batch_size=batch_size, shuffle=True);\n",
    "else:\n",
    "    # Split the dataset into trainset and testset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "    data.len=len(data)\n",
    "    train_len = int((train_val_split)*data.len)\n",
    "    test_len = data.len - train_len\n",
    "    train_subset, test_subset = random_split(data, [train_len, test_len])\n",
    "    trainset = DatasetFromSubset(\n",
    "        train_subset, transform=transform('train'))\n",
    "    testset = DatasetFromSubset(\n",
    "        test_subset, transform=transform('test'))\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "    testloader = torch.utils.data.DataLoader(testset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Get model, loss criterion and optimizer from student\n",
    "net = net.to(device)\n",
    "criterion = loss_func\n",
    "optimizer = optimizer\n",
    "# get weight initialization and lr scheduler, if appropriate\n",
    "weights_init = weights_init\n",
    "scheduler = scheduler\n",
    "\n",
    "# apply custom weight initialization, if it exists\n",
    "net.apply(weights_init)\n",
    "\n",
    "########################################################################\n",
    "#######                        Training                          #######\n",
    "########################################################################\n",
    "print(\"Start training...\")\n",
    "for epoch in range(1,epochs+1):\n",
    "    total_loss = 0\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in trainloader:           # Load batch\n",
    "        images, labels = batch \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = net(images)             # Process batch\n",
    "\n",
    "        loss = criterion(preds, labels) # Calculate loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                 # Calculate gradients\n",
    "        optimizer.step()                # Update weights\n",
    "\n",
    "        output = preds.argmax(dim=1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_images += labels.size(0)\n",
    "        total_correct += output.eq(labels).sum().item()\n",
    "        minibatch_loss_list.append(loss.item())  ## Added\n",
    "\n",
    "    # apply lr schedule, if it exists\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100 \n",
    "    train_accuracy_list.append(model_accuracy)  ## Added\n",
    "    print('ep {0}, loss: {1:.2f}, {2} train {3:.2f}%'.format(\n",
    "           epoch, total_loss, total_images, model_accuracy), end='')\n",
    "\n",
    "    if train_val_split < 1:\n",
    "        test_network(net,testloader, test_accuracy_list,\n",
    "                     print_confusion=(epoch % 10 == 0)) ## Added\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - start_time  ## Added Line\n",
    "    print(f'Time elapsed: {str(datetime.timedelta(seconds = time_elapsed))}') ## Added\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.state_dict(),'checkModel.pth')\n",
    "        print(\"   Model saved to checkModel.pth\")        \n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "time_elapsed = time.time() - start_time ## Added Line\n",
    "print(f'total time needed to train network: {str(datetime.timedelta(seconds = time_elapsed))}\\ntotal time in seconds: {time_elapsed}') ## Added Line\n",
    "torch.save(net.state_dict(),'savedModel.pth')\n",
    "print(\"   Model saved to savedModel.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 200\n",
      "learning rate: 0.001\n",
      "train_val_split: 0.8\n",
      "epochs: 150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-055fe160caa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Getting count of each cat breed, should be close to 8*0.8*1000 initially..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain_data_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cat_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtest_data_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cat_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\COMP9444\\Cat-Breed-Classification\\helper.py\u001b[0m in \u001b[0;36mget_cat_count\u001b[1;34m(dataloader, dataloadertype)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcounter_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mXs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-c70a18b6cf3c>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhue_factor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \"\"\"\n\u001b[0;32m    843\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HSV'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mnp_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 995\u001b[1;33m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdither\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#**        Data Information     **#\n",
    "###################################\n",
    "print(f'batch size: {batch_size}')\n",
    "print(f'learning rate: {learning_rate}')\n",
    "print(f'train_val_split: {train_val_split}')\n",
    "print(f'epochs: {epochs}')\n",
    "\n",
    "\n",
    "#############################\n",
    "#**         END           **#\n",
    "#############################\n",
    "\n",
    "\n",
    "# Getting count of each cat breed, should be close to 8*0.8*1000 initially..\n",
    "train_data_distribution = get_cat_count(trainloader, 'training data')\n",
    "# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\n",
    "test_data_distribution = get_cat_count(testloader, 'test data')\n",
    "\n",
    "print(f'training data distribution - {train_data_distribution}')\n",
    "print(f'test data distribution - {test_data_distribution}')\n",
    "\n",
    "plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                   num_epochs=epochs,\n",
    "                   iter_per_epoch=len(trainloader),\n",
    "                   results_dir=None,\n",
    "                   averaging_iterations=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_accuracy(train_acc_list=train_accuracy_list,\n",
    "              test_acc_list=test_accuracy_list,\n",
    "              results_dir=None)\n",
    "plt.show()\n",
    "\n",
    "net.cpu()\n",
    "show_examples(model=net, data_loader=testloader, class_dict=cat_dict)\n",
    "\n",
    "conf_matrix = compute_confusion_matrix(model=net, data_loader=testloader, device=torch.device('cpu'))\n",
    "print(conf_matrix)\n",
    "plot_confusion_matrix(conf_matrix, class_names=cat_dict.values(), test_data_distribution=test_data_distribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
