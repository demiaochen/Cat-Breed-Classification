{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from helper import get_cat_count, count_parameters, compute_confusion_matrix, show_examples, plot_training_loss, plot_accuracy, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9444 Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat breed classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**student.py**\n",
    "\n",
    "UNSW COMP9444 Neural Networks and Deep Learning\n",
    "\n",
    "You may modify this file however you wish, including creating additional\n",
    "variables, functions, classes, etc., so long as your code runs with the\n",
    "hw2main.py file unmodified, and you are only using the approved packages.\n",
    "\n",
    "You have been given some default values for the variables train_val_split,\n",
    "batch_size as well as the transform function.\n",
    "You are encouraged to modify these to improve the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question:**\n",
    "\n",
    "Briefly describe how your program works, and explain any design and training\n",
    "decisions you made along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "######     Specify transform(s) to be applied to the input images     ######\n",
    "############################################################################\n",
    "\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "    \"\"\"\n",
    "\n",
    "    # channel size = 3\n",
    "\n",
    "def transform(mode):\n",
    "    \"\"\"\n",
    "    Called when loading the data. Visit this URL for more information:\n",
    "    https://pytorch.org/vision/stable/transforms.html\n",
    "    You may specify different transforms for training and testing\n",
    "    \"\"\"\n",
    "    # Data Augmentation\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                transforms.RandomResizedCrop(size=80, scale=(0.55, 1.0), ratio=(0.75, 1.3)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomPerspective(p=0.2),\n",
    "                transforms.RandomAffine(degrees=(-15, 15), translate=(0.0, 0.5)),\n",
    "                transforms.RandomRotation((-10,10)),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.1, hue=0.02),\n",
    "                transforms.RandomPosterize(bits=3, p=0.3),\n",
    "                transforms.RandomEqualize(p=0.1),\n",
    "                transforms.RandomGrayscale(p=0.01),\n",
    "                transforms.RandomPerspective(distortion_scale=0.05, p=0.1, fill=0),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )\n",
    "    # Keep the testing data original to ensure accuracy\n",
    "    elif mode == 'test':\n",
    "        return transforms.Compose(\n",
    "            [   \n",
    "                transforms.ToTensor()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomResizedCrop(size=(80, 80), scale=(0.55, 1.0), ratio=(0.75, 1.3), interpolation=bilinear)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomPerspective(p=0.2)\n",
      "    RandomAffine(degrees=[-15.0, 15.0], translate=(0.0, 0.5))\n",
      "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.9, 1.1], hue=[-0.02, 0.02])\n",
      "    RandomPosterize(bits=3,p=0.3)\n",
      "    RandomEqualize(p=0.1)\n",
      "    RandomGrayscale(p=0.01)\n",
      "    RandomPerspective(p=0.1)\n",
      "    RandomAdjustSharpness(sharpness_factor=2,p=0.5)\n",
      "    ToTensor()\n",
      ")\n",
      "Network(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0, inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ELU(alpha=1.0, inplace=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ELU(alpha=1.0, inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ELU(alpha=1.0, inplace=True)\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ELU(alpha=1.0, inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ELU(alpha=1.0, inplace=True)\n",
      "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ELU(alpha=1.0, inplace=True)\n",
      "    (27): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ELU(alpha=1.0, inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "    (2): Linear(in_features=3072, out_features=2048, bias=True)\n",
      "    (3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.6, inplace=False)\n",
      "    (6): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Dropout(p=0.4, inplace=False)\n",
      "    (10): Linear(in_features=1024, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "+-----------------------+------------+\n",
      "|        Modules        | Parameters |\n",
      "+-----------------------+------------+\n",
      "|  conv_layers.0.weight |    1728    |\n",
      "|   conv_layers.0.bias  |     64     |\n",
      "|  conv_layers.1.weight |     64     |\n",
      "|   conv_layers.1.bias  |     64     |\n",
      "|  conv_layers.3.weight |   36864    |\n",
      "|   conv_layers.3.bias  |     64     |\n",
      "|  conv_layers.4.weight |     64     |\n",
      "|   conv_layers.4.bias  |     64     |\n",
      "|  conv_layers.7.weight |   73728    |\n",
      "|   conv_layers.7.bias  |    128     |\n",
      "|  conv_layers.8.weight |    128     |\n",
      "|   conv_layers.8.bias  |    128     |\n",
      "| conv_layers.10.weight |   147456   |\n",
      "|  conv_layers.10.bias  |    128     |\n",
      "| conv_layers.11.weight |    128     |\n",
      "|  conv_layers.11.bias  |    128     |\n",
      "| conv_layers.14.weight |   294912   |\n",
      "|  conv_layers.14.bias  |    256     |\n",
      "| conv_layers.15.weight |    256     |\n",
      "|  conv_layers.15.bias  |    256     |\n",
      "| conv_layers.17.weight |   589824   |\n",
      "|  conv_layers.17.bias  |    256     |\n",
      "| conv_layers.18.weight |    256     |\n",
      "|  conv_layers.18.bias  |    256     |\n",
      "| conv_layers.21.weight |   589824   |\n",
      "|  conv_layers.21.bias  |    256     |\n",
      "| conv_layers.22.weight |    256     |\n",
      "|  conv_layers.22.bias  |    256     |\n",
      "| conv_layers.24.weight |   589824   |\n",
      "|  conv_layers.24.bias  |    256     |\n",
      "| conv_layers.25.weight |    256     |\n",
      "|  conv_layers.25.bias  |    256     |\n",
      "| conv_layers.27.weight |   442368   |\n",
      "|  conv_layers.27.bias  |    192     |\n",
      "| conv_layers.28.weight |    192     |\n",
      "|  conv_layers.28.bias  |    192     |\n",
      "|   fc_layers.2.weight  |  6291456   |\n",
      "|    fc_layers.2.bias   |    2048    |\n",
      "|   fc_layers.3.weight  |    2048    |\n",
      "|    fc_layers.3.bias   |    2048    |\n",
      "|   fc_layers.6.weight  |  2097152   |\n",
      "|    fc_layers.6.bias   |    1024    |\n",
      "|   fc_layers.7.weight  |    1024    |\n",
      "|    fc_layers.7.bias   |    1024    |\n",
      "|  fc_layers.10.weight  |    8192    |\n",
      "|   fc_layers.10.bias   |     8      |\n",
      "+-----------------------+------------+\n",
      "Total Trainable Params: 11177352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11177352"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################\n",
    "#####                      Specify NN to be used                           ######\n",
    "#################################################################################\n",
    "\n",
    "### Simplified implementation of VGG16 with 12 layers instead of 16.\n",
    "### Cut layer = 256 - 256 conv layer. 512-512 * 3 conv layers at the end.\n",
    "### Reduced number of nodes on FC layer from 4096 to 1024.\n",
    "vgg_12 = [64, 64, 'maxpool', 128, 128, 'maxpool', 256, 256, 'maxpool', 512, 512, 512, 'maxpool', 'avgpool', 'fc1', 'fc2', 'fc3']    \n",
    "##########################################################################################\n",
    "# trying to take some inspirations from vgg16 but with less channels and fc layer nodes. #\n",
    "##########################################################################################\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ######### block 1 #########\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            \n",
    "            ######### block 2 #########\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            ######### block 3 #########   \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "        \n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2)),\n",
    "            \n",
    "            \n",
    "            ######### block 4 #########\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 192, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ELU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # shrink final conv layer width to 4\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4,4))\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten from conv layers\n",
    "\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(192*4*4, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(1024, 8)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.avgpool(x)       \n",
    "        x = self.fc_layers(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Network()\n",
    "\n",
    "############################################################################\n",
    "######      Specify the optimizer and loss function                   ######\n",
    "############################################################################\n",
    "learning_rate = 0.0005\n",
    "# optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=learning_rate)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# loss_func = F.nll_loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "############################################################################\n",
    "######  Custom weight initialization and lr scheduling are optional   ######\n",
    "############################################################################\n",
    "\n",
    "# Normally, the default weight initialization and fixed learing rate\n",
    "# should work fine. But, we have made it possible for you to define\n",
    "# your own custom weight initialization and lr scheduler, if you wish.\n",
    "def weights_init(m):\n",
    "    return\n",
    "\n",
    "scheduler = None\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#######              Metaparameters and training options              ######\n",
    "############################################################################\n",
    "dataset = \"./data\"\n",
    "train_val_split = 1\n",
    "batch_size = 256 \n",
    "epochs = 500\n",
    "\n",
    "\n",
    "###############################################\n",
    "#**          Print Network Information      **#\n",
    "###############################################\n",
    "print(transform('train'))\n",
    "print(net)\n",
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 2 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU if available, as it should be faster.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###########################\n",
    "## Cat breed dictionary  ##\n",
    "###########################\n",
    "cat_dict = {\n",
    "    0: 'bombay',\n",
    "    1: 'calico',\n",
    "    2: 'persian',\n",
    "    3: 'russianblue',\n",
    "    4: 'siamese',\n",
    "    5: 'tiger',\n",
    "    6: 'tortoiseshell',\n",
    "    7: 'tuxedo'\n",
    "}\n",
    "\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "\n",
    "# Test network on validation set, if it exists.\n",
    "## Added params\n",
    "def test_network(net,testloader,test_accuracy_list,print_confusion=False):\n",
    "    net.eval()\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "    conf_matrix = np.zeros((8,8))\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            conf_matrix = conf_matrix + metrics.confusion_matrix(\n",
    "                labels.cpu(),predicted.cpu(),labels=[0,1,2,3,4,5,6,7])\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    test_accuracy_list.append(model_accuracy)\n",
    "    print(', {0} test {1:.2f}%'.format(total_images,model_accuracy))\n",
    "    if print_confusion:\n",
    "        np.set_printoptions(precision=2, suppress=True)\n",
    "        print(conf_matrix)\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "Start training...\n",
      "ep 1, loss: 63.33, 8000 train 23.49%\n",
      "ep 2, loss: 58.75, 8000 train 29.96%\n",
      "ep 3, loss: 55.28, 8000 train 34.45%\n",
      "ep 4, loss: 53.20, 8000 train 37.48%\n",
      "ep 5, loss: 52.25, 8000 train 39.81%\n",
      "ep 6, loss: 51.31, 8000 train 40.23%\n",
      "ep 7, loss: 49.79, 8000 train 40.85%\n",
      "ep 8, loss: 49.53, 8000 train 42.50%\n",
      "ep 9, loss: 49.61, 8000 train 42.51%\n",
      "ep 10, loss: 47.62, 8000 train 45.62%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:04:18.772727\n",
      "ep 11, loss: 46.33, 8000 train 46.74%\n",
      "ep 12, loss: 45.90, 8000 train 47.25%\n",
      "ep 13, loss: 45.37, 8000 train 48.00%\n",
      "ep 14, loss: 44.29, 8000 train 48.96%\n",
      "ep 15, loss: 43.55, 8000 train 50.19%\n",
      "ep 16, loss: 43.20, 8000 train 50.75%\n",
      "ep 17, loss: 42.32, 8000 train 52.21%\n",
      "ep 18, loss: 41.62, 8000 train 52.59%\n",
      "ep 19, loss: 41.66, 8000 train 53.10%\n",
      "ep 20, loss: 40.44, 8000 train 54.64%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:08:31.478828\n",
      "ep 21, loss: 40.47, 8000 train 53.91%\n",
      "ep 22, loss: 39.84, 8000 train 54.71%\n",
      "ep 23, loss: 40.08, 8000 train 55.10%\n",
      "ep 24, loss: 38.86, 8000 train 55.93%\n",
      "ep 25, loss: 38.30, 8000 train 57.35%\n",
      "ep 26, loss: 39.07, 8000 train 55.76%\n",
      "ep 27, loss: 38.11, 8000 train 57.12%\n",
      "ep 28, loss: 37.26, 8000 train 57.99%\n",
      "ep 29, loss: 37.71, 8000 train 58.20%\n",
      "ep 30, loss: 37.45, 8000 train 58.13%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:12:45.137515\n",
      "ep 31, loss: 35.92, 8000 train 59.50%\n",
      "ep 32, loss: 36.61, 8000 train 58.64%\n",
      "ep 33, loss: 36.22, 8000 train 60.02%\n",
      "ep 34, loss: 36.13, 8000 train 59.35%\n",
      "ep 35, loss: 35.07, 8000 train 60.75%\n",
      "ep 36, loss: 35.04, 8000 train 60.75%\n",
      "ep 37, loss: 34.86, 8000 train 60.80%\n",
      "ep 38, loss: 33.62, 8000 train 62.82%\n",
      "ep 39, loss: 33.93, 8000 train 61.89%\n",
      "ep 40, loss: 34.38, 8000 train 61.81%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:16:51.999857\n",
      "ep 41, loss: 33.39, 8000 train 63.44%\n",
      "ep 42, loss: 33.17, 8000 train 63.42%\n",
      "ep 43, loss: 32.68, 8000 train 63.73%\n",
      "ep 44, loss: 32.76, 8000 train 63.55%\n",
      "ep 45, loss: 32.19, 8000 train 64.31%\n",
      "ep 46, loss: 31.64, 8000 train 64.91%\n",
      "ep 47, loss: 31.53, 8000 train 64.80%\n",
      "ep 48, loss: 31.39, 8000 train 65.08%\n",
      "ep 49, loss: 31.62, 8000 train 65.10%\n",
      "ep 50, loss: 30.75, 8000 train 66.07%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:21:02.327466\n",
      "ep 51, loss: 31.08, 8000 train 65.86%\n",
      "ep 52, loss: 30.99, 8000 train 65.36%\n",
      "ep 53, loss: 30.74, 8000 train 65.62%\n",
      "ep 54, loss: 30.21, 8000 train 66.80%\n",
      "ep 55, loss: 30.17, 8000 train 66.50%\n",
      "ep 56, loss: 29.84, 8000 train 67.47%\n",
      "ep 57, loss: 30.02, 8000 train 67.21%\n",
      "ep 58, loss: 30.42, 8000 train 66.50%\n",
      "ep 59, loss: 29.65, 8000 train 68.15%\n",
      "ep 60, loss: 28.60, 8000 train 69.05%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:25:26.928027\n",
      "ep 61, loss: 28.92, 8000 train 67.92%\n",
      "ep 62, loss: 28.76, 8000 train 68.65%\n",
      "ep 63, loss: 28.42, 8000 train 68.96%\n",
      "ep 64, loss: 28.56, 8000 train 68.54%\n",
      "ep 65, loss: 28.80, 8000 train 68.40%\n",
      "ep 66, loss: 28.29, 8000 train 68.54%\n",
      "ep 67, loss: 27.66, 8000 train 69.35%\n",
      "ep 68, loss: 28.07, 8000 train 69.33%\n",
      "ep 69, loss: 27.88, 8000 train 69.16%\n",
      "ep 70, loss: 27.76, 8000 train 70.03%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:30:00.501730\n",
      "ep 71, loss: 27.40, 8000 train 69.92%\n",
      "ep 72, loss: 26.95, 8000 train 70.01%\n",
      "ep 73, loss: 27.37, 8000 train 69.79%\n",
      "ep 74, loss: 27.44, 8000 train 69.69%\n",
      "ep 75, loss: 26.65, 8000 train 70.19%\n",
      "ep 76, loss: 26.55, 8000 train 70.64%\n",
      "ep 77, loss: 26.69, 8000 train 71.11%\n",
      "ep 78, loss: 26.01, 8000 train 71.15%\n",
      "ep 79, loss: 26.58, 8000 train 70.28%\n",
      "ep 80, loss: 25.52, 8000 train 72.66%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:34:23.779513\n",
      "ep 81, loss: 25.64, 8000 train 72.17%\n",
      "ep 82, loss: 25.04, 8000 train 72.74%\n",
      "ep 83, loss: 26.10, 8000 train 71.20%\n",
      "ep 84, loss: 26.60, 8000 train 71.25%\n",
      "ep 85, loss: 26.60, 8000 train 71.31%\n",
      "ep 86, loss: 25.52, 8000 train 71.97%\n",
      "ep 87, loss: 25.25, 8000 train 72.30%\n",
      "ep 88, loss: 25.52, 8000 train 72.34%\n",
      "ep 89, loss: 24.54, 8000 train 73.12%\n",
      "ep 90, loss: 25.16, 8000 train 72.36%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:38:58.433196\n",
      "ep 91, loss: 24.89, 8000 train 73.14%\n",
      "ep 92, loss: 24.09, 8000 train 73.71%\n",
      "ep 93, loss: 24.70, 8000 train 73.04%\n",
      "ep 94, loss: 24.71, 8000 train 72.79%\n",
      "ep 95, loss: 24.58, 8000 train 73.67%\n",
      "ep 96, loss: 25.03, 8000 train 72.01%\n",
      "ep 97, loss: 24.25, 8000 train 73.41%\n",
      "ep 98, loss: 24.05, 8000 train 73.98%\n",
      "ep 99, loss: 24.47, 8000 train 73.51%\n",
      "ep 100, loss: 23.84, 8000 train 73.88%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:43:22.556730\n",
      "ep 101, loss: 23.27, 8000 train 74.59%\n",
      "ep 102, loss: 23.59, 8000 train 74.12%\n",
      "ep 103, loss: 23.87, 8000 train 73.91%\n",
      "ep 104, loss: 23.53, 8000 train 74.34%\n",
      "ep 105, loss: 23.49, 8000 train 74.66%\n",
      "ep 106, loss: 23.04, 8000 train 75.08%\n",
      "ep 107, loss: 22.91, 8000 train 74.64%\n",
      "ep 108, loss: 23.25, 8000 train 74.34%\n",
      "ep 109, loss: 23.13, 8000 train 74.55%\n",
      "ep 110, loss: 22.51, 8000 train 75.20%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:47:49.978622\n",
      "ep 111, loss: 22.63, 8000 train 75.62%\n",
      "ep 112, loss: 22.65, 8000 train 75.26%\n",
      "ep 113, loss: 22.63, 8000 train 75.01%\n",
      "ep 114, loss: 21.96, 8000 train 75.75%\n",
      "ep 115, loss: 21.85, 8000 train 76.05%\n",
      "ep 116, loss: 22.41, 8000 train 76.02%\n",
      "ep 117, loss: 22.12, 8000 train 75.95%\n",
      "ep 118, loss: 22.04, 8000 train 75.91%\n",
      "ep 119, loss: 22.27, 8000 train 75.89%\n",
      "ep 120, loss: 21.64, 8000 train 76.64%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:52:36.688573\n",
      "ep 121, loss: 21.69, 8000 train 77.11%\n",
      "ep 122, loss: 21.56, 8000 train 76.36%\n",
      "ep 123, loss: 21.82, 8000 train 76.11%\n",
      "ep 124, loss: 21.92, 8000 train 76.10%\n",
      "ep 125, loss: 21.48, 8000 train 76.68%\n",
      "ep 126, loss: 21.68, 8000 train 76.75%\n",
      "ep 127, loss: 21.54, 8000 train 76.06%\n",
      "ep 128, loss: 21.94, 8000 train 76.75%\n",
      "ep 129, loss: 22.06, 8000 train 76.16%\n",
      "ep 130, loss: 21.50, 8000 train 76.76%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 0:56:51.828794\n",
      "ep 131, loss: 20.38, 8000 train 77.90%\n",
      "ep 132, loss: 21.04, 8000 train 76.78%\n",
      "ep 133, loss: 21.20, 8000 train 76.72%\n",
      "ep 134, loss: 21.44, 8000 train 76.39%\n",
      "ep 135, loss: 21.08, 8000 train 77.29%\n",
      "ep 136, loss: 20.82, 8000 train 77.19%\n",
      "ep 137, loss: 20.67, 8000 train 77.44%\n",
      "ep 138, loss: 20.45, 8000 train 77.68%\n",
      "ep 139, loss: 21.20, 8000 train 76.83%\n",
      "ep 140, loss: 20.27, 8000 train 77.79%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:00:22.144836\n",
      "ep 141, loss: 19.95, 8000 train 78.21%\n",
      "ep 142, loss: 20.48, 8000 train 77.86%\n",
      "ep 143, loss: 19.88, 8000 train 78.56%\n",
      "ep 144, loss: 19.93, 8000 train 78.56%\n",
      "ep 145, loss: 19.63, 8000 train 78.90%\n",
      "ep 146, loss: 19.90, 8000 train 78.25%\n",
      "ep 147, loss: 19.75, 8000 train 78.45%\n",
      "ep 148, loss: 20.95, 8000 train 77.88%\n",
      "ep 149, loss: 20.33, 8000 train 77.85%\n",
      "ep 150, loss: 19.29, 8000 train 79.04%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:03:57.664251\n",
      "ep 151, loss: 19.96, 8000 train 78.12%\n",
      "ep 152, loss: 18.98, 8000 train 79.53%\n",
      "ep 153, loss: 19.72, 8000 train 78.69%\n",
      "ep 154, loss: 18.93, 8000 train 79.79%\n",
      "ep 155, loss: 19.32, 8000 train 79.35%\n",
      "ep 156, loss: 18.92, 8000 train 79.75%\n",
      "ep 157, loss: 19.85, 8000 train 78.42%\n",
      "ep 158, loss: 19.33, 8000 train 78.56%\n",
      "ep 159, loss: 19.13, 8000 train 79.21%\n",
      "ep 160, loss: 20.12, 8000 train 78.06%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:07:44.517095\n",
      "ep 161, loss: 19.55, 8000 train 78.47%\n",
      "ep 162, loss: 19.16, 8000 train 78.92%\n",
      "ep 163, loss: 18.70, 8000 train 79.44%\n",
      "ep 164, loss: 18.46, 8000 train 80.45%\n",
      "ep 165, loss: 19.17, 8000 train 79.27%\n",
      "ep 166, loss: 19.31, 8000 train 79.20%\n",
      "ep 167, loss: 18.92, 8000 train 79.75%\n",
      "ep 168, loss: 18.54, 8000 train 79.76%\n",
      "ep 169, loss: 18.88, 8000 train 79.75%\n",
      "ep 170, loss: 18.18, 8000 train 79.84%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:11:35.088863\n",
      "ep 171, loss: 17.75, 8000 train 80.76%\n",
      "ep 172, loss: 18.55, 8000 train 79.96%\n",
      "ep 173, loss: 18.72, 8000 train 79.81%\n",
      "ep 174, loss: 19.57, 8000 train 78.67%\n",
      "ep 175, loss: 19.29, 8000 train 79.03%\n",
      "ep 176, loss: 18.49, 8000 train 79.83%\n",
      "ep 177, loss: 17.73, 8000 train 80.54%\n",
      "ep 178, loss: 18.31, 8000 train 80.03%\n",
      "ep 179, loss: 18.66, 8000 train 80.06%\n",
      "ep 180, loss: 17.92, 8000 train 80.60%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:15:33.434139\n",
      "ep 181, loss: 18.01, 8000 train 80.59%\n",
      "ep 182, loss: 17.79, 8000 train 80.31%\n",
      "ep 183, loss: 17.60, 8000 train 80.67%\n",
      "ep 184, loss: 17.60, 8000 train 81.24%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 185, loss: 17.73, 8000 train 80.83%\n",
      "ep 186, loss: 17.82, 8000 train 80.41%\n",
      "ep 187, loss: 18.17, 8000 train 80.60%\n",
      "ep 188, loss: 17.92, 8000 train 80.42%\n",
      "ep 189, loss: 17.13, 8000 train 80.95%\n",
      "ep 190, loss: 17.58, 8000 train 81.40%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:19:33.588451\n",
      "ep 191, loss: 17.48, 8000 train 80.85%\n",
      "ep 192, loss: 17.48, 8000 train 81.46%\n",
      "ep 193, loss: 18.04, 8000 train 80.60%\n",
      "ep 194, loss: 18.04, 8000 train 80.71%\n",
      "ep 195, loss: 17.51, 8000 train 81.21%\n",
      "ep 196, loss: 17.49, 8000 train 80.83%\n",
      "ep 197, loss: 17.04, 8000 train 81.54%\n",
      "ep 198, loss: 17.22, 8000 train 81.42%\n",
      "ep 199, loss: 17.16, 8000 train 80.95%\n",
      "ep 200, loss: 17.37, 8000 train 80.69%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:23:40.193217\n",
      "ep 201, loss: 17.20, 8000 train 81.54%\n",
      "ep 202, loss: 17.01, 8000 train 81.44%\n",
      "ep 203, loss: 17.62, 8000 train 81.29%\n",
      "ep 204, loss: 16.88, 8000 train 81.74%\n",
      "ep 205, loss: 16.97, 8000 train 81.34%\n",
      "ep 206, loss: 17.31, 8000 train 81.40%\n",
      "ep 207, loss: 17.21, 8000 train 80.90%\n",
      "ep 208, loss: 17.91, 8000 train 80.64%\n",
      "ep 209, loss: 16.97, 8000 train 81.86%\n",
      "ep 210, loss: 17.10, 8000 train 80.97%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:27:57.983673\n",
      "ep 211, loss: 16.57, 8000 train 82.10%\n",
      "ep 212, loss: 16.42, 8000 train 82.14%\n",
      "ep 213, loss: 16.85, 8000 train 81.21%\n",
      "ep 214, loss: 17.00, 8000 train 81.10%\n",
      "ep 215, loss: 16.65, 8000 train 81.61%\n",
      "ep 216, loss: 16.25, 8000 train 82.59%\n",
      "ep 217, loss: 17.01, 8000 train 81.21%\n",
      "ep 218, loss: 16.90, 8000 train 81.09%\n",
      "ep 219, loss: 17.19, 8000 train 81.53%\n",
      "ep 220, loss: 15.77, 8000 train 82.70%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:32:09.503900\n",
      "ep 221, loss: 16.02, 8000 train 83.12%\n",
      "ep 222, loss: 16.31, 8000 train 82.46%\n",
      "ep 223, loss: 15.87, 8000 train 82.66%\n",
      "ep 224, loss: 16.30, 8000 train 82.23%\n",
      "ep 225, loss: 16.53, 8000 train 82.12%\n",
      "ep 226, loss: 15.89, 8000 train 82.85%\n",
      "ep 227, loss: 16.10, 8000 train 82.65%\n",
      "ep 228, loss: 15.57, 8000 train 83.25%\n",
      "ep 229, loss: 16.12, 8000 train 82.25%\n",
      "ep 230, loss: 15.57, 8000 train 83.44%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:36:05.186423\n",
      "ep 231, loss: 15.98, 8000 train 82.04%\n",
      "ep 232, loss: 15.84, 8000 train 82.84%\n",
      "ep 233, loss: 16.23, 8000 train 82.50%\n",
      "ep 234, loss: 15.72, 8000 train 82.79%\n",
      "ep 235, loss: 15.31, 8000 train 83.38%\n",
      "ep 236, loss: 16.31, 8000 train 82.39%\n",
      "ep 237, loss: 15.75, 8000 train 82.46%\n",
      "ep 238, loss: 15.83, 8000 train 82.69%\n",
      "ep 239, loss: 15.46, 8000 train 83.03%\n",
      "ep 240, loss: 15.53, 8000 train 83.15%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:40:06.878444\n",
      "ep 241, loss: 15.58, 8000 train 82.85%\n",
      "ep 242, loss: 15.42, 8000 train 83.45%\n",
      "ep 243, loss: 15.90, 8000 train 82.95%\n",
      "ep 244, loss: 16.06, 8000 train 82.76%\n",
      "ep 245, loss: 15.49, 8000 train 82.97%\n",
      "ep 246, loss: 15.43, 8000 train 82.60%\n",
      "ep 247, loss: 15.06, 8000 train 83.38%\n",
      "ep 248, loss: 15.68, 8000 train 83.09%\n",
      "ep 249, loss: 15.64, 8000 train 83.14%\n",
      "ep 250, loss: 16.03, 8000 train 82.31%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:43:57.578260\n",
      "ep 251, loss: 14.86, 8000 train 83.75%\n",
      "ep 252, loss: 14.96, 8000 train 83.84%\n",
      "ep 253, loss: 15.17, 8000 train 83.58%\n",
      "ep 254, loss: 14.92, 8000 train 83.40%\n",
      "ep 255, loss: 14.63, 8000 train 84.17%\n",
      "ep 256, loss: 15.14, 8000 train 83.51%\n",
      "ep 257, loss: 14.60, 8000 train 83.96%\n",
      "ep 258, loss: 14.76, 8000 train 84.08%\n",
      "ep 259, loss: 14.68, 8000 train 84.17%\n",
      "ep 260, loss: 15.14, 8000 train 83.67%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:48:14.047765\n",
      "ep 261, loss: 15.14, 8000 train 83.55%\n",
      "ep 262, loss: 14.85, 8000 train 83.76%\n",
      "ep 263, loss: 15.19, 8000 train 83.70%\n",
      "ep 264, loss: 14.70, 8000 train 83.49%\n",
      "ep 265, loss: 15.13, 8000 train 83.75%\n",
      "ep 266, loss: 14.83, 8000 train 83.89%\n",
      "ep 267, loss: 14.84, 8000 train 84.15%\n",
      "ep 268, loss: 14.76, 8000 train 84.11%\n",
      "ep 269, loss: 14.53, 8000 train 84.33%\n",
      "ep 270, loss: 14.24, 8000 train 84.56%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:52:13.149164\n",
      "ep 271, loss: 14.86, 8000 train 84.00%\n",
      "ep 272, loss: 14.49, 8000 train 84.23%\n",
      "ep 273, loss: 14.15, 8000 train 84.51%\n",
      "ep 274, loss: 14.34, 8000 train 84.34%\n",
      "ep 275, loss: 14.13, 8000 train 84.69%\n",
      "ep 276, loss: 14.10, 8000 train 84.70%\n",
      "ep 277, loss: 14.18, 8000 train 84.30%\n",
      "ep 278, loss: 13.92, 8000 train 84.89%\n",
      "ep 279, loss: 14.71, 8000 train 84.39%\n",
      "ep 280, loss: 14.04, 8000 train 85.10%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 1:56:20.324654\n",
      "ep 281, loss: 13.63, 8000 train 84.94%\n",
      "ep 282, loss: 13.85, 8000 train 84.66%\n",
      "ep 283, loss: 13.55, 8000 train 84.81%\n",
      "ep 284, loss: 14.13, 8000 train 84.36%\n",
      "ep 285, loss: 14.69, 8000 train 84.33%\n",
      "ep 286, loss: 14.50, 8000 train 84.01%\n",
      "ep 287, loss: 14.47, 8000 train 84.50%\n",
      "ep 288, loss: 14.34, 8000 train 84.40%\n",
      "ep 289, loss: 14.23, 8000 train 83.97%\n",
      "ep 290, loss: 13.88, 8000 train 84.45%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:00:21.564265\n",
      "ep 291, loss: 14.45, 8000 train 84.29%\n",
      "ep 292, loss: 14.20, 8000 train 84.92%\n",
      "ep 293, loss: 13.58, 8000 train 84.82%\n",
      "ep 294, loss: 13.65, 8000 train 84.90%\n",
      "ep 295, loss: 13.39, 8000 train 85.62%\n",
      "ep 296, loss: 13.91, 8000 train 84.74%\n",
      "ep 297, loss: 14.25, 8000 train 84.56%\n",
      "ep 298, loss: 13.45, 8000 train 85.36%\n",
      "ep 299, loss: 13.83, 8000 train 84.92%\n",
      "ep 300, loss: 13.95, 8000 train 84.99%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:04:21.101118\n",
      "ep 301, loss: 13.36, 8000 train 85.62%\n",
      "ep 302, loss: 13.56, 8000 train 85.08%\n",
      "ep 303, loss: 14.06, 8000 train 84.67%\n",
      "ep 304, loss: 13.77, 8000 train 84.86%\n",
      "ep 305, loss: 13.87, 8000 train 84.88%\n",
      "ep 306, loss: 13.36, 8000 train 85.44%\n",
      "ep 307, loss: 13.65, 8000 train 85.32%\n",
      "ep 308, loss: 13.28, 8000 train 85.90%\n",
      "ep 309, loss: 13.21, 8000 train 85.36%\n",
      "ep 310, loss: 13.33, 8000 train 85.34%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:08:25.215364\n",
      "ep 311, loss: 13.17, 8000 train 85.65%\n",
      "ep 312, loss: 14.17, 8000 train 84.76%\n",
      "ep 313, loss: 13.84, 8000 train 85.09%\n",
      "ep 314, loss: 12.98, 8000 train 85.89%\n",
      "ep 315, loss: 13.21, 8000 train 85.59%\n",
      "ep 316, loss: 13.19, 8000 train 85.50%\n",
      "ep 317, loss: 12.93, 8000 train 85.91%\n",
      "ep 318, loss: 12.85, 8000 train 85.80%\n",
      "ep 319, loss: 12.93, 8000 train 86.22%\n",
      "ep 320, loss: 12.82, 8000 train 86.33%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:12:27.511322\n",
      "ep 321, loss: 12.78, 8000 train 85.41%\n",
      "ep 322, loss: 12.87, 8000 train 86.01%\n",
      "ep 323, loss: 12.99, 8000 train 85.85%\n",
      "ep 324, loss: 12.72, 8000 train 86.56%\n",
      "ep 325, loss: 13.32, 8000 train 85.09%\n",
      "ep 326, loss: 13.12, 8000 train 85.81%\n",
      "ep 327, loss: 12.64, 8000 train 86.50%\n",
      "ep 328, loss: 12.74, 8000 train 85.74%\n",
      "ep 329, loss: 12.29, 8000 train 86.29%\n",
      "ep 330, loss: 12.26, 8000 train 86.49%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:16:32.873600\n",
      "ep 331, loss: 12.57, 8000 train 86.49%\n",
      "ep 332, loss: 12.53, 8000 train 85.90%\n",
      "ep 333, loss: 12.69, 8000 train 85.88%\n",
      "ep 334, loss: 13.30, 8000 train 85.32%\n",
      "ep 335, loss: 12.65, 8000 train 86.29%\n",
      "ep 336, loss: 13.16, 8000 train 85.56%\n",
      "ep 337, loss: 12.89, 8000 train 86.20%\n",
      "ep 338, loss: 12.08, 8000 train 86.29%\n",
      "ep 339, loss: 12.76, 8000 train 85.78%\n",
      "ep 340, loss: 12.74, 8000 train 85.92%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:20:32.246639\n",
      "ep 341, loss: 12.84, 8000 train 85.89%\n",
      "ep 342, loss: 12.77, 8000 train 86.10%\n",
      "ep 343, loss: 12.66, 8000 train 86.12%\n",
      "ep 344, loss: 13.23, 8000 train 85.99%\n",
      "ep 345, loss: 12.74, 8000 train 86.61%\n",
      "ep 346, loss: 12.01, 8000 train 87.25%\n",
      "ep 347, loss: 12.30, 8000 train 86.61%\n",
      "ep 348, loss: 11.96, 8000 train 86.91%\n",
      "ep 349, loss: 11.85, 8000 train 87.19%\n",
      "ep 350, loss: 12.60, 8000 train 86.25%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:24:34.535985\n",
      "ep 351, loss: 12.25, 8000 train 86.58%\n",
      "ep 352, loss: 12.15, 8000 train 86.89%\n",
      "ep 353, loss: 11.80, 8000 train 87.02%\n",
      "ep 354, loss: 12.43, 8000 train 86.05%\n",
      "ep 355, loss: 12.78, 8000 train 85.82%\n",
      "ep 356, loss: 12.10, 8000 train 86.78%\n",
      "ep 357, loss: 12.33, 8000 train 86.74%\n",
      "ep 358, loss: 12.23, 8000 train 86.54%\n",
      "ep 359, loss: 12.24, 8000 train 86.60%\n",
      "ep 360, loss: 12.35, 8000 train 86.60%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:28:31.954674\n",
      "ep 361, loss: 11.74, 8000 train 86.89%\n",
      "ep 362, loss: 11.68, 8000 train 87.34%\n",
      "ep 363, loss: 11.96, 8000 train 87.31%\n",
      "ep 364, loss: 12.19, 8000 train 86.67%\n",
      "ep 365, loss: 12.41, 8000 train 86.54%\n",
      "ep 366, loss: 11.94, 8000 train 86.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 367, loss: 12.11, 8000 train 86.51%\n",
      "ep 368, loss: 11.86, 8000 train 87.05%\n",
      "ep 369, loss: 11.82, 8000 train 87.04%\n",
      "ep 370, loss: 11.43, 8000 train 87.71%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:32:29.591019\n",
      "ep 371, loss: 12.29, 8000 train 86.46%\n",
      "ep 372, loss: 11.30, 8000 train 87.67%\n",
      "ep 373, loss: 12.50, 8000 train 85.69%\n",
      "ep 374, loss: 10.89, 8000 train 88.33%\n",
      "ep 375, loss: 11.81, 8000 train 87.22%\n",
      "ep 376, loss: 11.69, 8000 train 87.15%\n",
      "ep 377, loss: 11.42, 8000 train 87.83%\n",
      "ep 378, loss: 11.58, 8000 train 87.11%\n",
      "ep 379, loss: 11.01, 8000 train 87.79%\n",
      "ep 380, loss: 11.80, 8000 train 87.36%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:36:42.180755\n",
      "ep 381, loss: 11.32, 8000 train 87.56%\n",
      "ep 382, loss: 11.11, 8000 train 87.72%\n",
      "ep 383, loss: 11.52, 8000 train 87.06%\n",
      "ep 384, loss: 11.23, 8000 train 88.35%\n",
      "ep 385, loss: 11.61, 8000 train 87.49%\n",
      "ep 386, loss: 11.28, 8000 train 87.56%\n",
      "ep 387, loss: 11.58, 8000 train 87.22%\n",
      "ep 388, loss: 11.84, 8000 train 86.91%\n",
      "ep 389, loss: 11.04, 8000 train 87.91%\n",
      "ep 390, loss: 11.59, 8000 train 87.17%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:40:50.191804\n",
      "ep 391, loss: 11.39, 8000 train 87.59%\n",
      "ep 392, loss: 11.03, 8000 train 87.99%\n",
      "ep 393, loss: 11.68, 8000 train 87.28%\n",
      "ep 394, loss: 11.43, 8000 train 87.48%\n",
      "ep 395, loss: 11.44, 8000 train 88.01%\n",
      "ep 396, loss: 10.75, 8000 train 88.26%\n",
      "ep 397, loss: 11.24, 8000 train 87.76%\n",
      "ep 398, loss: 11.33, 8000 train 88.15%\n",
      "ep 399, loss: 11.32, 8000 train 87.34%\n",
      "ep 400, loss: 11.39, 8000 train 87.71%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:44:47.493849\n",
      "ep 401, loss: 10.94, 8000 train 87.66%\n",
      "ep 402, loss: 11.12, 8000 train 88.36%\n",
      "ep 403, loss: 11.16, 8000 train 87.76%\n",
      "ep 404, loss: 11.04, 8000 train 88.01%\n",
      "ep 405, loss: 11.16, 8000 train 87.67%\n",
      "ep 406, loss: 11.68, 8000 train 87.12%\n",
      "ep 407, loss: 11.13, 8000 train 87.56%\n",
      "ep 408, loss: 11.18, 8000 train 87.46%\n",
      "ep 409, loss: 10.99, 8000 train 87.88%\n",
      "ep 410, loss: 10.94, 8000 train 88.10%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:48:50.618824\n",
      "ep 411, loss: 10.84, 8000 train 88.02%\n",
      "ep 412, loss: 11.26, 8000 train 87.40%\n",
      "ep 413, loss: 11.04, 8000 train 88.44%\n",
      "ep 414, loss: 10.75, 8000 train 88.59%\n",
      "ep 415, loss: 10.54, 8000 train 88.52%\n",
      "ep 416, loss: 11.24, 8000 train 87.99%\n",
      "ep 417, loss: 11.29, 8000 train 87.70%\n",
      "ep 418, loss: 11.27, 8000 train 87.52%\n",
      "ep 419, loss: 10.89, 8000 train 88.31%\n",
      "ep 420, loss: 11.38, 8000 train 87.81%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:53:00.577986\n",
      "ep 421, loss: 10.50, 8000 train 88.67%\n",
      "ep 422, loss: 10.71, 8000 train 88.52%\n",
      "ep 423, loss: 10.75, 8000 train 88.31%\n",
      "ep 424, loss: 10.90, 8000 train 88.14%\n",
      "ep 425, loss: 10.95, 8000 train 88.29%\n",
      "ep 426, loss: 10.78, 8000 train 87.96%\n",
      "ep 427, loss: 10.18, 8000 train 89.41%\n",
      "ep 428, loss: 11.41, 8000 train 87.88%\n",
      "ep 429, loss: 10.48, 8000 train 88.39%\n",
      "ep 430, loss: 10.14, 8000 train 89.53%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 2:57:16.760122\n",
      "ep 431, loss: 10.98, 8000 train 87.72%\n",
      "ep 432, loss: 11.83, 8000 train 87.35%\n",
      "ep 433, loss: 10.75, 8000 train 88.67%\n",
      "ep 434, loss: 10.69, 8000 train 88.16%\n",
      "ep 435, loss: 10.99, 8000 train 87.80%\n",
      "ep 436, loss: 10.62, 8000 train 88.36%\n",
      "ep 437, loss: 10.70, 8000 train 88.72%\n",
      "ep 438, loss: 10.52, 8000 train 88.41%\n",
      "ep 439, loss: 9.87, 8000 train 89.39%\n",
      "ep 440, loss: 10.30, 8000 train 89.06%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:01:28.067252\n",
      "ep 441, loss: 10.68, 8000 train 87.92%\n",
      "ep 442, loss: 10.48, 8000 train 88.91%\n",
      "ep 443, loss: 10.85, 8000 train 88.16%\n",
      "ep 444, loss: 10.35, 8000 train 88.34%\n",
      "ep 445, loss: 10.83, 8000 train 88.64%\n",
      "ep 446, loss: 10.36, 8000 train 88.78%\n",
      "ep 447, loss: 9.87, 8000 train 89.08%\n",
      "ep 448, loss: 10.07, 8000 train 88.99%\n",
      "ep 449, loss: 9.95, 8000 train 88.92%\n",
      "ep 450, loss: 10.50, 8000 train 89.03%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:05:31.494985\n",
      "ep 451, loss: 11.21, 8000 train 88.15%\n",
      "ep 452, loss: 10.71, 8000 train 87.79%\n",
      "ep 453, loss: 10.10, 8000 train 89.30%\n",
      "ep 454, loss: 10.43, 8000 train 89.01%\n",
      "ep 455, loss: 10.49, 8000 train 88.69%\n",
      "ep 456, loss: 10.46, 8000 train 88.65%\n",
      "ep 457, loss: 10.45, 8000 train 88.52%\n",
      "ep 458, loss: 9.84, 8000 train 88.96%\n",
      "ep 459, loss: 10.62, 8000 train 88.16%\n",
      "ep 460, loss: 10.70, 8000 train 88.70%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:09:39.389465\n",
      "ep 461, loss: 10.17, 8000 train 89.06%\n",
      "ep 462, loss: 10.58, 8000 train 88.66%\n",
      "ep 463, loss: 10.07, 8000 train 89.04%\n",
      "ep 464, loss: 10.22, 8000 train 88.78%\n",
      "ep 465, loss: 10.11, 8000 train 89.20%\n",
      "ep 466, loss: 10.27, 8000 train 88.60%\n",
      "ep 467, loss: 10.50, 8000 train 89.00%\n",
      "ep 468, loss: 10.26, 8000 train 88.67%\n",
      "ep 469, loss: 10.23, 8000 train 89.49%\n",
      "ep 470, loss: 9.86, 8000 train 89.39%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:13:49.797838\n",
      "ep 471, loss: 9.78, 8000 train 89.41%\n",
      "ep 472, loss: 10.21, 8000 train 88.76%\n",
      "ep 473, loss: 9.77, 8000 train 89.28%\n",
      "ep 474, loss: 9.52, 8000 train 89.89%\n",
      "ep 475, loss: 10.03, 8000 train 88.94%\n",
      "ep 476, loss: 10.08, 8000 train 88.85%\n",
      "ep 477, loss: 10.04, 8000 train 89.40%\n",
      "ep 478, loss: 9.60, 8000 train 89.80%\n",
      "ep 479, loss: 10.08, 8000 train 88.91%\n",
      "ep 480, loss: 10.01, 8000 train 89.18%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:17:53.965288\n",
      "ep 481, loss: 10.23, 8000 train 89.12%\n",
      "ep 482, loss: 9.98, 8000 train 88.83%\n",
      "ep 483, loss: 9.81, 8000 train 89.30%\n",
      "ep 484, loss: 9.79, 8000 train 89.60%\n",
      "ep 485, loss: 9.27, 8000 train 89.64%\n",
      "ep 486, loss: 9.63, 8000 train 89.71%\n",
      "ep 487, loss: 10.21, 8000 train 89.08%\n",
      "ep 488, loss: 9.53, 8000 train 89.74%\n",
      "ep 489, loss: 10.07, 8000 train 89.16%\n",
      "ep 490, loss: 9.86, 8000 train 89.31%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:22:04.174884\n",
      "ep 491, loss: 9.39, 8000 train 90.20%\n",
      "ep 492, loss: 9.70, 8000 train 89.41%\n",
      "ep 493, loss: 9.78, 8000 train 89.21%\n",
      "ep 494, loss: 9.02, 8000 train 90.22%\n",
      "ep 495, loss: 9.67, 8000 train 89.29%\n",
      "ep 496, loss: 9.18, 8000 train 90.09%\n",
      "ep 497, loss: 9.25, 8000 train 89.94%\n",
      "ep 498, loss: 9.77, 8000 train 89.26%\n",
      "ep 499, loss: 9.49, 8000 train 89.74%\n",
      "ep 500, loss: 9.72, 8000 train 89.70%\n",
      "   Model saved to checkModel.pth\n",
      "Time elapsed: 3:26:06.883693\n",
      "   Model saved to savedModel.pth\n",
      "total time needed to train network:         3:26:06.947694\n",
      "total time in seconds: 12366.94769358635\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Tracking training time ###\n",
    "##############################\n",
    "start_time = time.time() ## Added\n",
    "time_elapsed = 0  ## Added Line\n",
    "##############################\n",
    "\n",
    "###############################\n",
    "### Tracking nn performance ###\n",
    "###############################\n",
    "minibatch_loss_list, train_accuracy_list, test_accuracy_list = [], [], [] ## Added\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "print(\"Using device: {}\"\n",
    "      \"\\n\".format(str(device)))\n",
    "########################################################################\n",
    "#######                      Loading Data                        #######\n",
    "########################################################################\n",
    "data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "\n",
    "if train_val_split == 1:\n",
    "    # Train on the entire dataset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset,\n",
    "                        transform=transform('train'))\n",
    "    trainloader = torch.utils.data.DataLoader(data,\n",
    "                        batch_size=batch_size, shuffle=True);\n",
    "else:\n",
    "    # Split the dataset into trainset and testset\n",
    "    data = torchvision.datasets.ImageFolder(root=dataset)\n",
    "    data.len=len(data)\n",
    "    train_len = int((train_val_split)*data.len)\n",
    "    test_len = data.len - train_len\n",
    "    train_subset, test_subset = random_split(data, [train_len, test_len])\n",
    "    trainset = DatasetFromSubset(\n",
    "        train_subset, transform=transform('train'))\n",
    "    testset = DatasetFromSubset(\n",
    "        test_subset, transform=transform('test'))\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "    testloader = torch.utils.data.DataLoader(testset, \n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Get model, loss criterion and optimizer from student\n",
    "net = net.to(device)\n",
    "criterion = loss_func\n",
    "optimizer = optimizer\n",
    "# get weight initialization and lr scheduler, if appropriate\n",
    "weights_init = weights_init\n",
    "scheduler = scheduler\n",
    "\n",
    "# apply custom weight initialization, if it exists\n",
    "net.apply(weights_init)\n",
    "\n",
    "########################################################################\n",
    "#######                        Training                          #######\n",
    "########################################################################\n",
    "print(\"Start training...\")\n",
    "for epoch in range(1,epochs+1):\n",
    "    total_loss = 0\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for batch in trainloader:           # Load batch\n",
    "        images, labels = batch \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        preds = net(images)             # Process batch\n",
    "\n",
    "        loss = criterion(preds, labels) # Calculate loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()                 # Calculate gradients\n",
    "        optimizer.step()                # Update weights\n",
    "\n",
    "        output = preds.argmax(dim=1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_images += labels.size(0)\n",
    "        total_correct += output.eq(labels).sum().item()\n",
    "        minibatch_loss_list.append(loss.item())  ## Added\n",
    "\n",
    "    # apply lr schedule, if it exists\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100 \n",
    "    train_accuracy_list.append(model_accuracy)  ## Added\n",
    "    print('ep {0}, loss: {1:.2f}, {2} train {3:.2f}%'.format(\n",
    "           epoch, total_loss, total_images, model_accuracy), end='')\n",
    "\n",
    "    if train_val_split < 1:\n",
    "        test_network(net,testloader, test_accuracy_list,\n",
    "                     print_confusion=(epoch % 10 == 0)) ## Added\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "   \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(net.state_dict(),'v3_check.pth')\n",
    "        print(\"   Model saved to checkModel.pth\")\n",
    "        time_elapsed = time.time() - start_time  ## Added Line\n",
    "        print(f'Time elapsed: {str(datetime.timedelta(seconds = time_elapsed))}') ## TIME\n",
    "    \n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "torch.save(net.state_dict(),'v3_saved.pth')\n",
    "print(\"   Model saved to savedModel.pth\")\n",
    "time_elapsed = time.time() - start_time ## Added Line\n",
    "print(f'total time needed to train network: \\\n",
    "        {str(datetime.timedelta(seconds = time_elapsed))}\\ntotal time in seconds: {time_elapsed}') ## TIME\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 256\n",
      "learning rate: 0.0005\n",
      "train_val_split: 1\n",
      "epochs: 500\n",
      "training data - total instances = 8000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24852/2636902767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtrain_data_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cat_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtest_data_distribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_cat_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'training data distribution - {train_data_distribution}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#**        Data Information     **#\n",
    "###################################\n",
    "print(f'batch size: {batch_size}')\n",
    "print(f'learning rate: {learning_rate}')\n",
    "print(f'train_val_split: {train_val_split}')\n",
    "print(f'epochs: {epochs}')\n",
    "\n",
    "\n",
    "#############################\n",
    "#**         END           **#\n",
    "#############################\n",
    "\n",
    "\n",
    "# Getting count of each cat breed, should be close to 8*0.8*1000 initially..\n",
    "train_data_distribution = get_cat_count(trainloader, 'training data')\n",
    "# Getting count of each cat breed, should be close to 8*0.2*1000 initially..\n",
    "test_data_distribution = get_cat_count(testloader, 'test data')\n",
    "\n",
    "print(f'training data distribution - {train_data_distribution}')\n",
    "print(f'test data distribution - {test_data_distribution}')\n",
    "\n",
    "plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n",
    "                   num_epochs=epochs,\n",
    "                   iter_per_epoch=len(trainloader),\n",
    "                   results_dir=None,\n",
    "                   averaging_iterations=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_accuracy(train_acc_list=train_accuracy_list,\n",
    "              test_acc_list=test_accuracy_list,\n",
    "              results_dir=None)\n",
    "plt.show()\n",
    "\n",
    "net.cpu()\n",
    "show_examples(model=net, data_loader=testloader, class_dict=cat_dict)\n",
    "\n",
    "conf_matrix = compute_confusion_matrix(model=net, data_loader=testloader, device=torch.device('cpu'))\n",
    "print(conf_matrix)\n",
    "plot_confusion_matrix(conf_matrix, class_names=cat_dict.values(), test_data_distribution=test_data_distribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
